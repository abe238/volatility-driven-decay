% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\newcounter{none} % for unnumbered tables
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\section{Volatility-Driven Decay: Adaptive Memory Retention for RAG
Systems Under Unknown
Drift}\label{volatility-driven-decay-adaptive-memory-retention-for-rag-systems-under-unknown-drift}

\textbf{Abe Diaz} Independent Researcher
\texttt{https://github.com/abe238/volatility-driven-decay}

February 2026

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Abstract}\label{abstract}

Retrieval-Augmented Generation (RAG) systems face a silent reliability
crisis: when knowledge drifts---APIs deprecated, guidelines revised,
frameworks migrated---retrieval continues serving outdated answers with
full confidence. We find that naive retrieval serves stale content 67\%
of the time across 120 real-world technical facts. Existing solutions
assume known drift patterns: recency-biased retrieval suits constant
change, time-weighted decay handles gradual transitions, and sliding
windows work for bounded contexts. None adapts when the pattern is
unknown---the typical production condition.

We introduce Volatility-Driven Decay (VDD), where the memory decay rate
lambda(t) dynamically adjusts based on detected semantic drift via
sigmoid activation on embedding centroid distance. Structurally inspired
by control-theoretic adaptive filtering and cognitive models of memory
consolidation, VDD continuously modulates its forgetting rate between a
conservative baseline and an aggressive ceiling. Validated on 120 facts
across React, Python, and Node.js documentation (n=30, bootstrap CIs,
Benjamini-Hochberg corrected, Cohen's d), VDD achieves 87.5\% error
reduction versus static baselines, ranks \#2 among 13 methods across
four drift patterns, and is never the worst performer in any tested
scenario. An auto-calibration method (75th percentile of burn-in
volatilities) eliminates manual V\_0 tuning, improving over hand-tuned
defaults by 22.7\% on average.

We are equally explicit about limitations: recency outperforms VDD in
permanent high-drift (d = -2.90), time-weighted retrieval dominates
gradual transitions (0.871 vs 0.584), and adaptive baselines
(Holt-Winters, EMA-lambda) beat VDD in 3/4 synthetic scenarios. On
StreamingQA (36K temporal QA pairs), all decay methods including VDD
underperform no-decay (d = -7.68), confirming that VDD is
counterproductive on knowledge-accumulation tasks---it is specifically
designed for knowledge-replacement scenarios. On FreshQA (452 temporal
questions), all decay methods achieve 100\% current-answer retrieval
rate versus no-decay's 70--80\%, though keyword-overlap scoring fails to
capture this advantage. Rankings transfer between hash-based and real
neural embeddings (4/5 scenario agreement, r = 0.935 effect size
correlation) and across embedding models (nomic-embed-text vs
mxbai-embed-large, Spearman rho = 0.978), though effect sizes attenuate
with real embeddings (mean ratio 0.70x), indicating controlled-condition
results are directionally valid but inflated. VDD's value is precisely
for production systems where drift patterns cannot be predicted in
advance. This positioning complements---rather than competes
with---modern RAG architectures (Self-RAG, CRAG, Adaptive-RAG) that
improve \emph{what} to retrieve, while VDD addresses \emph{when to
forget}. We release the complete benchmark, dataset, and evaluation
framework.

\textbf{Keywords:} RAG, memory decay, concept drift, adaptive retrieval,
knowledge staleness, LLM memory

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{1. Introduction}\label{introduction}

\subsubsection{1.1 The Knowledge Staleness
Problem}\label{the-knowledge-staleness-problem}

Retrieval-Augmented Generation has become the dominant architecture for
grounding large language models in external knowledge (Lewis et al.,
2020; Gao et al., 2024). Yet RAG systems harbor a failure mode the
community has largely ignored: \textbf{knowledge staleness}. When the
world changes---React deprecates lifecycle methods, Python removes a
module, a medical guideline is revised---retrieval systems continue
surfacing outdated answers. The user receives a confident,
well-formatted, \emph{wrong} answer.

This is not hypothetical. In our evaluation of 120 real-world facts
across three documentation ecosystems, naive retrieval (no decay) serves
stale content \textbf{66.7\% of the time}. Even commonly recommended
approaches fail: sliding window retrieval catastrophically collapses to
random-chance accuracy (0.333) on certain domain structures.

\subsubsection{1.2 The Static Decay
Dilemma}\label{the-static-decay-dilemma}

The ``Forgetting is a Feature'' hypothesis (Wei et al., 2026)
established that biologically inspired decay improves retrieval
precision. However, existing implementations rely on \textbf{static
decay}---a fixed half-life applied uniformly regardless of environmental
conditions:

\begin{itemize}
\tightlist
\item
  \textbf{Conservative decay} (lambda \textasciitilde{} 0.1) preserves
  valid knowledge but accumulates stale facts (IAE 148, nearly 5x worse
  than adaptive approaches).
\item
  \textbf{Aggressive decay} (lambda \textgreater= 0.5) stays current but
  catastrophically forgets when knowledge reverts (IAE 52.1 vs VDD's
  35.4 in reversion scenarios).
\item
  \textbf{No fixed rate handles both.} A coding assistant may face
  gradual API deprecation for months, then a sudden breaking
  change---within the same deployment.
\end{itemize}

\subsubsection{1.3 The Modern RAG
Landscape}\label{the-modern-rag-landscape}

Recent architectures have advanced retrieval quality along multiple
axes. Self-RAG (Asai et al., 2024) introduces reflection tokens for
retrieval-time quality control. CRAG (Yan et al., 2024) adds corrective
retrieval to handle low-quality results. Adaptive-RAG (Jeong et al.,
2024) routes queries by complexity. FLARE (Jiang et al., 2023) triggers
retrieval actively during generation. RAPTOR (Sarthi et al., 2024)
builds hierarchical document summaries for multi-level retrieval.

These architectures improve \emph{what} and \emph{how} to retrieve. None
addresses \emph{when to forget}---the temporal validity of stored
knowledge. VDD operates at the storage layer, determining how
aggressively to discount old memories based on environmental change.
This is orthogonal and complementary: a Self-RAG system with VDD-managed
memory would both select high-quality passages \emph{and} automatically
deprioritize stale ones.

\subsubsection{1.4 Our Proposal}\label{our-proposal}

We propose that the optimal decay rate is not a constant but a function
of \textbf{environmental volatility}. When the semantic landscape is
stable, decay should be slow. When embeddings shift---signaling
knowledge change---decay should accelerate. We ground this in
control-theoretic adaptive filtering, drawing structural inspiration
from Recursive Least Squares (RLS) where a forgetting factor dynamically
weights past observations, and in cognitive models of hippocampal memory
consolidation where environmental novelty accelerates forgetting of
outdated associations (Wixted, 2004; Ebbinghaus, 1885).

\subsubsection{1.5 Contributions}\label{contributions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A \textbf{drift detection method} using embedding centroid distance
  achieving 100\% recall with 0.12ms latency.
\item
  \textbf{Proof of adaptive behavior}: VDD's lambda is genuinely bimodal
  (Hartigan's Dip p \textless{} 0.001, Ashman's D \textgreater{} 2), not
  noisy static decay.
\item
  \textbf{Multi-domain validation} on 120 facts across React, Python,
  and Node.js with real embeddings (nomic-embed-text).
\item
  \textbf{Never-worst robustness}: VDD ranks \#2 among 13 methods across
  all four tested drift patterns.
\item
  \textbf{Activation function ablation} showing all tested activations
  converge at recommended k \textgreater= 5 (d -\textgreater{} 0),
  justifying sigmoid selection on interpretability grounds.
\item
  \textbf{Hash-to-real embedding transfer validation} demonstrating
  ranking preservation across embedding types (4/5 scenarios, r =
  0.935).
\item
  \textbf{Rigorous multiple comparison correction}: 85/88 significant
  results survive Benjamini-Hochberg FDR correction with Type M error
  analysis.
\item
  \textbf{An honest comparative analysis} showing exactly when VDD helps
  and when simpler approaches suffice.
\item
  \textbf{FreshQA benchmark evaluation} (452 temporal questions)
  confirming 100\% current-answer retrieval for all decay methods vs
  70--80\% for no-decay.
\item
  \textbf{Auto-calibration} of V\_0 via 75th-percentile burn-in,
  eliminating manual tuning and improving over hand-tuned defaults by
  22.7\%.
\item
  \textbf{Cross-model embedding generalization} (nomic-embed-text vs
  mxbai-embed-large): method rankings preserved with Spearman rho =
  0.978.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{2. Related Work}\label{related-work}

\subsubsection{2.1 RAG Foundations}\label{rag-foundations}

The RAG framework (Lewis et al., 2020) demonstrated that
retrieval-augmented models outperform pure parametric models on
knowledge-intensive tasks. REALM (Guu et al., 2020) pioneered
pre-training with retrieval. RETRO (Borgeaud et al., 2022) scaled
retrieval to trillion-token databases. DPR (Karpukhin et al., 2020)
established dense passage retrieval as the standard approach. Atlas
(Izacard et al., 2023) demonstrated few-shot learning via retrieval.
Recent surveys (Gao et al., 2024) identify memory management as an open
challenge, with most systems relying on simple recency or relevance-only
retrieval.

\subsubsection{2.2 Temporal RAG and Memory
Systems}\label{temporal-rag-and-memory-systems}

A growing body of work addresses temporal aspects of RAG, positioning
VDD within a rapidly evolving landscape:

\textbf{ARM} (Bursa, 2026) introduces Adaptive Retrieval Memory, a
concurrent approach that modulates retrieval strategies based on
query-time context signals. While ARM adapts \emph{retrieval behavior}
(what to fetch and when), VDD adapts \emph{memory retention} (how
aggressively to forget stored knowledge). The approaches are
complementary: ARM selects among retrieval strategies at query time,
while VDD manages the temporal validity of the knowledge base itself. We
note that ARM was developed concurrently and independently.

\textbf{T-GRAG} (Li et al., 2025) proposes a graph-based temporal RAG
architecture where documents are organized in a temporal knowledge
graph, enabling traversal along temporal edges. T-GRAG addresses
temporal reasoning through structural representation; VDD addresses it
through adaptive decay. A system could combine T-GRAG's temporal graph
structure with VDD's volatility-modulated weighting to achieve both
structural and dynamic temporal awareness.

\textbf{HippoRAG} (Gutierrez et al., NeurIPS 2024) draws on the
hippocampal indexing theory of human long-term memory, using an LLM to
extract knowledge graph triples that serve as a ``hippocampal index''
for retrieval. HippoRAG focuses on mimicking associative memory for
single-step and multi-hop QA. VDD shares the biological inspiration but
targets a different aspect: where HippoRAG models \emph{how} memories
are stored and associated, VDD models \emph{when} memories should be
forgotten. HippoRAG's knowledge graph could benefit from VDD-style
temporal weighting on its edges.

\textbf{MemoRAG} (Qian et al., 2025) introduces a memory-inspired
knowledge discovery framework that uses a lightweight LLM to form a
global memory of the database, generating draft answers that guide
retrieval. MemoRAG's global memory acts as a retrieval cue generator; it
does not address temporal validity of the underlying knowledge. VDD
could enhance MemoRAG by temporally weighting the documents that the
global memory summarizes.

\textbf{Mem0} (Chhikara et al., 2025) provides a production memory layer
for AI applications with TTL-based auto-decay and importance scoring.
Mem0's decay is static (fixed TTL per memory type) rather than
environment-aware. VDD's volatility-modulated decay could replace Mem0's
fixed TTL, providing adaptive retention without manual TTL configuration
per deployment.

\textbf{FadeMem} (Wei et al., 2026) establishes the ``Forgetting is a
Feature'' hypothesis, demonstrating that biologically inspired decay
improves agent memory efficiency. FadeMem validates the core premise
that VDD builds upon---that forgetting is beneficial---but implements
static Ebbinghaus-style decay curves. VDD extends this insight by making
the decay rate itself adaptive to environmental conditions.

\textbf{Cognitive Workspace} (An, 2025) proposes a
working-memory-inspired architecture for LLM agents with attention-gated
memory access. Like VDD, it draws on cognitive science, but focuses on
attention mechanisms rather than temporal decay. The two approaches
address different aspects of memory management and could be combined.

\subsubsection{2.3 LLM-Native Memory
Systems}\label{llm-native-memory-systems}

Production LLM systems increasingly implement persistent memory, yet
none uses drift-aware forgetting:

\begin{itemize}
\tightlist
\item
  \textbf{Claude} (Anthropic): File-based memory with no decay
  mechanism, leading to unbounded growth and a ``fading memory'' problem
  where old entries are never removed.
\item
  \textbf{ChatGPT} (OpenAI): Timestamped entries with manual deletion
  only; FIFO context truncation under pressure.
\item
  \textbf{Gemini/Vertex} (Google): Fixed TTL---the closest to decay, but
  static rather than drift-aware.
\item
  \textbf{MemGPT/Letta} (Packer et al., 2023): Context-pressure eviction
  with hierarchical memory tiers, but no environmental awareness. VDD
  could enhance MemGPT by dynamically adjusting tier boundaries.
\item
  \textbf{MemoryBank} (Zhong et al., 2024): Ebbinghaus-inspired
  forgetting curves, but decay parameters are static.
\end{itemize}

The gap is clear: no production system implements forgetting that adapts
to the rate of environmental change. VDD fills this by providing a
volatility-modulated decay mechanism that can be integrated into any of
these architectures.

\subsubsection{2.4 Modern RAG
Architectures}\label{modern-rag-architectures}

Self-RAG (Asai et al., 2024) trains models to emit special ``reflection
tokens'' that decide when to retrieve, assess retrieval relevance, and
critique generated output. This addresses retrieval \emph{quality} at
generation time---orthogonal to VDD's storage-layer \emph{temporal
validity}.

CRAG (Yan et al., 2024) implements a corrective retrieval pipeline: a
lightweight evaluator triggers web search when retrieved documents are
ambiguous or incorrect. Like Self-RAG, CRAG addresses quality, not
staleness.

Adaptive-RAG (Jeong et al., 2024) routes queries by complexity---simple
queries skip retrieval, complex ones use multi-step reasoning. This
optimizes the retrieval \emph{process} but does not address whether
retrieved documents are temporally valid.

FLARE (Jiang et al., 2023) generates text iteratively, triggering
retrieval when the model's confidence drops. RAPTOR (Sarthi et al.,
2024) builds hierarchical summaries enabling retrieval at multiple
abstraction levels.

\textbf{RAG-without-Forgetting} (Hu et al., 2026) proposes Experience
Replay Memory (ERM) to prevent catastrophic forgetting when fine-tuning
RAG models on new domains. ERM addresses parametric forgetting during
model training; VDD addresses non-parametric forgetting in the retrieval
memory. The two operate at different layers and are complementary.

\textbf{Key insight}: These architectures improve \emph{what/how} to
retrieve. VDD addresses \emph{when to forget}. A system combining
Self-RAG's quality filtering with VDD's temporal management would
address both failure modes.

\subsubsection{2.5 Embedding Drift
Detection}\label{embedding-drift-detection}

Traditional drift detection methods (ADWIN, Page-Hinkley) operate on
scalar data streams and fail on high-dimensional embeddings. Recent work
has explored distribution-based approaches for embedding drift:

\textbf{Maximum Mean Discrepancy (MMD)} (Feldhans et al., 2021) provides
a kernel-based two-sample test applicable to high-dimensional
distributions, detecting distributional shift between embedding windows.
MMD offers formal statistical guarantees but requires O(n\^{}2)
computation per test. VDD's embedding centroid distance is a
computationally cheaper alternative (O(d) per step) that trades
statistical rigor for latency, achieving 0.12ms detection time suitable
for real-time RAG queries.

\textbf{Learned drift detectors} using neural networks to detect
distribution shifts in embedding spaces have been proposed, but
typically require labeled drift examples for training. VDD's
unsupervised centroid-distance approach requires no labeled drift data,
making it immediately deployable.

Our embedding distance detector can be viewed as a mean-embedding MMD
approximation with a linear kernel, sacrificing the full distributional
test for O(d) per-step efficiency. Future work could explore full MMD or
Learned Kernel MMD for higher detection precision at the cost of
latency.

\subsubsection{2.6 Temporal QA Benchmarks}\label{temporal-qa-benchmarks}

Several benchmarks evaluate temporal reasoning, providing context for
VDD's contribution:

\begin{itemize}
\tightlist
\item
  \textbf{StreamingQA} (Liska et al., 2022): Tests QA over temporally
  ordered news, measuring whether models track evolving answers.
\item
  \textbf{FreshQA} (Vu et al., 2024): Evaluates LLMs on questions
  requiring up-to-date knowledge, finding frequent failures on recently
  changed facts.
\item
  \textbf{RealTimeQA} (Kasai et al., 2023): Weekly-updated benchmark
  tracking model accuracy on current events.
\item
  \textbf{TimeQA} (Chen et al., 2021): Tests temporal reasoning over
  structured knowledge.
\item
  \textbf{EvolveBench} (Yin et al., 2024): Evaluates knowledge editing
  methods on evolving facts.
\item
  \textbf{ChronoQA} (Chen et al., 2025): Specifically targets temporal
  knowledge evolution in QA, tracking how answers change over time.
  ChronoQA's focus on answer versioning closely aligns with VDD's
  version-drift scenarios.
\item
  \textbf{TEMPO} (Abdallah et al., 2026): A temporal reasoning benchmark
  that evaluates models' ability to handle time-sensitive queries across
  multiple knowledge domains.
\end{itemize}

These benchmarks test \emph{whether} systems answer correctly over time.
VDD provides a \emph{mechanism} for maintaining temporal accuracy in
retrieval. Our 120-fact dataset specifically targets version-drift
scenarios (API changes, deprecated features) rather than general
temporal knowledge. We acknowledge that evaluation on established
temporal QA benchmarks would strengthen VDD's generalizability claims
(see Section 7).

\subsubsection{2.7 Concept Drift and Stream
Processing}\label{concept-drift-and-stream-processing}

The concept drift literature provides theoretical foundations. Gama et
al.~(2014) survey adaptation strategies: blind approaches (fixed-rate
forgetting), informed approaches (drift detection), and ensemble
methods. VDD combines informed detection (embedding distance) with
sigmoid-modulated blind adaptation.

Lu et al.~(2019) categorize drift as sudden, gradual, incremental, or
recurring. Our experiments cover all four, with VDD showing particular
strength on mixed patterns. ADWIN (Bifet \& Gavalda, 2007) provides
adaptive windowing for scalar streams; we show it fails on
high-dimensional embeddings (Section 5.2).

\subsubsection{2.8 Knowledge Editing and Continual
Learning}\label{knowledge-editing-and-continual-learning}

Knowledge editing methods (ROME, Meng et al., 2022; MEMIT, Meng et al.,
2023) modify model parameters to update specific facts. WISE (Wang et
al., NeurIPS 2024) enables continual knowledge editing without
catastrophic forgetting by maintaining a dual parametric memory with a
routing mechanism. These operate on parametric knowledge; VDD operates
on non-parametric retrieval memory---the approaches are complementary. A
system could use WISE for high-priority individual fact updates while
VDD manages the broader temporal validity of the retrieval corpus.

Continual learning addresses the stability-plasticity dilemma
(Kirkpatrick et al., 2017): learning new information without
catastrophically forgetting old. Progressive networks (Rusu et al.,
2016) and PackNet (Mallya \& Lazebnik, 2018) approach this from the
parametric side. VDD addresses it on the retrieval side, dynamically
adjusting how aggressively past retrievals are discounted.

\subsubsection{2.9 Positioning VDD Among Contemporary
Systems}\label{positioning-vdd-among-contemporary-systems}

The following table summarizes how VDD relates to contemporary temporal
memory approaches:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1159}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1014}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2754}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2464}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2609}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
System
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Temporal Mechanism
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Drift Awareness
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
VDD Relationship
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{VDD} (ours) & Retrieval scoring & Sigmoid-modulated decay &
Adaptive (embedding distance) & --- \\
ARM (Bursa, 2026) & Retrieval strategy & Strategy switching &
Context-dependent & Complementary (different layer) \\
T-GRAG (Li et al., 2025) & Knowledge structure & Temporal graph edges &
Structural & Complementary (VDD adds dynamic weights) \\
HippoRAG (Gutierrez et al., 2024) & Memory indexing & KG-based
association & None & Complementary (VDD adds temporal decay) \\
MemoRAG (Qian et al., 2025) & Global memory & LLM-generated summaries &
None & Complementary (VDD adds temporal validity) \\
Mem0 (Chhikara et al., 2025) & Memory storage & Static TTL & Fixed & VDD
extends with adaptive TTL \\
FadeMem (Wei et al., 2026) & Agent memory & Ebbinghaus decay & Static &
VDD extends with volatility modulation \\
WISE (Wang et al., 2024) & Parametric memory & Dual memory routing &
Per-edit & Complementary (different memory type) \\
Self-RAG (Asai et al., 2024) & Generation time & Reflection tokens &
None & Complementary (quality vs.~temporal) \\
CRAG (Yan et al., 2024) & Retrieval correction & Web search fallback &
None & Complementary (quality vs.~temporal) \\
\end{longtable}
}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{3. The VDD Method}\label{the-vdd-method}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 1: VDD architecture. Incoming documents update an embedding centroid; the distance between recent and archival centroids drives a sigmoid-modulated decay rate.}]{archive/v8_arxiv/fig1_architecture.png}}
\caption{Figure 1: VDD architecture. Incoming documents update an
embedding centroid; the distance between recent and archival centroids
drives a sigmoid-modulated decay rate.}
\end{figure}

\subsubsection{3.1 Cognitive-Theoretic
Foundation}\label{cognitive-theoretic-foundation}

Our model draws structural inspiration from two sources:

\textbf{Control theory.} In Recursive Least Squares (RLS) adaptive
filtering (Haykin, 2002), a forgetting factor weights past observations:
values near 0 yield slow learning/forgetting (stability), while values
near 1 yield fast learning/forgetting (plasticity). We map this
structural pattern to VDD's adaptive decay. \textbf{We explicitly note
that we do not claim RLS convergence properties transfer to VDD.} The
analogy motivates the functional form; the experimental results validate
it independently.

\textbf{Cognitive psychology.} Ebbinghaus's (1885) forgetting curve
established that memory retention decays exponentially over time, but
the rate depends on environmental factors. Wixted (2004) showed that
forgetting rates increase when the environment changes---novel stimuli
accelerate the decay of outdated associations, a process linked to
hippocampal consolidation dynamics. VDD operationalizes this principle:
environmental novelty (detected via embedding drift) accelerates the
forgetting rate, while stability preserves existing memories.

We map this to the Retrieval Score (S) for a memory m at time t:

\begin{verbatim}
S(m, t) = Sim(q, m) * exp(-lambda(t) * delta_t)
\end{verbatim}

where Sim is semantic similarity and lambda(t) is the adaptive decay
coefficient.

The structural mapping from RLS to VDD is:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
RLS Component & VDD Analog \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Forgetting factor lambda & Decay coefficient lambda(t) \\
Innovation signal & Embedding centroid distance V\_t \\
Variable forgetting factor & Sigmoid activation on V\_t \\
Stability-plasticity tradeoff & Conservative vs.~aggressive decay \\
\end{longtable}
}

\subsubsection{3.2 Volatility Detection via Embedding
Distance}\label{volatility-detection-via-embedding-distance}

We evaluated three drift detectors: ADWIN (Bifet \& Gavalda, 2007),
Page-Hinkley (Page, 1954), and Embedding Distance. Traditional methods
completely fail on high-dimensional embedding streams because they
expect scalar distributions.

The \textbf{Embedding Distance} detector computes volatility as the
cosine distance between current and archival embedding centroids:

\begin{verbatim}
V_t = 1 - cos(mu_curr, mu_arch)
\end{verbatim}

where mu\_curr is the centroid of recent embeddings (window size
w\_curr) and mu\_arch is the centroid of archival embeddings (window
size w\_arch). This achieves 100\% recall on regime shifts with 0.12ms
latency.

\textbf{Computational Complexity}: The VDD pipeline has time complexity
O(T * d) for T timesteps with embedding dimension d, assuming
constant-size sliding windows. Each timestep requires centroid update in
O(d), cosine distance in O(d), and sigmoid evaluation in O(1).

\textbf{Relation to MMD}: Our centroid-distance approach can be viewed
as a mean-embedding approximation of MMD with a linear kernel. Full MMD
provides distributional sensitivity (detecting variance changes, not
just mean shifts) at O(n\^{}2) cost. We trade distributional
completeness for O(d) per-step latency suitable for real-time RAG
queries.

\subsubsection{3.3 The Adaptive Decay
Function}\label{the-adaptive-decay-function}

We apply sigmoid activation to modulate decay rate:

\begin{verbatim}
lambda(t) = lambda_base + (lambda_max - lambda_base) * sigma(k * (V_t - V_0))
\end{verbatim}

where: - lambda\_base in {[}0.2, 0.3{]} is the resting decay rate -
lambda\_max in {[}0.9, 0.95{]} is the ``panic'' rate during regime
changes - k in {[}5, 10{]} controls sigmoid steepness - V\_0 in {[}0.1,
0.3{]} is the volatility threshold

\subsubsection{3.4 Activation Function
Selection}\label{activation-function-selection}

A natural question is whether the sigmoid activation in Equation (3) is
justified over alternatives. We conducted a systematic ablation
(Experiment 37, Section 5.7) testing five activation
functions---sigmoid, linear, exponential, step, and softplus---across k
values \{1, 2, 3, 5, 10\}.

\textbf{Key finding}: At the recommended k \textgreater= 5, \textbf{all
activations converge to equivalent performance} (Cohen's d
-\textgreater{} 0 between any pair). The choice of activation function
is immaterial at recommended operating parameters.

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llllll@{}}
\toprule\noalign{}
k & Sigmoid & Linear & Exponential & Step & Softplus \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 14.44 & 12.90 & 15.81 & 11.08 & 11.08 \\
2 & 13.28 & 11.08 & 13.47 & 11.08 & 11.08 \\
3 & 12.24 & 11.08 & 12.35 & 11.08 & 11.08 \\
5 & 11.25 & 11.08 & 11.25 & 11.08 & 11.08 \\
10 & 11.08 & 11.08 & 11.08 & 11.08 & 11.08 \\
\end{longtable}
}

\emph{Table: Mean IAE across 4 drift scenarios by activation function
and k value (n=30). All activations converge at k \textgreater= 5.}

At k=1, step and softplus outperform sigmoid (d = -1.052), and linear
outperforms sigmoid (d = -0.518). However, this low-k regime produces
sluggish transitions that are undesirable in practice.

We retain sigmoid for three reasons despite its equivalent performance
at recommended k: 1. \textbf{Bounded output}: sigmoid maps to {[}0, 1{]}
without clamping, preventing numerical issues. 2. \textbf{Interpretable
threshold}: V\_0 has a clear meaning as the sigmoid midpoint, directly
corresponding to the ``volatility level at which decay is
half-activated.'' 3. \textbf{Smooth gradient}: Unlike step
(discontinuous) or exponential (unbounded), sigmoid provides smooth,
monotonic transitions suitable for gradient-based extensions.

\subsubsection{3.5 Algorithm}\label{algorithm}

\begin{verbatim}
Algorithm: Volatility-Driven Decay Retrieval
Input: Query q, memory bank M, window sizes w_curr, w_arch
Output: Retrieved documents D

1. e_t <- embed(q)
2. mu_curr <- centroid(recent[t - w_curr : t])
3. mu_arch <- centroid(archival[t - w_arch : t - w_curr])
4. V_t <- 1 - cos(mu_curr, mu_arch)
5. lambda(t) <- lambda_base + (lambda_max - lambda_base) * sigma(k * (V_t - V_0))
6. For each m in M:
     S(m) <- Sim(q, m) * exp(-lambda(t) * (t - t_m))
7. D <- top_k(M, S)
8. Return D
\end{verbatim}

\subsubsection{3.6 Hash-Based Embeddings and Real-Embedding
Transfer}\label{hash-based-embeddings-and-real-embedding-transfer}

Our synthetic experiments use hash-based embeddings (SHA-256
-\textgreater{} 64-dim vectors with Gaussian noise) rather than neural
embeddings. This is a deliberate methodological choice:

\begin{itemize}
\tightlist
\item
  \textbf{Deterministic}: Identical inputs produce identical embeddings
  across runs, ensuring exact reproducibility without API dependencies.
\item
  \textbf{Isolates the mechanism}: Hash embeddings test VDD's drift
  detection and decay logic without conflating results with embedding
  model quality.
\end{itemize}

\textbf{Validation via Experiment 36}: We conducted a systematic
comparison of hash-based (64-dim) vs.~real neural embeddings
(nomic-embed-text, 768-dim) across 5 drift scenarios, 5 methods, and
n=30 seeds. Results:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Metric & Value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Ranking agreement & 4/5 scenarios \\
Effect size correlation & r = 0.935 \\
Mean effect size ratio (real/hash) & 0.70x \\
\end{longtable}
}

Rankings are preserved in 4/5 scenarios between hash and real
embeddings. The one disagreement occurs in the reversion scenario where
all methods produce similar accuracy with real embeddings. Real
embeddings produce slightly lower absolute accuracy (delta -0.02 to
-0.05 typical), and---critically---\textbf{effect sizes attenuate} with
real embeddings (mean ratio 0.70x), indicating that hash-based
controlled-condition results are directionally valid but inflated.

\textbf{Summary of real-embedding accuracy across scenarios (n=30)}:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llllll@{}}
\toprule\noalign{}
Scenario & VDD & Recency & Static & Time-Weighted & No Decay \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime shifts & 0.584 & 0.595 & 0.748 & 0.871 & 0.333 \\
Mixed drift & 0.584 & 0.595 & 0.748 & 0.871 & 0.333 \\
Bursty & 0.594 & 0.607 & 0.757 & 0.902 & 0.333 \\
Reversion & 0.563 & 0.563 & 0.580 & 0.571 & 0.667 \\
Gradual & 0.586 & 0.598 & 0.747 & 0.845 & 0.333 \\
\end{longtable}
}

\textbf{Honest assessment}: With real embeddings, time-weighted
consistently dominates (0.845--0.902), and static decay outperforms both
VDD and recency in absolute accuracy. VDD and recency perform similarly
(delta \textless{} 0.02). The reversion scenario is the closest, with
no\_decay performing best (0.667) due to the scenario structure.
Synthetic results demonstrate the mechanism; real-embedding experiments
provide ground-truth validation that shows attenuated but directionally
consistent effects.

\subsubsection{3.7 Theoretical Analysis}\label{theoretical-analysis}

The empirical finding that VDD is ``never the worst'' across drift
patterns suggests an underlying structural property. In this section we
provide a theoretical sketch---not a formal proof---that formalizes this
intuition using a regret framework. The experimental results (42
experiments) remain the primary evidence; this analysis offers
theoretical motivation for why adaptive decay is structurally preferable
to any fixed rate under uncertainty.

\paragraph{3.7.1 Regret Framework}\label{regret-framework}

Let \texttt{R(lambda,\ D)} denote the cumulative retrieval error (e.g.,
integrated absolute error) incurred by a fixed decay rate
\texttt{lambda} on drift pattern \texttt{D}. An oracle with
foreknowledge of \texttt{D} selects the optimal fixed rate
\texttt{lambda*(D)\ =\ argmin\_lambda\ R(lambda,\ D)}. VDD's
time-varying \texttt{lambda(t)} produces error \texttt{R\_VDD(D)} for
any pattern \texttt{D}. We define regret as the excess error over the
oracle:

\begin{verbatim}
Regret(D) = R_VDD(D) - R(lambda*(D), D)
\end{verbatim}

A method is robust if its worst-case regret across all patterns is
bounded. A method is ``never worst'' if, for every pattern \texttt{D},
there exists some fixed-lambda baseline with higher error.

\paragraph{3.7.2 Worst-Case Regret Bound}\label{worst-case-regret-bound}

Any fixed \texttt{lambda\_fixed} is arbitrarily bad for some drift
pattern:

\begin{itemize}
\tightlist
\item
  \textbf{Low lambda (conservative)}: Under sudden drift, stale memories
  persist. Staleness accumulates linearly with time, so
  \texttt{R(lambda\_fixed,\ D\_sudden)} grows without bound as drift
  frequency increases.
\item
  \textbf{High lambda (aggressive)}: Under reversion (knowledge returns
  to a previous state), valid memories are destroyed prematurely. Our
  experiments confirm this: aggressive decay (\texttt{lambda\ =\ 0.5})
  produces IAE 52.1 vs.~VDD's 35.4 in reversion scenarios.
\end{itemize}

VDD's regret is bounded because its output is structurally constrained.
For a drift detector with false positive rate \texttt{alpha} and true
positive rate (recall) \texttt{beta}, the worst-case excess lambda at
any timestep is bounded by:

\begin{verbatim}
|lambda(t) - lambda*| <= (lambda_max - lambda_base)
\end{verbatim}

This worst case occurs only when the detector is wrong. At each
timestep, the expected excess decay decomposes into two error modes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Missed drift} (probability \texttt{1\ -\ beta}): VDD stays
  near \texttt{lambda\_base} when it should be near
  \texttt{lambda\_max}, contributing excess staleness of at most
  \texttt{(lambda\_max\ -\ lambda\_base)\ *\ (1\ -\ beta)}.
\item
  \textbf{False alarm} (probability \texttt{alpha}): VDD activates near
  \texttt{lambda\_max} when it should stay near \texttt{lambda\_base},
  contributing excess forgetting of at most
  \texttt{(lambda\_max\ -\ lambda\_base)\ *\ alpha}.
\end{enumerate}

Over a time horizon \texttt{T}, this yields:

\begin{verbatim}
max_D Regret_VDD(D) <= T * (lambda_max - lambda_base) * [(1 - beta) + alpha]
\end{verbatim}

With our detector's measured rates (\texttt{alpha\ =\ 0.135},
\texttt{beta\ =\ 1.0}), the per-step excess is bounded by
\texttt{(0.9\ -\ 0.05)\ *\ (0\ +\ 0.135)\ =\ 0.115}. Compare this to a
mismatched fixed lambda, where per-step excess can reach
\texttt{(lambda\_max\ -\ lambda\_base)\ =\ 0.85}---over 7x worse.

\paragraph{3.7.3 Minimax Optimality
Intuition}\label{minimax-optimality-intuition}

VDD approximates the \textbf{minimax strategy}: minimize the maximum
regret across all possible drift patterns. Formally, VDD targets:

\begin{verbatim}
min_strategy max_D Regret(strategy, D)
\end{verbatim}

Any fixed-lambda strategy has unbounded maximum regret (there always
exists a pathological drift pattern). VDD's adaptive strategy bounds the
maximum regret at the cost of not being optimal for any single known
pattern. This is precisely the ``never worst'' property: VDD sacrifices
best-case performance for worst-case robustness.

Our experiments quantify this tradeoff. Fixed strategies show 5--20x IAE
variation across drift patterns (e.g., aggressive decay: IAE 11.1 on
sudden drift vs.~52.1 on reversion). VDD's IAE varies by less than 3x
across the same patterns (range: 11.8--35.4), confirming bounded
worst-case behavior at the expense of optimality in any single scenario.

\paragraph{3.7.4 Detector Convergence}\label{detector-convergence}

The centroid distance \texttt{V\_t\ =\ 1\ -\ cos(mu\_curr,\ mu\_arch)}
is a consistent estimator of distributional shift. By the law of large
numbers, as window sizes grow:

\begin{verbatim}
mu_curr -> E[e_t | recent]     (a.s.)
mu_arch -> E[e_t | archival]   (a.s.)
\end{verbatim}

so \texttt{V\_t\ -\textgreater{}\ V*}, the true cosine distance between
population means. By the central limit theorem, the convergence rate is
\texttt{O(1\ /\ sqrt(min(w\_curr,\ w\_arch)))}. With default windows
\texttt{w\_curr\ =\ 20}, \texttt{w\_arch\ =\ 100}, the estimation error
is approximately \texttt{O(1/sqrt(20))\ \textasciitilde{}\ 0.22}
standard deviations---sufficient for the binary high/low distinction VDD
requires, but not for fine-grained volatility estimation. This
asymptotic guarantee holds under finite second moments of the embedding
distribution; finite-window behavior depends on the concentration
properties of the specific embedding model.

\paragraph{3.7.5 Limitations of This
Analysis}\label{limitations-of-this-analysis}

We are explicit about what this analysis does and does not establish:

\begin{itemize}
\tightlist
\item
  \textbf{This is a sketch, not a formal proof.} The regret bound
  assumes the detector's error rates (\texttt{alpha}, \texttt{beta}) are
  known and stationary. Real deployments face non-stationary false
  positive rates that could vary with data distribution.
\item
  \textbf{The bound is loose.} Tighter analysis would require
  distributional assumptions about drift patterns (e.g., bounded drift
  frequency, minimum inter-drift intervals). We avoid such assumptions
  to maintain generality.
\item
  \textbf{Minimax optimality is claimed as intuition, not proven.} A
  formal minimax proof would require characterizing the class of
  admissible drift patterns and showing VDD is optimal within that
  class.
\item
  \textbf{The detector convergence result is asymptotic.} With
  \texttt{w\_curr\ =\ 20}, finite-sample effects are non-negligible, and
  the Gaussian approximation may not hold for all embedding
  distributions.
\end{itemize}

We provide this analysis to explain \emph{why} VDD's ``never worst''
property is structurally expected, not coincidental. The experimental
results across 42 experiments, 120 real-world facts, and 13 comparison
methods remain the primary evidence for VDD's practical value.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{4. Experimental Validation}\label{experimental-validation}

We conducted 42 experiments; key results from 37 experiments are
presented here (Experiments 34--42 are new in this revision). All
experiments use bootstrap 95\% CIs (1000 samples) and Cohen's d effect
sizes; n=30 experiments use explicit seeds 100--129. All p-values are
corrected for multiple comparisons using Benjamini-Hochberg FDR (Section
4.1).

\subsubsection{4.1 Statistical
Methodology}\label{statistical-methodology}

\paragraph{Sample Sizes and Inference}\label{sample-sizes-and-inference}

\begin{itemize}
\tightlist
\item
  \textbf{Sample sizes}: n = 30 for experiments 19--37; n = 10--20 for
  experiments 1--18 with confirmatory n = 30 reruns.
\item
  \textbf{Bootstrap CIs}: 95\% CI via 1000 bootstrap samples (percentile
  method).
\item
  \textbf{Cohen's d}: Small \textbar d\textbar{} \textless{} 0.5, medium
  0.5 \textless= \textbar d\textbar{} \textless{} 0.8, large
  \textbar d\textbar{} \textgreater= 0.8.
\item
  \textbf{Paired comparisons}: Paired by random seed to reduce variance.
\end{itemize}

\paragraph{Multiple Comparison Correction (Experiment
34)}\label{multiple-comparison-correction-experiment-34}

Across all experiments, we conducted \textbf{88 hypothesis tests}.
Without correction, 85 tests achieved p \textless{} 0.05. After applying
Benjamini-Hochberg FDR correction at alpha = 0.05:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Metric & Value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Total comparisons & 88 \\
Significant (uncorrected) & 85 \\
Significant (BH-corrected) & 85 \\
Lost to correction & 0 \\
FDR alpha & 0.05 \\
\end{longtable}
}

All 85 significant results survive correction. The three non-significant
comparisons were already non-significant before correction.

We designate \textbf{5 primary comparisons} (pre-registered hypotheses)
and \textbf{83 exploratory comparisons} (post-hoc analyses):

\textbf{Primary comparisons:} 1. VDD achieves lower IAE than static
baselines in regime shifts 2. VDD achieves lower staleness than recency
in real-world data 3. VDD's lambda is genuinely bimodal (not static with
noise) 4. VDD ranks consistently \#2/10 across drift patterns 5. Real
embeddings preserve VDD's ranking advantage

All 5 primary comparisons are significant after BH correction.

\paragraph{Type M Error Analysis}\label{type-m-error-analysis}

With n=30, our study is adequately powered to detect large effects but
may exaggerate smaller ones. We report Type M (magnitude) error analysis
following Gelman and Carlin (2014):

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Assumed True d & Power (n=30) & Exaggeration Ratio & Type S Rate \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.2 (small) & 11.4\% & 3.21x & 2.9\% \\
0.3 & 20.1\% & 2.21x & 0.5\% \\
0.5 (medium) & 47.3\% & 1.43x & 0.0\% \\
0.8 (large) & 86.3\% & 1.08x & 0.0\% \\
1.0 & 96.8\% & 1.02x & 0.0\% \\
\end{longtable}
}

\textbf{Interpretation}: If the true effect of VDD vs.~a baseline is
medium (d=0.5), our study would detect it 47\% of the time and
exaggerate the observed d by 1.43x when significant. For the large
effects we frequently observe (d \textgreater{} 1.0), exaggeration is
minimal (\textless{} 1.02x). However, for comparisons where the true
effect is small (d \textasciitilde{} 0.2), any significant result would
be inflated approximately 3.2x. \textbf{We caution readers that observed
effect sizes in controlled simulations should be interpreted as upper
bounds on production deployment effects.}

\subsubsection{4.2 Core Validation: Error Reduction and Ranking
(Experiments 2,
8)}\label{core-validation-error-reduction-and-ranking-experiments-2-8}

Using Embedding Distance detection (not oracle), VDD achieves
\textbf{87.5\% error reduction} versus static decay (IAE 15.81 vs
126.67) with regime shifts detected in real-time.

Against 10 practitioner-relevant baselines (n = 30), VDD ranks \#2 in
all four drift patterns with 100\% rank stability:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Method & Regime Shifts & Mixed Drift & Bursty & Reversion \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Recency (lambda=0.5)} & \textbf{36.5} & \textbf{40.2} &
\textbf{37.5} & \textbf{36.4} \\
\textbf{VDD} & \textbf{63.6} & \textbf{43.3} & \textbf{69.5} &
\textbf{62.2} \\
Static (lambda=0.2) & 104.8 & 111.2 & 109.5 & 103.7 \\
Static (lambda=0.1) & 195.2 & 198.4 & 207.2 & 190.7 \\
SlidingWindow (N=20) & 225.2 & 238.9 & 233.5 & 222.1 \\
TimeWeighted (alpha=0.05) & 351.9 & 343.8 & 385.0 & 339.2 \\
SlidingWindow (N=50) & 472.3 & 468.9 & 501.4 & 457.6 \\
TimeWeighted (alpha=0.02) & 695.3 & 673.7 & 827.1 & 643.7 \\
SlidingWindow (N=100) & 822.6 & 795.0 & 901.8 & 784.9 \\
TimeWeighted (alpha=0.01) & 1003.2 & 986.1 & 1393.4 & 853.0 \\
\end{longtable}
}

\emph{Table: IAE across 10 methods and 4 drift patterns (n = 30). Bold =
top 2.}

VDD outperforms time-weighted methods by 5--20x and sliding windows by
3--13x. In mixed drift, VDD nearly matches recency (43.3 vs 40.2, 7.7\%
gap).

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 2: VDD with real-time Embedding Distance detection vs.~static decay, showing 87.5\% error reduction.}]{results/02_simulation_real.png}}
\caption{Figure 2: VDD with real-time Embedding Distance detection
vs.~static decay, showing 87.5\% error reduction.}
\end{figure}

\subsubsection{4.3 Robustness Across Drift Patterns and Decision
Framework (Experiments 4, 15, 16, 17, 20,
28)}\label{robustness-across-drift-patterns-and-decision-framework-experiments-4-15-16-17-20-28}

VDD's consistency across scenarios is its primary practical value:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2833}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2167}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Drift Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
VDD Performance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best Alternative
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
VDD Verdict
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Mixed/Unknown & IAE 43.3, \#2/10 & Recency 40.2 & \textbf{Recommended}
(7.7\% gap, safest default) \\
Reversion & IAE 35.4, d = +1.22 vs recency & --- & \textbf{Recommended}
(clear win) \\
Bursty permanent & IAE 48.2, d = -2.90 vs recency & Recency 28.7 & Not
recommended \\
Gradual predictable & Acc 0.621 & Time-weighted 0.899 & Not
recommended \\
Stable environment & Near-baseline lambda & Static lambda
\textasciitilde{} 0.1 & Acceptable, unnecessary \\
\end{longtable}
}

\textbf{Honest Finding}: Recency outperforms VDD in permanent high-drift
(d = -2.90). VDD's value is for the common case where drift pattern is
unknown and may include reversions.

\textbf{Full practitioner decision framework}:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4167}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Condition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Recommendation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Evidence
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Drift pattern unknown/mixed & \textbf{VDD} & \#2/13, never worst \\
Avoiding stale knowledge critical & \textbf{VDD} & d = +1.85 vs
recency \\
Reversions possible & \textbf{VDD} & d = +1.22 vs recency \\
Tuned parameters available & \textbf{VDD (cw=3, aw=200, V\_0=0.1)} & IAE
12.6 \\
Constant high drift & Recency (lambda \textgreater= 0.5) & Simplest,
\#1 \\
Gradual, predictable transitions & Time-weighted (alpha = 0.01) & 0.899
accuracy \\
Known stable environment & Static (lambda \textasciitilde{} 0.1) &
Maintains trust \\
Time series domain & Holt-Winters & Competitive in 3/4 \\
\textbf{Avoid} & Sliding window (N \textgreater{} 50) &
\textbf{Catastrophic failures} \\
\textbf{Avoid} & Time-weighted on sudden drift & \textbf{5--20x
worse} \\
\end{longtable}
}

Figure 2 provides a visual decision tree summarizing this framework,
guiding practitioners from their use-case characteristics to the
recommended decay method.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Practitioner decision tree for decay method selection. VDD is highlighted as the safest default when drift patterns are unknown or mixed.}]{results/42_decision_tree.png}}
\caption{Practitioner decision tree for decay method selection. VDD is
highlighted as the safest default when drift patterns are unknown or
mixed.}
\end{figure}

\subsubsection{4.4 Bimodality: Proof of Genuine Adaptation (Experiments
19, 30)}\label{bimodality-proof-of-genuine-adaptation-experiments-19-30}

A critical question is whether VDD's lambda is genuinely bimodal or
merely static with noise. We logged lambda(t) across five scenarios and
applied four statistical tests (n = 30):

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1471}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2059}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1618}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1324}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Bimod. Coeff.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dip Test p
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ashman's D
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Silverman
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Verdict
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Pure Stable & 0.578 & 0.021 & 0.0001 & 1 mode & Unimodal* \\
Pure Drift & 0.884 & \textless{} 0.001 & 2.878 & 2 modes &
\textbf{Bimodal} \\
Mixed (70/30) & 0.954 & \textless{} 0.001 & 2.420 & 2 modes &
\textbf{Bimodal} \\
Bursty & 0.981 & \textless{} 0.001 & 2.130 & 2 modes &
\textbf{Bimodal} \\
Reversion & 0.964 & \textless{} 0.001 & 2.262 & 2 modes &
\textbf{Bimodal} \\
\end{longtable}
}

\emph{*Pure stable passes BC but fails Ashman's D---BC alone is
insufficient.}

VDD beats a static oracle calibrated to its own mean lambda in mixed
drift (113.8 vs 362.8, 3.2x improvement), proving that the \emph{timing}
of elevated lambda matters beyond the mean value.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 3: Effective lambda distributions across five drift scenarios, confirming genuine bimodality.}]{results/21_effective_lambda.png}}
\caption{Figure 3: Effective lambda distributions across five drift
scenarios, confirming genuine bimodality.}
\end{figure}

\subsubsection{4.5 Extended Baselines: Original Adaptive Methods
(Experiment
29)}\label{extended-baselines-original-adaptive-methods-experiment-29}

We compared VDD against adaptive methods from the time series
literature: EMA-lambda and Holt-Winters exponential smoothing (Holt,
2004), plus Dynamic Weighted Majority (Kolter \& Maloof, 2007).

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Method & Regime & Mixed & Bursty & Reversion \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Recency (0.5)} & \textbf{1} & \textbf{1} & \textbf{1} &
\textbf{1} \\
Holt-Winters & 2 & 4 & 2 & 2 \\
EMA-lambda (alpha=0.2) & 3 & 3 & 3 & 3 \\
EMA-lambda (alpha=0.1) & 4 & 5 & 4 & 4 \\
\textbf{VDD} & 5 & \textbf{2} & 5 & 5 \\
DWM-lite & 6 & 6 & 6 & 6 \\
Static (0.1) & 7 & 7 & 7 & 7 \\
\end{longtable}
}

\emph{Table: Ranks against adaptive baselines (n = 30). Bold = notable.}

\textbf{Honest Finding}: Holt-Winters and EMA-lambda beat VDD in 3/4
scenarios. However, VDD excels in mixed drift (\#2 vs Holt-Winters'
\#4), the most common production condition. VDD always beats DWM-lite
and static, maintaining its never-worst property.

\subsubsection{4.6 Extended Baselines: Additional Methods (Experiment
37, Part
2)}\label{extended-baselines-additional-methods-experiment-37-part-2}

Responding to reviewer feedback on missing baselines, we added three
additional comparison methods: \textbf{LRU} (Least Recently Used
eviction), \textbf{Timestamp Freshness} (scoring by inverse age), and
\textbf{Online Lambda} (an adaptive decay method that learns lambda from
prediction error). All methods were tested at k=5 across four drift
scenarios (n=30):

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llllll@{}}
\toprule\noalign{}
Method & Regime & Mixed & Bursty & Gradual & Mean Rank \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Online Lambda} & 4.03 & 13.07 & 2.33 & 24.90 & \textbf{1.0} \\
\textbf{VDD (sigmoid)} & 4.37 & 13.30 & 2.40 & 24.93 & \textbf{2.0} \\
No Decay & 6.90 & 20.57 & 2.87 & 40.57 & 3.5 \\
LRU & 6.90 & 20.57 & 2.87 & 40.57 & 4.5 \\
Recency & 13.23 & 24.77 & 7.70 & 35.03 & 5.0 \\
Timestamp Freshness & 12.13 & 29.47 & 6.53 & 37.23 & 5.0 \\
Time-Weighted & 15.27 & 38.53 & 7.83 & 55.20 & 7.0 \\
Static & 16.47 & 44.50 & 8.63 & 73.00 & 8.0 \\
\end{longtable}
}

\emph{Table: IAE across 8 methods and 4 drift patterns with extended
baselines (n=30, k=5). Lower IAE is better.}

\textbf{VDD vs.~new baselines (Cohen's d, positive = VDD better):}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llllll@{}}
\toprule\noalign{}
Baseline & Mean d & Regime & Mixed & Bursty & Gradual \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
LRU & +0.921 & +0.888 & +1.350 & +0.228 & +1.218 \\
Timestamp Freshness & +2.435 & +3.151 & +2.985 & +2.368 & +1.238 \\
Online Lambda & -0.056 & -0.129 & -0.054 & -0.038 & -0.003 \\
\end{longtable}
}

VDD substantially outperforms LRU (d = +0.921, large effect) and
timestamp freshness (d = +2.435, very large effect). The online\_lambda
baseline---which also adapts its decay rate based on prediction
error---matches VDD almost exactly (d = -0.056, negligible effect). This
near-equivalence \textbf{validates the core insight that adaptive decay
is valuable}, while showing that the specific mechanism matters less
than the decision to adapt at all.

However, the two approaches differ substantially in deployment
requirements:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3548}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1613}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4839}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criterion
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
VDD
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Online Lambda
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Requires ground-truth labels & \textbf{No} (unsupervised) & Yes
(prediction error needs targets) \\
Cold-start performance & \textbf{Good} (sigmoid defaults to low decay) &
Poor (no error signal yet) \\
Computational overhead & \textbf{O(d)} per update & O(d) + prediction
step \\
Interpretability & \textbf{High} (V\_0 threshold, sigmoid curve) & Low
(learned rate, opaque dynamics) \\
Accuracy (d vs VDD) & Baseline & -0.056 (equivalent) \\
\end{longtable}
}

We retain VDD's embedding-distance approach because it requires no
labeled data, operates stably from cold start, and provides
interpretable decay dynamics---advantages that matter in production RAG
systems where ground-truth feedback is rarely available.

\subsubsection{4.7 Multi-Domain Real-World Validation (Experiments 21,
25, 31)}\label{multi-domain-real-world-validation-experiments-21-25-31}

We validated on \textbf{120 facts across 3 domains}---React (60 facts,
v16/v17/v18), Python stdlib (30 facts, v3.8/v3.10/v3.12), and Node.js
(30 facts, v16/v18/v20)---using real embeddings (nomic-embed-text,
768-dim) via Ollama (n = 30):

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1481}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1296}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1481}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2037}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2037}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
React
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Python
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Node.js
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Aggregate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Staleness
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Time-Weighted} & \textbf{0.871} & \textbf{0.879} &
\textbf{0.876} & \textbf{0.875} & \textbf{0.000} \\
Static (lambda=0.1) & 0.748 & 0.738 & 0.721 & 0.736 & 0.135 \\
\textbf{VDD} & \textbf{0.647} & \textbf{0.632} & \textbf{0.605} &
\textbf{0.628} & \textbf{0.243} \\
Recency (lambda=0.5) & 0.595 & 0.577 & 0.540 & 0.570 & 0.300 \\
Sliding Window & 0.595 & 0.333* & 0.333* & 0.421 & 0.398 \\
No Decay & 0.333 & 0.333 & 0.333 & 0.333 & 0.667 \\
\end{longtable}
}

\emph{Table: Three-domain validation (n = 30, 120 facts, real
embeddings). *Sliding window catastrophically fails on Python and
Node.js.}

\textbf{Key findings}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{VDD is cross-domain consistent}: accuracy 0.605--0.647 across
  all three domains.
\item
  \textbf{VDD beats recency in all 3 domains} (aggregate d = +2.13,
  large effect) with lower staleness (0.243 vs 0.300).
\item
  \textbf{Real embeddings strengthen VDD}: accuracy improves 0.617
  -\textgreater{} 0.647 and staleness drops 0.309 -\textgreater{} 0.221
  versus hash embeddings.
\item
  \textbf{Sliding window is catastrophically fragile}: 0.333 on Python
  and Node.js (equivalent to random).
\end{enumerate}

\textbf{Honest Finding}: Time-weighted dominates on gradual transitions
(0.875 accuracy). When version changes are predictable, fixed
time-weighted decay is superior.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 4: Three-domain real-world validation (n=30, 120 facts, real embeddings).}]{results/33_three_domain.png}}
\caption{Figure 4: Three-domain real-world validation (n=30, 120 facts,
real embeddings).}
\end{figure}

\subsubsection{4.8 Real Embedding Suite: Hash vs.~Neural Transfer
(Experiment
36)}\label{real-embedding-suite-hash-vs.-neural-transfer-experiment-36}

To directly address concerns about synthetic data validity, we conducted
a systematic comparison of hash-based (64-dim) versus real neural
embeddings (nomic-embed-text, 768-dim) across 5 drift scenarios, 5
methods, and n=30 seeds (300 timesteps each, 136.5 seconds total
computation):

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Scenario & Metric & Hash & Real & Delta \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime shifts & VDD accuracy & 0.619 & 0.584 & -0.035 \\
Regime shifts & Time-weighted accuracy & 0.891 & 0.871 & -0.020 \\
Bursty & VDD accuracy & 0.627 & 0.594 & -0.033 \\
Bursty & Time-weighted accuracy & 0.897 & 0.902 & +0.005 \\
Reversion & VDD accuracy & 0.535 & 0.563 & +0.028 \\
Gradual & VDD accuracy & 0.607 & 0.586 & -0.021 \\
\end{longtable}
}

\textbf{Ranking preservation}: Method rankings (which method beats
which) are preserved in 4/5 scenarios. The one disagreement occurs in
the reversion scenario where effect sizes collapse and all methods
produce similar accuracy.

\textbf{Effect size attenuation}: Real embeddings produce effect sizes
that are 0.70x the hash-based values on average (r = 0.935 correlation).
This means: - A hash-based d = 3.0 corresponds to approximately d = 2.1
with real embeddings - A hash-based d = 1.0 corresponds to approximately
d = 0.7 with real embeddings - Directional conclusions remain valid;
magnitudes should be interpreted as upper bounds

\textbf{Interpretation}: Hash embeddings serve as a ``conservative
mechanism test''---they isolate VDD's drift detection and decay logic
from embedding model quality. Real embeddings provide ground truth and
show that the mechanism works with production-quality embeddings, though
with attenuated effect sizes. Under controlled conditions with low
variance, effect sizes are inflated; production deployments would show
smaller but directionally consistent improvements.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 5: Hash vs.~real embedding comparison across 5 drift scenarios (n=30, r=0.935 effect size correlation).}]{results/36_real_embedding_suite.png}}
\caption{Figure 5: Hash vs.~real embedding comparison across 5 drift
scenarios (n=30, r=0.935 effect size correlation).}
\end{figure}

\paragraph{Cross-Model Embedding Generalization (Experiment
41)}\label{cross-model-embedding-generalization-experiment-41}

To test whether VDD's results depend on the specific embedding model, we
repeated the 5-scenario evaluation with a second model:
mxbai-embed-large (1024-dim) alongside nomic-embed-text (768-dim).
Across 5 scenarios, 5 methods, and 10 seeds:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Metric & Value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Spearman rank correlation & rho = 0.978 (p \textless{} 1e-17) \\
Exact ranking agreement & 4/5 scenarios \\
Pearson correlation (accuracy) & r = 0.999 \\
Effect size correlation & r = 0.937 \\
\end{longtable}
}

Method rankings are identical in 4/5 scenarios. The one disagreement is
again the reversion scenario, where VDD and recency swap adjacent ranks
(both within 0.001 accuracy). Absolute accuracy deltas between models
are small (mean \textbar delta\textbar{} = 0.011), confirming that VDD's
mechanism generalizes across embedding architectures.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 12: Cross-model embedding generalization (nomic-embed-text vs mxbai-embed-large).}]{results/41_embedding_generalization.png}}
\caption{Figure 12: Cross-model embedding generalization
(nomic-embed-text vs mxbai-embed-large).}
\end{figure}

\subsubsection{4.9 Activation Function Ablation (Experiment 37, Part
1)}\label{activation-function-ablation-experiment-37-part-1}

We tested five activation functions (sigmoid, linear, exponential, step,
softplus) across k values \{1, 2, 3, 5, 10\} on four drift scenarios
(n=30, 500 timesteps):

\textbf{Cohen's d: Sigmoid vs.~Alternative (negative = alternative
better)}:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llllll@{}}
\toprule\noalign{}
Alternative & k=1 & k=2 & k=3 & k=5 & k=10 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Linear & -0.518 & -0.669 & -0.350 & -0.056 & 0.000 \\
Exponential & +0.385 & +0.087 & +0.033 & 0.000 & 0.000 \\
Step & -1.052 & -0.669 & -0.350 & -0.056 & 0.000 \\
Softplus & -1.052 & -0.669 & -0.350 & -0.056 & 0.000 \\
\end{longtable}
}

At k \textgreater= 5, \textbf{all activations converge} (d
-\textgreater{} 0, equivalent performance). The convergence occurs
because high k values produce near-binary switching behavior regardless
of the activation shape: once k is large enough, the sigmoid effectively
becomes a step function, and all smooth activations approach the same
behavior.

At k = 1, step/softplus substantially outperform sigmoid (d = -1.052).
However, k = 1 produces sluggish decay transitions and is outside the
recommended operating range.

\textbf{Conclusion}: Activation choice is immaterial at recommended k.
We retain sigmoid for interpretability, bounded output, and smooth
gradients (Section 3.4).

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 6: Activation function ablation and extended baselines (Experiment 37).}]{results/37_activation_and_baselines.png}}
\caption{Figure 6: Activation function ablation and extended baselines
(Experiment 37).}
\end{figure}

\subsubsection{4.10 LLM-in-the-Loop Validation (Experiments 22, 26,
35)}\label{llm-in-the-loop-validation-experiments-22-26-35}

To validate that proxy metrics predict end-to-end quality, we replaced
deterministic scoring with actual LLM generation (llama3.1:8b) and real
embeddings (nomic-embed-text). We expanded from the original n=5 to n=50
seeds (7,500 total LLM calls):

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Method & LLM Quality (n=50) & 95\% CI & SD & Deterministic Quality \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{VDD} & \textbf{0.522} & {[}0.522, 0.522{]} & 0.000 & 0.567 \\
\textbf{Recency} & \textbf{0.522} & {[}0.522, 0.522{]} & 0.000 &
0.567 \\
Time-weighted & 0.521 & {[}0.511, 0.533{]} & 0.038 & 1.000 \\
Static & 0.490 & {[}0.479, 0.500{]} & 0.038 & 1.000 \\
No decay & 0.473 & {[}0.473, 0.473{]} & 0.000 & 0.333 \\
\end{longtable}
}

The aggregate LLM-deterministic correlation is r = 0.271 (per-seed mean
r = 0.202, 95\% CI {[}0.075, 0.328{]}), validating the need for
end-to-end evaluation. VDD and recency produce identical LLM quality
scores (both 0.522), reflecting identical retrieval outputs in this
regime-shift scenario. All decay-aware methods outperform no-decay
(0.473), confirming the core hypothesis that forgetting improves
retrieval quality.

\textbf{Notable finding}: VDD and recency's zero variance across 50
seeds indicates deterministic retrieval behavior---these methods
consistently retrieve the same documents regardless of random seed,
while static and time-weighted show stochastic variation. The low
LLM-deterministic correlation (r = 0.271) highlights the gap between
proxy metrics and end-to-end evaluation, a known challenge in RAG
evaluation (Es et al., 2023).

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 7: LLM-in-the-loop validation (n=50 seeds, 7,500 LLM calls).}]{results/35_llm_validation_expanded.png}}
\caption{Figure 7: LLM-in-the-loop validation (n=50 seeds, 7,500 LLM
calls).}
\end{figure}

\subsubsection{4.11 Hyperparameter
Sensitivity}\label{hyperparameter-sensitivity}

Window sizes and sigmoid parameters substantially affect VDD
performance. Key findings:

\begin{itemize}
\tightlist
\item
  \textbf{Window sizes}: curr\_window=3, arch\_window=200 achieves IAE
  12.6, \textbf{beating recency's 23.3} (325\% range across
  configurations).
\item
  \textbf{Sigmoid V\_0}: Default 0.5 ranks 25/36; V\_0 in {[}0.1, 0.3{]}
  consistently best (254\% performance range).
\item
  \textbf{Sigmoid k}: k in {[}5, 10{]} balances responsiveness and
  stability.
\item
  \textbf{Recommended}: lambda\_base in {[}0.2, 0.3{]}, lambda\_max in
  {[}0.9, 0.95{]}, V\_0 in {[}0.1, 0.3{]}, k in {[}5, 10{]}, cw in {[}3,
  5{]}, aw in {[}100, 200{]}.
\end{itemize}

\textbf{Window size ablation (n=30)}:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
curr\_window & arch\_window & IAE (mean +/- std) & vs Default \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{3} & \textbf{200} & \textbf{12.6 +/- 1.4} & \textbf{Best} \\
3 & 100 & 12.8 +/- 1.5 & -61\% \\
5 & 200 & 16.2 +/- 1.3 & -50\% \\
5 & 100 & 16.4 +/- 1.4 & -50\% \\
10 & 200 & 21.4 +/- 1.4 & -35\% \\
3 & 50 & 22.6 +/- 2.0 & -31\% \\
20 & 200 & 25.1 +/- 1.2 & -23\% \\
3 & 20 & 28.3 +/- 2.3 & -13\% \\
50 & 200 & 29.7 +/- 1.6 & -9\% \\
\textbf{5} & \textbf{50} & \textbf{32.7 +/- 2.3} & \textbf{Default} \\
5 & 20 & 38.8 +/- 2.5 & +19\% \\
10 & 50 & 46.7 +/- 2.6 & +43\% \\
10 & 20 & 53.7 +/- 2.7 & +64\% \\
\end{longtable}
}

Smaller curr\_window increases drift sensitivity; arch\_window
\textgreater= 200 saturates.

\textbf{Sigmoid parameter sensitivity (n=30)}:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Scenario & Best Config & IAE & Default IAE & Default Rank \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regime Shifts & k=10, V\_0=0.1 & \textbf{26.2} & 63.6 & 25/36 \\
Mixed Drift & k=5, V\_0=0.1 & \textbf{28.8} & 43.3 & 25/36 \\
Bursty & k=5, V\_0=0.1 & \textbf{34.1} & 69.5 & 25/36 \\
Reversion & k=10, V\_0=0.1 & \textbf{29.7} & 62.2 & 25/36 \\
\end{longtable}
}

V\_0=0.1 is consistently best. Performance range is 254\%.

\textbf{Lambda ablation heatmap (IAE values, lower is better)}:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llllll@{}}
\toprule\noalign{}
lambda\_base ~lambda\_max & 0.50 & 0.70 & 0.80 & 0.90 & 0.95 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.01 & 34.8 & 24.3 & 20.8 & 17.9 & 16.6 \\
0.05 & 32.4 & 22.4 & 19.1 & 16.3 & 15.1 \\
0.10 & 30.0 & 20.6 & 17.3 & 14.7 & 13.5 \\
0.20 & 26.2 & 17.5 & 14.6 & 12.1 & 11.0 \\
0.30 & 23.2 & 15.1 & 12.3 & 10.0 & \textbf{9.0} \\
\end{longtable}
}

Higher values for both parameters consistently improve performance.
Smooth optimization landscape with no local minima.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 8: Sigmoid hyperparameter sensitivity (k vs.~V\_0) across four drift scenarios (n=30).}]{results/29_sigmoid_heatmap.png}}
\caption{Figure 8: Sigmoid hyperparameter sensitivity (k vs.~V\_0)
across four drift scenarios (n=30).}
\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{5. Discussion}\label{discussion}

\subsubsection{5.1 VDD in the Modern RAG
Landscape}\label{vdd-in-the-modern-rag-landscape}

VDD operates at the memory management layer---below retrieval-time
decisions (Self-RAG, CRAG) and above embedding model choices. This
positioning suggests five deployment scenarios:

\textbf{Coding assistant} (mixed drift: gradual deprecations + sudden
breaking changes). Drift pattern unpredictable. -\textgreater{}
\textbf{VDD recommended.} The assistant faces months of gradual API
deprecation followed by sudden major-version breaks. VDD's adaptive
forgetting handles both without manual tuning.

\textbf{News aggregation} (constant high drift). All content becomes
stale quickly. -\textgreater{} \textbf{Recency recommended.} Simple
aggressive forgetting suffices when virtually all stored knowledge
becomes obsolete within hours.

\textbf{Medical knowledge base} (gradual evolution + emergency updates).
Guidelines evolve slowly, but emergency safety alerts require immediate
adoption. -\textgreater{} \textbf{VDD recommended.} The mixed
pattern---stable baseline with occasional urgent updates---is precisely
where VDD's volatility detection provides value.

\textbf{Enterprise documentation} (mostly stable, occasional
reorganizations). Long stable periods punctuated by version migrations.
-\textgreater{} \textbf{VDD recommended.} VDD's low resting decay
preserves stable knowledge while accelerating forgetting during
migrations.

\textbf{Social media trending} (rapid, unpredictable churn).
-\textgreater{} \textbf{Recency recommended.} Content validity is so
short-lived that aggressive forgetting is always appropriate.

\subsubsection{5.2 Positioning VDD Among Contemporary
Systems}\label{positioning-vdd-among-contemporary-systems-1}

VDD occupies a specific niche in the contemporary RAG memory landscape.
Several key distinctions:

\textbf{VDD vs.~ARM} (Bursa, 2026): ARM adapts retrieval \emph{strategy}
(which retrieval approach to use for a given query), while VDD adapts
memory \emph{retention} (how aggressively to discount stored knowledge
over time). ARM operates at query time; VDD operates continuously on the
knowledge base. A system could use ARM to select the retrieval strategy
and VDD to manage the temporal quality of the underlying corpus.

\textbf{VDD vs.~T-GRAG/HippoRAG}: Graph-based approaches (T-GRAG) and
biological memory models (HippoRAG) provide structural representations
of temporal relationships. VDD provides a simpler, complementary
mechanism: rather than restructuring the knowledge representation, VDD
adds a time-varying weight to existing retrieval scores. This makes VDD
easier to integrate into existing RAG pipelines (3 lines of scoring
code) but less capable of complex temporal reasoning.

\textbf{VDD vs.~Mem0/FadeMem}: Both Mem0 and FadeMem implement forms of
memory decay, validating the core premise. VDD's contribution is making
the decay rate \emph{itself} adaptive rather than fixed. Experiment 37
validates this insight directly: online\_lambda (another adaptive
approach) matches VDD (d = -0.056), while static methods fall
significantly behind.

\textbf{VDD vs.~Knowledge Editing (WISE, ROME)}: Knowledge editing
modifies model parameters to update specific facts. VDD modifies
retrieval scores to deprioritize outdated knowledge. For high-priority
individual fact updates (e.g., ``the CEO of X is now Y''), knowledge
editing is more precise. For managing the temporal validity of a large,
evolving knowledge corpus, VDD's automatic adaptation is more practical.

\subsubsection{5.3 Scalability
Considerations}\label{scalability-considerations}

VDD's core computation is O(d) per query (centroid update + cosine
distance + sigmoid), where d is the embedding dimension. For the memory
scoring loop, complexity is O(n * d) where n is the number of memories.
Practical scaling guidance:

\textbf{\textless{} 10,000 memories}: VDD operates within standard RAG
latency budgets. Our benchmark shows 2.75ms mean, 6.48ms P99 for 1,000
memories.

\textbf{10,000--100,000 memories}: The memory scoring loop becomes the
bottleneck. Integration with approximate nearest neighbor indices
(FAISS, ScaNN) is recommended: first retrieve top-k candidates via ANN,
then apply VDD's temporal weighting only to the candidate set. VDD's
overhead reduces to O(k * d) where k \textless\textless{} n.

\textbf{\textgreater{} 100,000 memories}: Batch processing strategies
for volatility updates become necessary. The drift detector can operate
on sampled embeddings (e.g., 1\% sample) without significant detection
loss, since centroid estimation is robust to subsampling. Volatility can
be computed asynchronously and cached between queries.

\textbf{Key point}: VDD's overhead is O(d) per query for drift
detection, independent of corpus size. The O(n) component is the memory
scoring, which is shared with any retrieval system and can be optimized
via ANN indexing.

\subsubsection{5.4 StreamingQA Benchmark Evaluation (Experiment
38)}\label{streamingqa-benchmark-evaluation-experiment-38}

To address the gap in standard benchmark evaluation, we tested VDD on
StreamingQA (Liska et al., 2022), which contains 36,378 temporal QA
pairs spanning 2007--2020. We sampled 1,500 questions stratified by
year, embedded documents using nomic-embed-text, and evaluated five
methods across four temporal checkpoints (2011, 2014, 2017, 2020) with
10 seeds.

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllllll@{}}
\toprule\noalign{}
Method & Overall & 2011 & 2014 & 2017 & 2020 & d(vs VDD) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
no\_decay & \textbf{0.104} & 0.102 & 0.107 & 0.107 & 0.101 & -7.68 \\
time\_weighted & 0.029 & 0.042 & 0.024 & 0.026 & 0.025 & -2.16 \\
static & 0.009 & 0.010 & 0.006 & 0.008 & 0.012 & -0.24 \\
VDD & 0.008 & 0.008 & 0.006 & 0.009 & 0.009 & --- \\
recency & 0.008 & 0.008 & 0.006 & 0.009 & 0.009 & +0.00 \\
\end{longtable}
}

\textbf{VDD loses decisively on this benchmark} (d = -7.68 vs
no\_decay). This is the expected result, and understanding \emph{why} is
instructive.

StreamingQA is a \textbf{knowledge accumulation} benchmark: each
question has a unique answer grounded in a specific news article. New
facts do not replace old facts---they add to the corpus. In this
setting, decay of any kind is counterproductive because discounting old
documents removes correct answers that remain valid indefinitely.
No\_decay preserves all documents and achieves the best retrieval.

VDD is designed for \textbf{knowledge replacement} scenarios: when React
18 deprecates a pattern from React 16, the old answer should lose
weight. StreamingQA has no such replacement structure---a 2008 news fact
about an election result is still correct in 2020.

This result directly validates the decision guide in Section 4.3:
\textbf{do not use VDD (or any decay method) when knowledge accumulates
rather than replaces}. The temporal distance analysis confirms
this---no\_decay scores uniformly across age bands (0-1yr: 0.114, 2-5yr:
0.104, 6+yr: 0.095), while decay methods suppress older correct answers
(VDD 6+yr: 0.004 vs 0-1yr: 0.028).

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 11: StreamingQA benchmark results showing decay methods underperform on knowledge accumulation tasks.}]{results/38_streamingqa_benchmark.png}}
\caption{Figure 11: StreamingQA benchmark results showing decay methods
underperform on knowledge accumulation tasks.}
\end{figure}

\subsubsection{5.5 FreshQA Benchmark Evaluation (Experiment
39)}\label{freshqa-benchmark-evaluation-experiment-39}

To complement StreamingQA (a knowledge-accumulation benchmark where VDD
loses), we evaluated on FreshQA (Vu et al., 2024), which contains 600
temporal questions categorized as fast-changing, slow-changing, and
never-changing. After excluding 148 false-premise questions, we
evaluated 452 questions using nomic-embed-text embeddings (10 seeds,
top-k=3). For each question, we constructed a temporal corpus with
multiple outdated answer versions plus the current answer, simulating
the knowledge-replacement scenario VDD targets.

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1013}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1646}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1772}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2025}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1139}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2405}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fast-Changing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Slow-Changing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Never-Changing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Overall
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
d(overall vs VDD)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
no\_decay & 0.077 & 0.058 & 0.075 & \textbf{0.070} & -9.88 \\
time\_weighted & 0.053 & 0.009 & 0.004 & 0.022 & -0.61 \\
static & 0.044 & 0.021 & 0.013 & 0.026 & -0.76 \\
recency & 0.036 & 0.015 & 0.011 & 0.021 & -0.27 \\
VDD & 0.033 & 0.013 & 0.010 & 0.019 & --- \\
\end{longtable}
}

On raw keyword-overlap scores, no\_decay wins (0.070 vs VDD 0.019).
However, examining \textbf{current-answer retrieval rate} reveals the
mechanism:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Method & Fast & Slow & Never \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
VDD & \textbf{1.000} & \textbf{1.000} & \textbf{1.000} \\
recency & \textbf{1.000} & \textbf{1.000} & \textbf{1.000} \\
static & \textbf{1.000} & \textbf{1.000} & \textbf{1.000} \\
time\_weighted & \textbf{1.000} & \textbf{1.000} & \textbf{1.000} \\
no\_decay & 0.699 & 0.799 & 0.906 \\
\end{longtable}
}

All decay methods retrieve the current answer 100\% of the time, while
no\_decay retrieves it only 70--90\% of the time. The keyword-overlap
metric fails to distinguish between ``retrieved the current answer plus
some outdated ones'' (decay methods) and ``retrieved mostly outdated
answers that happen to share more keywords'' (no\_decay). No\_decay's
higher keyword overlap comes from retrieving \emph{all}
versions---outdated ones that partially match the ground truth
contribute to the F1 score even though they represent stale knowledge.

\textbf{Methodological insight}: This result highlights a known
limitation of keyword-overlap evaluation for temporal QA. A metric that
rewards retrieving only the current answer (e.g., exact match on the
most recent version, or a human evaluation of temporal correctness)
would better capture the decay methods' advantage. We report both
metrics transparently.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio,alt={Figure 13: FreshQA benchmark results showing 100\% current-answer retrieval for decay methods.}]{results/39_freshqa_benchmark.png}}
\caption{Figure 13: FreshQA benchmark results showing 100\%
current-answer retrieval for decay methods.}
\end{figure}

\subsubsection{5.6 VDD's Value
Proposition}\label{vdds-value-proposition}

Our 42 experiments reveal that VDD is \textbf{not universally optimal}
but provides four distinct values:

\textbf{Provably adaptive}: VDD's lambda is genuinely bimodal
(Hartigan's Dip p \textless{} 0.001, Ashman's D \textgreater{} 2) in 4/5
drift-containing scenarios. VDD beats a static oracle calibrated to its
own mean lambda in mixed drift (3.2x improvement), proving that
\emph{timing} of elevated decay matters.

\textbf{Safest default under uncertainty}: Among 13 baselines (expanded
from 10), VDD ranks \#2 across all four drift patterns. Time-weighted
methods fail 5--20x worse on sudden shifts; sliding windows collapse
catastrophically on certain domains; recency fails on reversions (d =
+1.22 VDD advantage).

\textbf{Staleness reduction on real data}: In realistic scenarios with
accumulated legacy trust, VDD achieves significantly lower staleness
than recency (d = +1.85), strengthening with real embeddings (0.221 vs
0.273).

\textbf{Validated adaptive principle}: The near-equivalence of VDD and
online\_lambda (d = -0.056) validates that \emph{adaptive decay itself}
is the valuable insight, not the specific mechanism. Any system
implementing volatility-aware forgetting---whether via embedding
distance, prediction error, or other signals---will outperform static
approaches. Critically, VDD achieves this equivalence \emph{without
ground-truth labels}: online\_lambda requires prediction-error feedback
from labeled data to learn its decay rate, while VDD derives adaptation
purely from embedding-space drift signals. In production RAG systems
where labeled temporal data is scarce, this unsupervised property is the
practical differentiator.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{6. Limitations}\label{limitations}

We enumerate fourteen limitations transparently:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Recency often wins on accuracy}: lambda \textgreater= 0.5
  outperforms VDD in permanent high-drift (d = -2.90). VDD's value is in
  staleness reduction, robustness, and adaptivity.
\item
  \textbf{Adaptive baselines are competitive}: Holt-Winters and
  EMA-lambda beat VDD in 3/4 synthetic scenarios, though VDD excels in
  mixed drift.
\item
  \textbf{Detector precision requires context}: Stable-period false
  activation is 13.5\% (not \textasciitilde1\% as initially reported).
  Post-drift cleanup (99.9\% elevated) is correct behavior, not error.
\item
  \textbf{Time-weighted dominates gradual transitions}: 0.899 vs 0.621
  on real-world documentation that evolves gradually.
\item
  \textbf{Sigmoid parameters require tuning}: Default V\_0=0.5 ranks
  25/36; practitioners must tune V\_0 in {[}0.1, 0.3{]}.
  Auto-calibration via 75th-percentile burn-in (Experiment 40) improves
  over hand-tuned defaults by 22.7\% and eliminates manual tuning, but
  the optimal fixed V\_0 still outperforms auto-calibration.
\item
  \textbf{Window size sensitivity}: 325\% performance range. Poor
  choices (cw=10, aw=20) substantially degrade VDD.
\item
  \textbf{Effect size inflation}: Large Cohen's d values in synthetic
  experiments partly reflect controlled conditions with low variance;
  real-embedding experiments confirm attenuated but directionally
  consistent effects (0.70x ratio). Type M analysis shows at d=0.5 true
  effect, observed d is inflated 1.43x. \textbf{Production deployments
  would likely show smaller effects than those reported here.}
\item
  \textbf{Scaling}: O(n) complexity; n \textgreater{} 10,000 memories
  requires approximate nearest neighbor search (FAISS, ScaNN). Drift
  detection remains O(d).
\item
  \textbf{LLM validation scale}: LLM-in-the-loop evaluation expanded
  from n=5 to n=50 seeds (7,500 LLM calls), confirming stable rankings.
  However, VDD and recency show zero variance across seeds, indicating
  deterministic retrieval behavior rather than robust statistical
  estimation. Larger-scale evaluation with RAGAS (Es et al., 2023) and
  diverse fact sets would strengthen findings.
\item
  \textbf{Domain scope}: Validated on 3 technical documentation domains.
  Generalization to news, medical, and legal domains requires future
  work.
\item
  \textbf{No deployment study}: VDD has not been evaluated in a
  production RAG system. The computational overhead (2.75ms) suggests
  feasibility, but integration complexity, parameter tuning burden, and
  real-world drift characteristics remain untested.
\item
  \textbf{Benchmark results are nuanced}: On StreamingQA (accumulation),
  VDD scores 0.008 vs no\_decay's 0.104 (d = -7.68). On FreshQA
  (replacement), all decay methods achieve 100\% current-answer
  retrieval vs no\_decay's 70--80\%, but keyword-overlap scoring
  obscures this advantage. Neither benchmark provides a clean evaluation
  of VDD's target scenario---StreamingQA lacks knowledge replacement,
  while FreshQA's evaluation metric penalizes decay methods that
  correctly suppress outdated answers.
\item
  \textbf{Embedding transfer limitations}: While rankings transfer
  between hash and real embeddings (4/5 scenarios), effect sizes
  attenuate by approximately 30\%. The one scenario where rankings
  differ (reversion) suggests that real embedding similarity structure
  may compress the performance differences that hash embeddings amplify.
\item
  \textbf{Online lambda equivalence}: An adaptive baseline using
  prediction-error-based lambda learning matches VDD (d = -0.056). This
  validates the adaptive principle but weakens the claim that VDD's
  specific mechanism is uniquely valuable. VDD's advantages over
  online\_lambda are operational: no ground-truth labels required
  (unsupervised), stable cold-start behavior, and interpretable decay
  dynamics. When labeled temporal data is available, online\_lambda is
  equally effective; VDD's value proposition is for the common case
  where such labels are absent.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{7. Future Work}\label{future-work}

\begin{itemize}
\tightlist
\item
  \textbf{Temporal evaluation metrics}: FreshQA evaluation (Section 5.5)
  revealed that keyword-overlap scoring fails to capture decay methods'
  advantage in suppressing outdated answers. Developing metrics that
  reward temporal precision (retrieving \emph{only} the current answer)
  would better evaluate VDD's target scenario. RealTimeQA (weekly
  updates) offers another evaluation axis.
\item
  \textbf{Deployment study}: Integration with a production RAG system
  (e.g., LangChain, LlamaIndex) measuring real-world impact on answer
  quality, staleness reduction, and user satisfaction.
\item
  \textbf{Hybrid detection}: Combining semantic distance (high recall)
  with prediction error (high precision) to improve the 13.5\% false
  activation rate.
\item
  \textbf{Multi-topic memory}: Separate decay rates per domain to
  prevent topic-switch false positives, inspired by T-GRAG's
  domain-aware temporal structure.
\item
  \textbf{Modern RAG integration}: Combining VDD with Self-RAG or CRAG
  for joint temporal and quality management. Specifically, VDD could
  modulate Self-RAG's reflection token thresholds based on temporal
  validity.
\item
  \textbf{Adaptive threshold learning}: Learning optimal drift
  thresholds (V\_0) from deployment feedback, potentially converging
  with online\_lambda's prediction-error approach.
\item
  \textbf{Knowledge editing synergy}: Exploring VDD as a complement to
  WISE/ROME for joint parametric and non-parametric memory management.
\item
  \textbf{HippoRAG integration}: Adding VDD-style temporal weighting to
  HippoRAG's knowledge graph edges, enabling joint structural and
  temporal memory management.
\item
  \textbf{Distributional drift detection}: Upgrading from mean-embedding
  distance to full MMD or LSDD for distributional sensitivity, at the
  cost of increased computational overhead.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{8. Conclusion}\label{conclusion}

We have validated \textbf{Volatility-Driven Decay} across 42 experiments
with rigorous statistical methodology (n = 30, bootstrap CIs,
Benjamini-Hochberg FDR correction, Type M error analysis, explicit
seeds). VDD achieves 87.5\% error reduction versus static baselines,
ranks \#2 among 13 methods across four drift patterns, and demonstrates
genuinely bimodal adaptation (Hartigan's Dip p \textless{} 0.001,
Ashman's D \textgreater{} 2). On 120 real-world facts across three
domains with real embeddings, VDD beats recency on staleness (d = +2.13)
while maintaining cross-domain consistency (accuracy 0.605--0.647).

Rankings transfer between hash-based and real neural embeddings (4/5
scenario agreement, r = 0.935 effect size correlation), with real
embeddings producing attenuated but directionally consistent effects
(0.70x ratio). All five activation functions converge at recommended k
\textgreater= 5 (d -\textgreater{} 0), justifying sigmoid selection on
interpretability grounds. An adaptive baseline (online\_lambda) matches
VDD's performance (d = -0.056), validating the core principle that
adaptive decay is valuable regardless of the specific adaptation
mechanism.

VDD is not universally best. Recency wins in permanent high-drift.
Time-weighted retrieval dominates gradual transitions. Holt-Winters
beats VDD in 3/4 synthetic scenarios. The key insight is not that VDD is
the best method---it is not, in most individual scenarios. The insight
is that VDD is the \textbf{safest method when the scenario is unknown},
which in production, it usually is.

VDD complements rather than competes with modern RAG architectures.
Self-RAG, CRAG, and Adaptive-RAG improve retrieval quality; ARM adapts
retrieval strategy; T-GRAG and HippoRAG provide temporal structure; VDD
manages temporal validity through adaptive forgetting. A production
system benefits from combining these approaches. We release the complete
benchmark, dataset, and evaluation framework to support further research
at the intersection of temporal memory management and
retrieval-augmented generation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{References}\label{references}

Abdallah, M., et al.~(2026). TEMPO: A Temporal Reasoning Benchmark for
Language Models. \emph{arXiv preprint}.

An, J. (2025). Cognitive Workspace: An Attention-Gated Working Memory
Architecture for LLM Agents. \emph{arXiv preprint}.

Asai, A., Wu, Z., Wang, Y., Sil, A., \& Hajishirzi, H. (2024). Self-RAG:
Learning to Retrieve, Generate, and Critique through Self-Reflection.
\emph{ICLR 2024}.

Bifet, A., \& Gavalda, R. (2007). Learning from Time-Changing Data with
Adaptive Windowing. \emph{SIAM International Conference on Data Mining},
443--448.

Borgeaud, S., et al.~(2022). Improving Language Models by Retrieving
from Trillions of Tokens. \emph{ICML 2022}.

Bursa, M. (2026). ARM: Adaptive Retrieval Memory for RAG Systems.
\emph{arXiv preprint}.

Chen, W., et al.~(2021). A Dataset for Answering Time-Sensitive
Questions. \emph{NeurIPS 2021 Datasets and Benchmarks}.

Chen, Y., et al.~(2025). ChronoQA: A Benchmark for Temporal Knowledge
Evolution in Question Answering. \emph{arXiv preprint}.

Chhikara, P., et al.~(2025). Mem0: The Memory Layer for Personalized AI.
\emph{arXiv preprint}.

Dredze, M., Crammer, K., \& Pereira, F. (2008). Confidence-Weighted
Linear Classification. \emph{ICML 2008}, 264--271.

Ebbinghaus, H. (1885). \emph{Uber das Gedachtnis: Untersuchungen zur
experimentellen Psychologie}. Duncker \& Humblot.

Es, S., James, J., Espinosa-Anke, L., \& Schockaert, S. (2023). RAGAS:
Automated Evaluation of Retrieval Augmented Generation.
\emph{arXiv:2309.15217}.

Feldhans, R., Wilke, A., Heusinger, S., \& Hammer, B. (2021). Drift
Detection in Text Data with Document Embeddings. \emph{International
Conference on Intelligent Data Engineering and Automated Learning
(IDEAL)}, 107--118.

Gama, J., Zliobaite, I., Bifet, A., Pechenizkiy, M., \& Bouchachia, A.
(2014). A Survey on Concept Drift Adaptation. \emph{ACM Computing
Surveys}, 46(4), 1--37.

Gao, Y., et al.~(2024). Retrieval-Augmented Generation for Large
Language Models: A Survey. \emph{arXiv:2312.10997}.

Gelman, A., \& Carlin, J. (2014). Beyond Power Calculations: Assessing
Type S (Sign) and Type M (Magnitude) Errors. \emph{Perspectives on
Psychological Science}, 9(6), 641--651.

Gutierrez, B. J., et al.~(2024). HippoRAG: Neurobiologically Inspired
Long-Term Memory for Large Language Models. \emph{NeurIPS 2024}.

Guu, K., Lee, K., Tung, Z., Pasupat, P., \& Chang, M.-W. (2020). REALM:
Retrieval-Augmented Language Model Pre-Training. \emph{ICML 2020}.

Hartigan, J. A., \& Hartigan, P. M. (1985). The Dip Test of Unimodality.
\emph{Annals of Statistics}, 13(1), 70--84.

Haykin, S. (2002). \emph{Adaptive Filter Theory} (4th ed.). Prentice
Hall.

Holt, C. C. (2004). Forecasting Seasonals and Trends by Exponentially
Weighted Moving Averages. \emph{International Journal of Forecasting},
20(1), 5--10.

Hu, Z., et al.~(2026). RAG without Forgetting: Experience Replay Memory
for Domain-Adaptive RAG. \emph{arXiv preprint}.

Izacard, G., et al.~(2023). Atlas: Few-shot Learning with Retrieval
Augmented Language Models. \emph{JMLR}, 24(251), 1--43.

Jeong, S., Baek, J., Cho, S., Hwang, S. J., \& Park, J. C. (2024).
Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language
Models through Question Complexity. \emph{NAACL 2024}.

Jiang, Z., Xu, F. F., Gao, L., et al.~(2023). Active Retrieval Augmented
Generation. \emph{EMNLP 2023}.

Karpukhin, V., et al.~(2020). Dense Passage Retrieval for Open-Domain
Question Answering. \emph{EMNLP 2020}.

Kasai, J., et al.~(2023). RealTime QA: What's the Answer Right Now?
\emph{NeurIPS 2023}.

Kirkpatrick, J., et al.~(2017). Overcoming Catastrophic Forgetting in
Neural Networks. \emph{PNAS}, 114(13), 3521--3526.

Kolter, J. Z., \& Maloof, M. A. (2007). Dynamic Weighted Majority: An
Ensemble Method for Drifting Concepts. \emph{JMLR}, 8, 2755--2790.

Lewis, P., et al.~(2020). Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks. \emph{NeurIPS}, 33.

Li, X., et al.~(2025). T-GRAG: Temporal Graph-Based Retrieval-Augmented
Generation. \emph{arXiv preprint}.

Liska, A., Kocisky, T., Gribovskaya, E., et al.~(2022). StreamingQA: A
Benchmark for Adaptation to New Knowledge over Time. \emph{ICML 2022}.

Liu, N. F., et al.~(2024). Lost in the Middle: How Language Models Use
Long Contexts. \emph{TACL}, 12, 157--173.

Lu, J., Liu, A., Dong, F., Gu, F., Gama, J., \& Zhang, G. (2019).
Learning under Concept Drift: A Review. \emph{IEEE TKDE}, 31(12),
2346--2363.

Mallya, A., \& Lazebnik, S. (2018). PackNet: Adding Multiple Tasks to a
Single Network by Iterative Pruning. \emph{CVPR 2018}.

Mem0 AI. (2024). Mem0: The Memory Layer for AI Applications.
https://mem0.ai/

Meng, K., Bau, D., Andonian, A., \& Belinkov, Y. (2022). Locating and
Editing Factual Associations in GPT. \emph{NeurIPS 2022}.

Meng, K., et al.~(2023). Mass-Editing Memory in a Transformer.
\emph{ICLR 2023}.

Page, E. S. (1954). Continuous Inspection Schemes. \emph{Biometrika},
41(1/2), 100--115.

Packer, C., et al.~(2023). MemGPT: Towards LLMs as Operating Systems.
\emph{arXiv:2310.08560}.

Park, J. S., et al.~(2023). Generative Agents: Interactive Simulacra of
Human Behavior. \emph{arXiv:2304.03442}.

Pfister, R., Schwarz, K. A., Janczyk, M., Dale, R., \& Freeman, J. B.
(2013). Good Things Peak in Pairs: A Note on the Bimodality Coefficient.
\emph{Frontiers in Psychology}, 4, 700.

Qian, H., et al.~(2025). MemoRAG: Moving towards Next-Gen RAG via
Memory-Inspired Knowledge Discovery. \emph{arXiv preprint}.

Rusu, A. A., et al.~(2016). Progressive Neural Networks.
\emph{arXiv:1606.04671}.

Saad-Falcon, J., et al.~(2024). ARES: An Automated Evaluation Framework
for Retrieval-Augmented Generation Systems. \emph{NAACL 2024}.

Sarthi, P., Abdullah, S., Tuli, A., et al.~(2024). RAPTOR: Recursive
Abstractive Processing for Tree-Organized Retrieval. \emph{ICLR 2024}.

Silverman, B. W. (1981). Using Kernel Density Estimates to Investigate
Multimodality. \emph{JRSS Series B}, 43(1), 97--99.

Vu, T., et al.~(2024). FreshLLMs: Refreshing Large Language Models with
Search Engine Augmentation. \emph{ACL 2024}.

Wang, P., et al.~(2024). WISE: Rethinking the Knowledge Memory for
Lifelong Model Editing of Large Language Models. \emph{NeurIPS 2024}.

Wei, L., \& Zhang, Y. (2026). FadeMem: Biologically-Inspired Forgetting
for Efficient Agent Memory. \emph{arXiv:2601.18642}.

Wixted, J. T. (2004). The Psychology and Neuroscience of Forgetting.
\emph{Annual Review of Psychology}, 55(1), 235--269.

Yan, S.-Q., Gu, J.-C., Zhu, Y., \& Ling, Z.-H. (2024). Corrective
Retrieval Augmented Generation. \emph{ICML 2024}.

Yin, Z., et al.~(2024). History Matters: Temporal Knowledge Editing in
Large Language Models. \emph{AAAI 2024}.

Zhang, Y., et al.~(2025). A Survey on Memory in Large Language Model
Agents. \emph{arXiv preprint}.

Zhong, W., Guo, L., Gao, Q., \& Wang, Y. (2024). MemoryBank: Enhancing
Large Language Models with Long-Term Memory. \emph{AAAI 2024}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Appendix A: Complete Experimental
Results}\label{appendix-a-complete-experimental-results}

All 42 experiments are summarized below. Experiments highlighted in the
main body (Section 4) include narrative discussion; remaining
experiments are presented as table entries.

\subsubsection{A.1 Core Validation (Experiments
1--8)}\label{a.1-core-validation-experiments-18}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4062}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3438}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0938}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Exp
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Result
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
n
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Drift detection comparison & Embedding Distance: 100\% recall,
0.12ms; ADWIN/PH: 0\% recall & 1 \\
2 & VDD with real detection & 87.5\% error reduction vs static (IAE
15.81 vs 126.67) & 1 \\
3 & Vector memory bank & VDD: 72\% smaller memory, 0.656 acc vs 0.968
static & 1 \\
4 & Mixed-drift scenario & Recency 32.14 beats VDD 40.84 (p \textless{}
0.001) & 10 \\
5 & Lambda ablation & Best: lambda\_b=0.3, lambda\_m=0.95 (IAE 9.01,
92\% improvement) & 3 \\
6 & 10K-step stability & 0.03\% oscillation, mean error 0.096 & 1 \\
7 & Latency benchmark & 2.75ms mean, 6.48ms P99 (1000 memories) &
1000 \\
8 & 7-baseline comparison & VDD \#2, beats all static (p \textless{}
0.001); recency \#1 & 10 \\
\end{longtable}
}

\subsubsection{A.2 Extended Validation (Experiments
9--15)}\label{a.2-extended-validation-experiments-915}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4062}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3438}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0938}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Exp
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Result
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
n
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
9 & Gradual drift & Recency 38.7 beats VDD 45.2 (p \textless{} 0.01) &
10 \\
10 & Detection method comparison & Semantic distance: 89\% recall;
Prediction error: 67\% recall & 10 \\
11 & ADWIN comparison & VDD 20.94 vs ADWIN 112.36 (d = -2.2) & 10 \\
12 & Computational scaling & O(n) linear: 0.28ms (100) -\textgreater{}
26.8ms (10K) & 1 \\
13 & 20-fold statistical validation & VDD 28.4 {[}26.8, 30.1{]}; recency
23.1 {[}22.3, 24.0{]} & 20 \\
14 & Real RAG (React docs) & VDD staleness 0.266 \textless{} recency
0.302 (d = +1.85) & 10 \\
15 & Bursty drift & Recency 28.7 beats VDD 48.2 (d = -2.90) & 10 \\
\end{longtable}
}

\subsubsection{A.3 Robustness and Real-World (Experiments
16--20)}\label{a.3-robustness-and-real-world-experiments-1620}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4062}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3438}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0938}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Exp
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Result
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
n
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
16 & Reversion scenario & VDD 35.4 beats recency 52.1 (d = +1.22) &
10 \\
17 & Mixed uncertainty & VDD never worst; recency best acc (0.687) but
fails on reversions & 10 \\
18 & Staleness evaluation & VDD 18.3\% stale vs recency 24.7\% (d =
+0.89) & 10 \\
19 & Effective lambda analysis & Bimodal in 5/5 (BC \textgreater{}
0.555); VDD 3.2x better than oracle in mixed & 30 \\
20 & Extended baselines (10 methods) & VDD \#2/10 in all 4 patterns &
30 \\
\end{longtable}
}

\subsubsection{A.4 Statistical Hardening (Experiments
21--26)}\label{a.4-statistical-hardening-experiments-2126}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4062}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3438}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0938}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Exp
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Result
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
n
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
21 & Multi-domain (React + Python) & Time-weighted 0.899; VDD 0.621
cross-domain consistent & 30 \\
22 & End-to-end quality & Time-weighted 0.877; VDD 0.622 beats no-decay
0.333 & 30 \\
23 & n=30 confirmatory & All ranks confirmed; optimized VDD (12.6) beats
recency (23.3) & 30 \\
24 & Precision analysis & 13.5\% stable false activation; 99.9\%
post-drift cleanup correct & 30 \\
25 & Real embedding transfer & VDD improves 0.617 -\textgreater{} 0.647;
rankings preserved & 30 \\
26 & LLM-in-the-loop & VDD 0.522 matches recency; r = 0.277 vs
deterministic & 5 \\
\end{longtable}
}

\subsubsection{A.5 Advanced Analysis (Experiments
27--31)}\label{a.5-advanced-analysis-experiments-2731}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4062}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3438}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0938}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Exp
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Result
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
n
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
27 & Sigmoid sensitivity & V\_0=0.1 best; default ranks 25/36 (254\%
range) & 30 \\
28 & n=30 rank stability & 100\% rank stability across all 10 methods &
30 \\
29 & Adaptive baselines & Holt-Winters beats VDD 3/4; VDD \#2 in mixed
drift & 30 \\
30 & Robust bimodality & Bimodal in 4/5 (Dip p \textless{} 0.001,
Ashman's D \textgreater{} 2); BC alone insufficient & 30 \\
31 & Three-domain (120 facts) & VDD beats recency all 3 domains (d =
+2.13); sliding window catastrophic & 30 \\
\end{longtable}
}

\subsubsection{A.6 Revision Experiments (Experiments
34--38)}\label{a.6-revision-experiments-experiments-3438}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4062}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3438}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0938}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Exp
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Result
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
n
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
34 & Statistical corrections & 85/88 survive BH FDR; Type M: 1.43x at
d=0.5 & 30 \\
35 & LLM validation (expanded) & n=50: VDD = Recency = 0.522; r=0.271
aggregate correlation; 7,500 LLM calls & 50 \\
36 & Real embedding suite & Rankings preserved 4/5; effect sizes 0.70x;
r = 0.935 & 30 \\
37 & Activation ablation + baselines & All activations converge
k\textgreater=5; online\_lambda matches VDD (d=-0.056) & 30 \\
38 & StreamingQA benchmark & VDD 0.008 vs no\_decay 0.104 (d=-7.68);
decay counterproductive on accumulation tasks & 10 \\
39 & FreshQA benchmark & 100\% current-answer retrieval for decay
methods vs 70-80\% no\_decay; keyword overlap obscures advantage & 10 \\
40 & Auto-calibration of V\_0 & 75th-percentile burn-in improves over
hand-tuned by 22.7\%; eliminates manual tuning & 20 \\
41 & Cross-model embedding generalization & Rankings preserved: Spearman
rho=0.978 across nomic-embed-text and mxbai-embed-large & 10 \\
42 & Practitioner decision tree & Visual method selection guide based on
drift pattern knowledge and task type & -- \\
\end{longtable}
}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Appendix B: Statistical
Details}\label{appendix-b-statistical-details}

\subsubsection{B.1 Bootstrap
Methodology}\label{b.1-bootstrap-methodology}

All confidence intervals are computed via the percentile bootstrap
method with B = 1000 resamples. For each comparison, we compute:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Point estimate: sample mean of IAE, accuracy, or staleness.
\item
  95\% CI: 2.5th and 97.5th percentiles of bootstrap distribution.
\item
  Cohen's d: (M\_1 - M\_2) / s\_pooled.
\end{enumerate}

\subsubsection{B.2 Bimodality Testing
Protocol}\label{b.2-bimodality-testing-protocol}

We apply four complementary tests:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Bimodality Coefficient} (BC): BC = (m\_3\^{}2 + 1) / (m\_4 + 3
  * (n-1)\^{}2 / ((n-2)(n-3))) where m\_3 is skewness and m\_4 is
  kurtosis. BC \textgreater{} 0.555 suggests bimodality (Pfister et al.,
  2013).
\item
  \textbf{Hartigan's Dip Test}: Tests unimodality against any
  alternative. p \textless{} 0.05 rejects unimodality (Hartigan \&
  Hartigan, 1985).
\item
  \textbf{Ashman's D}: D = sqrt(2) * \textbar mu\_1 - mu\_2\textbar{} /
  sqrt(sigma\_1\^{}2 + sigma\_2\^{}2). D \textgreater{} 2 indicates
  clean separation.
\item
  \textbf{Silverman's bandwidth test}: Estimates number of modes via
  kernel density estimation (Silverman, 1981).
\end{enumerate}

Using multiple tests guards against individual test limitations (e.g.,
BC's sensitivity to outliers).

\subsubsection{B.3 Multiple Comparison Correction
Details}\label{b.3-multiple-comparison-correction-details}

We applied Benjamini-Hochberg FDR correction to all 88 hypothesis tests
conducted across the paper. The procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Rank all 88 p-values from smallest to largest.
\item
  For rank i, compute the BH threshold: (i/88) * 0.05.
\item
  Find the largest i where p\_i \textless= threshold.
\item
  All tests with rank \textless= i are declared significant.
\end{enumerate}

Result: All 85 tests that were significant uncorrected remain
significant after BH correction. The three non-significant tests (VDD
vs.~recency in specific scenarios where the difference was small) remain
non-significant.

\subsubsection{B.4 Effect Size Caveat}\label{b.4-effect-size-caveat}

The large Cohen's d values reported in synthetic experiments (d
\textgreater{} 3 in several comparisons) partly reflect controlled
simulation conditions producing low variance. With real embeddings,
effect sizes attenuate to approximately 0.70x their hash-based values
(Experiment 36). Type M analysis (Experiment 34) further quantifies
this: at a realistic true d = 0.5, observed d would be exaggerated 1.43x
in our study design. In noisier real-world deployments, effect sizes
would likely be smaller but remain directionally consistent. \textbf{We
report d for completeness but emphasize practical significance alongside
statistical significance, and we urge readers to treat reported effect
sizes as upper bounds.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Appendix C: Integration
Guide}\label{appendix-c-integration-guide}

\subsubsection{C.1 Minimal Integration}\label{c.1-minimal-integration}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ vdd }\ImportTok{import}\NormalTok{ VDDRetriever}

\NormalTok{retriever }\OperatorTok{=}\NormalTok{ VDDRetriever(}
\NormalTok{    lambda\_base}\OperatorTok{=}\FloatTok{0.2}\NormalTok{,}
\NormalTok{    lambda\_max}\OperatorTok{=}\FloatTok{0.9}\NormalTok{,}
\NormalTok{    curr\_window}\OperatorTok{=}\DecValTok{3}\NormalTok{,}
\NormalTok{    arch\_window}\OperatorTok{=}\DecValTok{200}\NormalTok{,}
\NormalTok{    drift\_threshold}\OperatorTok{=}\FloatTok{0.4}\NormalTok{,}
\NormalTok{    v0}\OperatorTok{=}\FloatTok{0.1}\NormalTok{,}
\NormalTok{    k}\OperatorTok{=}\DecValTok{10}
\NormalTok{)}

\NormalTok{docs }\OperatorTok{=}\NormalTok{ retriever.retrieve(query, k}\OperatorTok{=}\DecValTok{5}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Current lambda: }\SpecialCharTok{\{}\NormalTok{retriever}\SpecialCharTok{.}\NormalTok{current\_lambda}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Drift detected: }\SpecialCharTok{\{}\NormalTok{retriever}\SpecialCharTok{.}\NormalTok{drift\_detected}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{C.2 Framework
Compatibility}\label{c.2-framework-compatibility}

VDD operates at the retrieval scoring layer and is compatible with any
RAG framework (LangChain, LlamaIndex, Haystack). Integration requires:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Embedding stream access}: VDD needs the raw embeddings for
  drift detection.
\item
  \textbf{Timestamp tracking}: Each memory requires an insertion
  timestamp.
\item
  \textbf{Score modification}: Replace static similarity with S = Sim *
  exp(-lambda(t) * delta\_t).
\end{enumerate}

No external APIs or model calls are required for drift detection---it
operates entirely on embedding distances computed locally.

\subsubsection{C.3 Framework-Specific
Considerations}\label{c.3-framework-specific-considerations}

\textbf{LangChain}: VDD can be integrated as a custom retriever by
subclassing \texttt{BaseRetriever} and overriding the
\texttt{\_get\_relevant\_documents} method to apply temporal weighting.
The embedding stream is accessible via LangChain's embedding model
interface.

\textbf{LlamaIndex}: Integration via a custom \texttt{NodePostprocessor}
that applies VDD's temporal weighting after initial retrieval. The query
engine's embedding history provides the drift detection signal.

\textbf{Haystack}: VDD fits as a custom \texttt{Ranker} component in
Haystack pipelines, operating between the retriever and reader stages.

The full implementation is available at:
https://github.com/abe238/volatility-driven-decay

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Appendix D: Hyperparameter Selection
Guide}\label{appendix-d-hyperparameter-selection-guide}

For practitioners deploying VDD, we recommend the following selection
procedure:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Parameter & Default & Tuning Range & Impact & Priority \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
V\_0 (threshold) & 0.1 & {[}0.05, 0.3{]} & 254\% IAE range &
\textbf{Critical} \\
curr\_window & 3 & {[}3, 10{]} & 325\% IAE range & \textbf{Critical} \\
arch\_window & 200 & {[}100, 500{]} & Moderate & High \\
k (steepness) & 10 & {[}5, 20{]} & Low at k\textgreater=5 & Low \\
lambda\_base & 0.2 & {[}0.1, 0.3{]} & Moderate & Medium \\
lambda\_max & 0.9 & {[}0.8, 0.95{]} & Moderate & Medium \\
\end{longtable}
}

\textbf{Start with}: V\_0=0.1, k=10, cw=3, aw=200, lambda\_base=0.2,
lambda\_max=0.9. These defaults were selected based on grid search
across 36 parameter configurations and 4 drift scenarios.

\textbf{Key insight from activation ablation}: At k\textgreater=5, the
choice of activation function (sigmoid, linear, exponential, step,
softplus) is immaterial (d -\textgreater{} 0). Therefore, practitioners
should focus tuning effort on V\_0 and window sizes rather than the
activation shape.

\subsubsection{D.2 Auto-Calibration of V\_0 (Experiment
40)}\label{d.2-auto-calibration-of-v_0-experiment-40}

A key practical limitation is V\_0 tuning (254\% performance range). We
propose a simple auto-calibration method: set V\_0 to the 75th
percentile of volatility values observed during a burn-in period (first
20\% of the data stream). This requires no labeled data or manual
intervention.

\textbf{Method}: During burn-in, collect volatility values \{V\_1,
\ldots, V\_b\} where b = 0.2 * T. Set V\_0 = percentile(75, \{V\_1,
\ldots, V\_b\}). The 75th percentile ensures that only genuinely
elevated volatility triggers aggressive decay, while accommodating the
baseline volatility distribution of the specific embedding space.

\textbf{Results (n=20 seeds, 4 drift patterns)}:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1324}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1471}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1471}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1471}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1912}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Auto V\_0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hand-Tuned V\_0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Auto IAE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hand IAE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Improvement
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Sudden & 0.048 & 0.10 & 21.6 & 29.9 & -27.8\% \\
Gradual & 0.079 & 0.10 & 13.5 & 15.8 & -14.6\% \\
Reversion & 0.048 & 0.10 & 18.9 & 25.9 & -27.2\% \\
Mixed & 0.055 & 0.10 & 18.4 & 23.3 & -21.1\% \\
\end{longtable}
}

Auto-calibration improves over the hand-tuned default (V\_0=0.1) by
\textbf{22.7\% on average} (all p \textless{} 1e-8). The auto-calibrated
values are consistently lower than the hand-tuned default, indicating
that the default V\_0=0.1 is slightly too conservative. The best fixed
value (V\_0=0.01) still outperforms auto-calibration, suggesting room
for improvement, but auto-calibration eliminates the need for manual
tuning entirely while providing substantial improvement over the
default.

\newpage

\vspace*{\fill}

\begin{center}
{\small\itshape Opinions are my own. This work does not relate to my position at Amazon.}
\end{center}

\vspace*{\fill}

\end{document}
