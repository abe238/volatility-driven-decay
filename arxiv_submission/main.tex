\documentclass[11pt]{article}

% Standard packages
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{verbatim}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{parskip}
\usepackage{placeins}

% Prevent overfull lines
\setlength{\emergencystretch}{3em}

\title{Volatility-Driven Decay: Adaptive Memory Retention for RAG Systems Under Unknown Drift}
\author{Abe Diaz\\Independent Researcher\\\texttt{https://github.com/abe238/volatility-driven-decay}}
\date{February 2026}

\begin{document}
\maketitle

\begin{abstract}
Retrieval-Augmented Generation (RAG) systems face a silent reliability crisis: when knowledge drifts---APIs deprecated, guidelines revised, frameworks migrated---retrieval continues serving outdated answers with full confidence. We find that naive retrieval serves stale content 67\% of the time across 120 real-world technical facts. Existing solutions assume known drift patterns: recency-biased retrieval suits constant change, time-weighted decay handles gradual transitions, and sliding windows work for bounded contexts. None adapts when the pattern is unknown---the typical production condition.

We introduce Volatility-Driven Decay (VDD), where the memory decay rate $\lambda(t)$ dynamically adjusts based on detected semantic drift via sigmoid activation on embedding centroid distance. Structurally inspired by control-theoretic adaptive filtering and cognitive models of memory consolidation, VDD continuously modulates its forgetting rate between a conservative baseline and an aggressive ceiling. Validated on 120 facts across React, Python, and Node.js documentation ($n=30$, bootstrap CIs, Benjamini--Hochberg corrected, Cohen's $d$), VDD achieves 87.5\% error reduction versus static baselines, ranks \#2 among 13 methods across four drift patterns, and is never the worst performer in any tested scenario.

An auto-calibration method (75th percentile of burn-in volatilities) eliminates manual $V_0$ tuning, improving over hand-tuned defaults by 22.7\% on average.

We are equally explicit about limitations: recency outperforms VDD in permanent high-drift ($d = -2.90$), time-weighted retrieval dominates gradual transitions (0.871 vs 0.584), and adaptive baselines (Holt--Winters, EMA-lambda) beat VDD in 3/4 synthetic scenarios. On StreamingQA (36K temporal QA pairs), all decay methods including VDD underperform no-decay ($d = -7.68$), confirming that VDD is counterproductive on knowledge-accumulation tasks---it is specifically designed for knowledge-replacement scenarios. On FreshQA (452 temporal questions), all decay methods achieve 100\% current-answer retrieval rate versus no-decay's 70--80\%, though keyword-overlap scoring fails to capture this advantage. Rankings transfer between hash-based and real neural embeddings (4/5 scenario agreement, $r = 0.935$ effect size correlation) and across embedding models (nomic-embed-text vs mxbai-embed-large, Spearman $\rho = 0.978$), though effect sizes attenuate with real embeddings (mean ratio 0.70x), indicating controlled-condition results are directionally valid but inflated. VDD's value is precisely for production systems where drift patterns cannot be predicted in advance. This positioning complements---rather than competes with---modern RAG architectures (Self-RAG, CRAG, Adaptive-RAG) that improve \emph{what} to retrieve, while VDD addresses \emph{when to forget}. We release the complete benchmark, dataset, and evaluation framework.

\medskip
\noindent\textbf{Keywords:} RAG, memory decay, concept drift, adaptive retrieval, knowledge staleness, LLM memory
\end{abstract}

%% ============================================================
\section{Introduction}
\label{sec:introduction}

\subsection{The Knowledge Staleness Problem}
\label{sec:staleness-problem}

Retrieval-Augmented Generation has become the dominant architecture for grounding large language models in external knowledge \cite{lewis2020,gao2024}. Yet RAG systems harbor a failure mode the community has largely ignored: \textbf{knowledge staleness}. When the world changes---React deprecates lifecycle methods, Python removes a module, a medical guideline is revised---retrieval systems continue surfacing outdated answers. The user receives a confident, well-formatted, \emph{wrong} answer.

This is not hypothetical. In our evaluation of 120 real-world facts across three documentation ecosystems, naive retrieval (no decay) serves stale content \textbf{66.7\% of the time}. Even commonly recommended approaches fail: sliding window retrieval catastrophically collapses to random-chance accuracy (0.333) on certain domain structures.

\subsection{The Static Decay Dilemma}
\label{sec:static-decay}

The ``Forgetting is a Feature'' hypothesis \cite{wei2026} established that biologically inspired decay improves retrieval precision. However, existing implementations rely on \textbf{static decay}---a fixed half-life applied uniformly regardless of environmental conditions:

\begin{itemize}
  \item \textbf{Conservative decay} ($\lambda \sim 0.1$) preserves valid knowledge but accumulates stale facts (IAE 148, nearly 5x worse than adaptive approaches).
  \item \textbf{Aggressive decay} ($\lambda \geq 0.5$) stays current but catastrophically forgets when knowledge reverts (IAE 52.1 vs VDD's 35.4 in reversion scenarios).
  \item \textbf{No fixed rate handles both.} A coding assistant may face gradual API deprecation for months, then a sudden breaking change---within the same deployment.
\end{itemize}

\subsection{The Modern RAG Landscape}
\label{sec:modern-rag}

Recent architectures have advanced retrieval quality along multiple axes. Self-RAG \cite{asai2024} introduces reflection tokens for retrieval-time quality control. CRAG \cite{yan2024} adds corrective retrieval to handle low-quality results. Adaptive-RAG \cite{jeong2024} routes queries by complexity. FLARE \cite{jiang2023} triggers retrieval actively during generation. RAPTOR \cite{sarthi2024} builds hierarchical document summaries for multi-level retrieval.

These architectures improve \emph{what} and \emph{how} to retrieve. None addresses \emph{when to forget}---the temporal validity of stored knowledge. VDD operates at the storage layer, determining how aggressively to discount old memories based on environmental change. This is orthogonal and complementary: a Self-RAG system with VDD-managed memory would both select high-quality passages \emph{and} automatically deprioritize stale ones.

\subsection{Our Proposal}
\label{sec:proposal}

We propose that the optimal decay rate is not a constant but a function of \textbf{environmental volatility}. When the semantic landscape is stable, decay should be slow. When embeddings shift---signaling knowledge change---decay should accelerate. We ground this in control-theoretic adaptive filtering, drawing structural inspiration from Recursive Least Squares (RLS) where a forgetting factor dynamically weights past observations, and in cognitive models of hippocampal memory consolidation where environmental novelty accelerates forgetting of outdated associations \cite{wixted2004,ebbinghaus1885}.

\subsection{Contributions}
\label{sec:contributions}

\begin{enumerate}
  \item A \textbf{drift detection method} using embedding centroid distance achieving 100\% recall with 0.12ms latency.
  \item \textbf{Proof of adaptive behavior}: VDD's $\lambda$ is genuinely bimodal (Hartigan's Dip $p < 0.001$, Ashman's $D > 2$), not noisy static decay.
  \item \textbf{Multi-domain validation} on 120 facts across React, Python, and Node.js with real embeddings (nomic-embed-text).
  \item \textbf{Never-worst robustness}: VDD ranks \#2 among 13 methods across all four tested drift patterns.
  \item \textbf{Activation function ablation} showing all tested activations converge at recommended $k \geq 5$ ($d \to 0$), justifying sigmoid selection on interpretability grounds.
  \item \textbf{Hash-to-real embedding transfer validation} demonstrating ranking preservation across embedding types (4/5 scenarios, $r = 0.935$).
  \item \textbf{Rigorous multiple comparison correction}: 85/88 significant results survive Benjamini--Hochberg FDR correction with Type~M error analysis.
  \item \textbf{An honest comparative analysis} showing exactly when VDD helps and when simpler approaches suffice.
  \item \textbf{FreshQA benchmark evaluation} (452 temporal questions) confirming 100\% current-answer retrieval for all decay methods vs 70--80\% for no-decay.
  \item \textbf{Auto-calibration} of $V_0$ via 75th-percentile burn-in, eliminating manual tuning and improving over hand-tuned defaults by 22.7\%.
  \item \textbf{Cross-model embedding generalization} (nomic-embed-text vs mxbai-embed-large): method rankings preserved with Spearman $\rho = 0.978$.
\end{enumerate}

%% ============================================================
\section{Related Work}
\label{sec:related-work}

\subsection{RAG Foundations}
\label{sec:rag-foundations}

The RAG framework \cite{lewis2020} demonstrated that retrieval-augmented models outperform pure parametric models on knowledge-intensive tasks. REALM \cite{guu2020} pioneered pre-training with retrieval. RETRO \cite{borgeaud2022} scaled retrieval to trillion-token databases. DPR \cite{karpukhin2020} established dense passage retrieval as the standard approach. Atlas \cite{izacard2023} demonstrated few-shot learning via retrieval. Recent surveys \cite{gao2024} identify memory management as an open challenge, with most systems relying on simple recency or relevance-only retrieval.

\subsection{Temporal RAG and Memory Systems}
\label{sec:temporal-rag}

A growing body of work addresses temporal aspects of RAG, positioning VDD within a rapidly evolving landscape:

\textbf{ARM} \cite{bursa2026} introduces Adaptive Retrieval Memory, a concurrent approach that modulates retrieval strategies based on query-time context signals. While ARM adapts \emph{retrieval behavior} (what to fetch and when), VDD adapts \emph{memory retention} (how aggressively to forget stored knowledge). The approaches are complementary: ARM selects among retrieval strategies at query time, while VDD manages the temporal validity of the knowledge base itself. We note that ARM was developed concurrently and independently.

\textbf{T-GRAG} \cite{li2025} proposes a graph-based temporal RAG architecture where documents are organized in a temporal knowledge graph, enabling traversal along temporal edges. T-GRAG addresses temporal reasoning through structural representation; VDD addresses it through adaptive decay. A system could combine T-GRAG's temporal graph structure with VDD's volatility-modulated weighting to achieve both structural and dynamic temporal awareness.

\textbf{HippoRAG} \cite{gutierrez2024} draws on the hippocampal indexing theory of human long-term memory, using an LLM to extract knowledge graph triples that serve as a ``hippocampal index'' for retrieval. HippoRAG focuses on mimicking associative memory for single-step and multi-hop QA. VDD shares the biological inspiration but targets a different aspect: where HippoRAG models \emph{how} memories are stored and associated, VDD models \emph{when} memories should be forgotten. HippoRAG's knowledge graph could benefit from VDD-style temporal weighting on its edges.

\textbf{MemoRAG} \cite{qian2025} introduces a memory-inspired knowledge discovery framework that uses a lightweight LLM to form a global memory of the database, generating draft answers that guide retrieval. MemoRAG's global memory acts as a retrieval cue generator; it does not address temporal validity of the underlying knowledge. VDD could enhance MemoRAG by temporally weighting the documents that the global memory summarizes.

\textbf{Mem0} \cite{chhikara2025} provides a production memory layer for AI applications with TTL-based auto-decay and importance scoring. Mem0's decay is static (fixed TTL per memory type) rather than environment-aware. VDD's volatility-modulated decay could replace Mem0's fixed TTL, providing adaptive retention without manual TTL configuration per deployment.

\textbf{FadeMem} \cite{wei2026} establishes the ``Forgetting is a Feature'' hypothesis, demonstrating that biologically inspired decay improves agent memory efficiency. FadeMem validates the core premise that VDD builds upon---that forgetting is beneficial---but implements static Ebbinghaus-style decay curves. VDD extends this insight by making the decay rate itself adaptive to environmental conditions.

\textbf{Cognitive Workspace} \cite{an2025} proposes a working-memory-inspired architecture for LLM agents with attention-gated memory access. Like VDD, it draws on cognitive science, but focuses on attention mechanisms rather than temporal decay. The two approaches address different aspects of memory management and could be combined.

\subsection{LLM-Native Memory Systems}
\label{sec:llm-memory}

Production LLM systems increasingly implement persistent memory, yet none uses drift-aware forgetting:

\begin{itemize}
  \item \textbf{Claude} (Anthropic): File-based memory with no decay mechanism, leading to unbounded growth and a ``fading memory'' problem where old entries are never removed.
  \item \textbf{ChatGPT} (OpenAI): Timestamped entries with manual deletion only; FIFO context truncation under pressure.
  \item \textbf{Gemini/Vertex} (Google): Fixed TTL---the closest to decay, but static rather than drift-aware.
  \item \textbf{MemGPT/Letta} \cite{packer2023}: Context-pressure eviction with hierarchical memory tiers, but no environmental awareness. VDD could enhance MemGPT by dynamically adjusting tier boundaries.
  \item \textbf{MemoryBank} \cite{zhong2024}: Ebbinghaus-inspired forgetting curves, but decay parameters are static.
\end{itemize}

The gap is clear: no production system implements forgetting that adapts to the rate of environmental change. VDD fills this by providing a volatility-modulated decay mechanism that can be integrated into any of these architectures.

\subsection{Modern RAG Architectures}
\label{sec:modern-rag-arch}

Self-RAG \cite{asai2024} trains models to emit special ``reflection tokens'' that decide when to retrieve, assess retrieval relevance, and critique generated output. This addresses retrieval \emph{quality} at generation time---orthogonal to VDD's storage-layer \emph{temporal validity}.

CRAG \cite{yan2024} implements a corrective retrieval pipeline: a lightweight evaluator triggers web search when retrieved documents are ambiguous or incorrect. Like Self-RAG, CRAG addresses quality, not staleness.

Adaptive-RAG \cite{jeong2024} routes queries by complexity---simple queries skip retrieval, complex ones use multi-step reasoning. This optimizes the retrieval \emph{process} but does not address whether retrieved documents are temporally valid.

FLARE \cite{jiang2023} generates text iteratively, triggering retrieval when the model's confidence drops. RAPTOR \cite{sarthi2024} builds hierarchical summaries enabling retrieval at multiple abstraction levels.

\textbf{RAG-without-Forgetting} \cite{hu2026} proposes Experience Replay Memory (ERM) to prevent catastrophic forgetting when fine-tuning RAG models on new domains. ERM addresses parametric forgetting during model training; VDD addresses non-parametric forgetting in the retrieval memory. The two operate at different layers and are complementary.

\textbf{Key insight}: These architectures improve \emph{what/how} to retrieve. VDD addresses \emph{when to forget}. A system combining Self-RAG's quality filtering with VDD's temporal management would address both failure modes.

\subsection{Embedding Drift Detection}
\label{sec:embedding-drift}

Traditional drift detection methods (ADWIN, Page--Hinkley) operate on scalar data streams and fail on high-dimensional embeddings. Recent work has explored distribution-based approaches for embedding drift:

\textbf{Maximum Mean Discrepancy (MMD)} \cite{feldhans2021} provides a kernel-based two-sample test applicable to high-dimensional distributions, detecting distributional shift between embedding windows. MMD offers formal statistical guarantees but requires $O(n^2)$ computation per test. VDD's embedding centroid distance is a computationally cheaper alternative ($O(d)$ per step) that trades statistical rigor for latency, achieving 0.12ms detection time suitable for real-time RAG queries.

\textbf{Learned drift detectors} using neural networks to detect distribution shifts in embedding spaces have been proposed, but typically require labeled drift examples for training. VDD's unsupervised centroid-distance approach requires no labeled drift data, making it immediately deployable.

Our embedding distance detector can be viewed as a mean-embedding MMD approximation with a linear kernel, sacrificing the full distributional test for $O(d)$ per-step efficiency. Future work could explore full MMD or Learned Kernel MMD for higher detection precision at the cost of latency.

\subsection{Temporal QA Benchmarks}
\label{sec:temporal-benchmarks}

Several benchmarks evaluate temporal reasoning, providing context for VDD's contribution:

\begin{itemize}
  \item \textbf{StreamingQA} \cite{liska2022}: Tests QA over temporally ordered news, measuring whether models track evolving answers.
  \item \textbf{FreshQA} \cite{vu2024}: Evaluates LLMs on questions requiring up-to-date knowledge, finding frequent failures on recently changed facts.
  \item \textbf{RealTimeQA} \cite{kasai2023}: Weekly-updated benchmark tracking model accuracy on current events.
  \item \textbf{TimeQA} \cite{chen2021}: Tests temporal reasoning over structured knowledge.
  \item \textbf{EvolveBench} \cite{yin2024}: Evaluates knowledge editing methods on evolving facts.
  \item \textbf{ChronoQA} \cite{chen2025}: Specifically targets temporal knowledge evolution in QA, tracking how answers change over time. ChronoQA's focus on answer versioning closely aligns with VDD's version-drift scenarios.
  \item \textbf{TEMPO} \cite{abdallah2026}: A temporal reasoning benchmark that evaluates models' ability to handle time-sensitive queries across multiple knowledge domains.
\end{itemize}

These benchmarks test \emph{whether} systems answer correctly over time. VDD provides a \emph{mechanism} for maintaining temporal accuracy in retrieval. Our 120-fact dataset specifically targets version-drift scenarios (API changes, deprecated features) rather than general temporal knowledge. We acknowledge that evaluation on established temporal QA benchmarks would strengthen VDD's generalizability claims (see Section~\ref{sec:future-work}).

\subsection{Concept Drift and Stream Processing}
\label{sec:concept-drift}

The concept drift literature provides theoretical foundations. Gama et al.\ \cite{gama2014} survey adaptation strategies: blind approaches (fixed-rate forgetting), informed approaches (drift detection), and ensemble methods. VDD combines informed detection (embedding distance) with sigmoid-modulated blind adaptation.

Lu et al.\ \cite{lu2019} categorize drift as sudden, gradual, incremental, or recurring. Our experiments cover all four, with VDD showing particular strength on mixed patterns. ADWIN \cite{bifet2007} provides adaptive windowing for scalar streams; we show it fails on high-dimensional embeddings (Section~\ref{sec:hash-real}).

\subsection{Knowledge Editing and Continual Learning}
\label{sec:knowledge-editing}

Knowledge editing methods (ROME, \cite{meng2022}; MEMIT, \cite{meng2023}) modify model parameters to update specific facts. WISE \cite{wang2024} enables continual knowledge editing without catastrophic forgetting by maintaining a dual parametric memory with a routing mechanism. These operate on parametric knowledge; VDD operates on non-parametric retrieval memory---the approaches are complementary. A system could use WISE for high-priority individual fact updates while VDD manages the broader temporal validity of the retrieval corpus.

Continual learning addresses the stability--plasticity dilemma \cite{kirkpatrick2017}: learning new information without catastrophically forgetting old. Progressive networks \cite{rusu2016} and PackNet \cite{mallya2018} approach this from the parametric side. VDD addresses it on the retrieval side, dynamically adjusting how aggressively past retrievals are discounted.

\subsection{Positioning VDD Among Contemporary Systems}
\label{sec:positioning}

The following table summarizes how VDD relates to contemporary temporal memory approaches:

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{System} & \textbf{Layer} & \textbf{Temporal Mech.} & \textbf{Drift Aware.} & \textbf{VDD Rel.} \\
\midrule
\textbf{VDD} (ours) & Retrieval scoring & Sigmoid decay & Adaptive (emb.\ dist.) & --- \\
ARM \cite{bursa2026} & Retrieval strategy & Strategy switching & Context-dependent & Complementary \\
T-GRAG \cite{li2025} & Knowledge struct. & Temporal graph & Structural & Complementary \\
HippoRAG \cite{gutierrez2024} & Memory indexing & KG association & None & Complementary \\
MemoRAG \cite{qian2025} & Global memory & LLM summaries & None & Complementary \\
Mem0 \cite{chhikara2025} & Memory storage & Static TTL & Fixed & VDD extends \\
FadeMem \cite{wei2026} & Agent memory & Ebbinghaus decay & Static & VDD extends \\
WISE \cite{wang2024} & Parametric mem. & Dual mem.\ routing & Per-edit & Complementary \\
Self-RAG \cite{asai2024} & Generation time & Reflection tokens & None & Complementary \\
CRAG \cite{yan2024} & Retrieval corr. & Web search fallback & None & Complementary \\
\bottomrule
\end{tabular}
\caption{VDD in the context of contemporary temporal memory approaches.}
\label{tab:positioning}
\end{table}

%% ============================================================
\section{The VDD Method}
\label{sec:method}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig1_architecture.png}
\caption{VDD architecture. Incoming documents update an embedding centroid; the distance between recent and archival centroids drives a sigmoid-modulated decay rate $\lambda(t)$. High volatility triggers aggressive forgetting; stability preserves memories.}
\label{fig:architecture}
\end{figure}

\subsection{Cognitive-Theoretic Foundation}
\label{sec:cognitive-foundation}

Our model draws structural inspiration from two sources:

\textbf{Control theory.} In Recursive Least Squares (RLS) adaptive filtering \cite{haykin2002}, a forgetting factor weights past observations: values near 0 yield slow learning/forgetting (stability), while values near 1 yield fast learning/forgetting (plasticity). We map this structural pattern to VDD's adaptive decay. \textbf{We explicitly note that we do not claim RLS convergence properties transfer to VDD.} The analogy motivates the functional form; the experimental results validate it independently.

\textbf{Cognitive psychology.} Ebbinghaus's \cite{ebbinghaus1885} forgetting curve established that memory retention decays exponentially over time, but the rate depends on environmental factors. Wixted \cite{wixted2004} showed that forgetting rates increase when the environment changes---novel stimuli accelerate the decay of outdated associations, a process linked to hippocampal consolidation dynamics. VDD operationalizes this principle: environmental novelty (detected via embedding drift) accelerates the forgetting rate, while stability preserves existing memories.

We map this to the Retrieval Score ($S$) for a memory $m$ at time $t$:
\begin{equation}
S(m, t) = \text{Sim}(q, m) \cdot \exp(-\lambda(t) \cdot \Delta t)
\label{eq:retrieval-score}
\end{equation}
where $\text{Sim}$ is semantic similarity and $\lambda(t)$ is the adaptive decay coefficient.

The structural mapping from RLS to VDD is:

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{RLS Component} & \textbf{VDD Analog} \\
\midrule
Forgetting factor $\lambda$ & Decay coefficient $\lambda(t)$ \\
Innovation signal & Embedding centroid distance $V_t$ \\
Variable forgetting factor & Sigmoid activation on $V_t$ \\
Stability--plasticity tradeoff & Conservative vs.\ aggressive decay \\
\bottomrule
\end{tabular}
\caption{Structural mapping from RLS adaptive filtering to VDD.}
\label{tab:rls-mapping}
\end{table}

\subsection{Volatility Detection via Embedding Distance}
\label{sec:volatility-detection}

We evaluated three drift detectors: ADWIN \cite{bifet2007}, Page--Hinkley \cite{page1954}, and Embedding Distance. Traditional methods completely fail on high-dimensional embedding streams because they expect scalar distributions.

The \textbf{Embedding Distance} detector computes volatility as the cosine distance between current and archival embedding centroids:
\begin{equation}
V_t = 1 - \cos(\mu_{\text{curr}}, \mu_{\text{arch}})
\label{eq:volatility}
\end{equation}
where $\mu_{\text{curr}}$ is the centroid of recent embeddings (window size $w_{\text{curr}}$) and $\mu_{\text{arch}}$ is the centroid of archival embeddings (window size $w_{\text{arch}}$). This achieves 100\% recall on regime shifts with 0.12ms latency.

\textbf{Computational Complexity}: The VDD pipeline has time complexity $O(T \cdot d)$ for $T$ timesteps with embedding dimension $d$, assuming constant-size sliding windows. Each timestep requires centroid update in $O(d)$, cosine distance in $O(d)$, and sigmoid evaluation in $O(1)$.

\textbf{Relation to MMD}: Our centroid-distance approach can be viewed as a mean-embedding approximation of MMD with a linear kernel. Full MMD provides distributional sensitivity (detecting variance changes, not just mean shifts) at $O(n^2)$ cost. We trade distributional completeness for $O(d)$ per-step latency suitable for real-time RAG queries.

\subsection{The Adaptive Decay Function}
\label{sec:adaptive-decay}

We apply sigmoid activation to modulate decay rate:
\begin{equation}
\lambda(t) = \lambda_{\text{base}} + (\lambda_{\text{max}} - \lambda_{\text{base}}) \cdot \sigma(k \cdot (V_t - V_0))
\label{eq:adaptive-decay}
\end{equation}
where:
\begin{itemize}
  \item $\lambda_{\text{base}} \in [0.2, 0.3]$ is the resting decay rate
  \item $\lambda_{\text{max}} \in [0.9, 0.95]$ is the ``panic'' rate during regime changes
  \item $k \in [5, 10]$ controls sigmoid steepness
  \item $V_0 \in [0.1, 0.3]$ is the volatility threshold
\end{itemize}

\subsection{Activation Function Selection}
\label{sec:activation-selection}

A natural question is whether the sigmoid activation in Equation~(\ref{eq:adaptive-decay}) is justified over alternatives. We conducted a systematic ablation (Experiment~37, Section~\ref{sec:activation-ablation}) testing five activation functions---sigmoid, linear, exponential, step, and softplus---across $k$ values $\{1, 2, 3, 5, 10\}$.

\textbf{Key finding}: At the recommended $k \geq 5$, \textbf{all activations converge to equivalent performance} (Cohen's $d \to 0$ between any pair). The choice of activation function is immaterial at recommended operating parameters.

\begin{table}[h]
\centering
\begin{tabular}{@{}cccccc@{}}
\toprule
$k$ & Sigmoid & Linear & Exponential & Step & Softplus \\
\midrule
1 & 14.44 & 12.90 & 15.81 & 11.08 & 11.08 \\
2 & 13.28 & 11.08 & 13.47 & 11.08 & 11.08 \\
3 & 12.24 & 11.08 & 12.35 & 11.08 & 11.08 \\
5 & 11.25 & 11.08 & 11.25 & 11.08 & 11.08 \\
10 & 11.08 & 11.08 & 11.08 & 11.08 & 11.08 \\
\bottomrule
\end{tabular}
\caption{Mean IAE across 4 drift scenarios by activation function and $k$ value ($n=30$). All activations converge at $k \geq 5$.}
\label{tab:activation-iae}
\end{table}

At $k=1$, step and softplus outperform sigmoid ($d = -1.052$), and linear outperforms sigmoid ($d = -0.518$). However, this low-$k$ regime produces sluggish transitions that are undesirable in practice.

We retain sigmoid for three reasons despite its equivalent performance at recommended $k$:
\begin{enumerate}
  \item \textbf{Bounded output}: sigmoid maps to $[0, 1]$ without clamping, preventing numerical issues.
  \item \textbf{Interpretable threshold}: $V_0$ has a clear meaning as the sigmoid midpoint, directly corresponding to the ``volatility level at which decay is half-activated.''
  \item \textbf{Smooth gradient}: Unlike step (discontinuous) or exponential (unbounded), sigmoid provides smooth, monotonic transitions suitable for gradient-based extensions.
\end{enumerate}

\subsection{Algorithm}
\label{sec:algorithm}

\begin{verbatim}
Algorithm: Volatility-Driven Decay Retrieval
Input: Query q, memory bank M, window sizes w_curr, w_arch
Output: Retrieved documents D

1. e_t <- embed(q)
2. mu_curr <- centroid(recent[t - w_curr : t])
3. mu_arch <- centroid(archival[t - w_arch : t - w_curr])
4. V_t <- 1 - cos(mu_curr, mu_arch)
5. lambda(t) <- lambda_base + (lambda_max - lambda_base)
                * sigma(k * (V_t - V_0))
6. For each m in M:
     S(m) <- Sim(q, m) * exp(-lambda(t) * (t - t_m))
7. D <- top_k(M, S)
8. Return D
\end{verbatim}

\subsection{Hash-Based Embeddings and Real-Embedding Transfer}
\label{sec:hash-real}

Our synthetic experiments use hash-based embeddings (SHA-256 $\to$ 64-dim vectors with Gaussian noise) rather than neural embeddings. This is a deliberate methodological choice:

\begin{itemize}
  \item \textbf{Deterministic}: Identical inputs produce identical embeddings across runs, ensuring exact reproducibility without API dependencies.
  \item \textbf{Isolates the mechanism}: Hash embeddings test VDD's drift detection and decay logic without conflating results with embedding model quality.
\end{itemize}

\textbf{Validation via Experiment~36}: We conducted a systematic comparison of hash-based (64-dim) vs.\ real neural embeddings (nomic-embed-text, 768-dim) across 5 drift scenarios, 5 methods, and $n=30$ seeds. Results:

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Ranking agreement & 4/5 scenarios \\
Effect size correlation & $r = 0.935$ \\
Mean effect size ratio (real/hash) & 0.70x \\
\bottomrule
\end{tabular}
\caption{Hash vs.\ real embedding transfer validation.}
\label{tab:transfer-validation}
\end{table}

Rankings are preserved in 4/5 scenarios between hash and real embeddings. The one disagreement occurs in the reversion scenario where all methods produce similar accuracy with real embeddings. Real embeddings produce slightly lower absolute accuracy ($\Delta$ -0.02 to -0.05 typical), and---critically---\textbf{effect sizes attenuate} with real embeddings (mean ratio 0.70x), indicating that hash-based controlled-condition results are directionally valid but inflated.

\textbf{Summary of real-embedding accuracy across scenarios ($n=30$)}:

\begin{table}[h]
\centering
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Scenario} & \textbf{VDD} & \textbf{Recency} & \textbf{Static} & \textbf{Time-Weighted} & \textbf{No Decay} \\
\midrule
Regime shifts & 0.584 & 0.595 & 0.748 & 0.871 & 0.333 \\
Mixed drift & 0.584 & 0.595 & 0.748 & 0.871 & 0.333 \\
Bursty & 0.594 & 0.607 & 0.757 & 0.902 & 0.333 \\
Reversion & 0.563 & 0.563 & 0.580 & 0.571 & 0.667 \\
Gradual & 0.586 & 0.598 & 0.747 & 0.845 & 0.333 \\
\bottomrule
\end{tabular}
\caption{Real-embedding accuracy across scenarios ($n=30$).}
\label{tab:real-embedding-accuracy}
\end{table}

\textbf{Honest assessment}: With real embeddings, time-weighted consistently dominates (0.845--0.902), and static decay outperforms both VDD and recency in absolute accuracy. VDD and recency perform similarly ($\Delta < 0.02$). The reversion scenario is the closest, with no\_decay performing best (0.667) due to the scenario structure. Synthetic results demonstrate the mechanism; real-embedding experiments provide ground-truth validation that shows attenuated but directionally consistent effects.

\subsection{Theoretical Analysis}
\label{sec:theory}

The empirical finding that VDD is ``never the worst'' across drift patterns suggests an underlying structural property. In this section we provide a theoretical sketch---not a formal proof---that formalizes this intuition using a regret framework. The experimental results (42 experiments) remain the primary evidence; this analysis offers theoretical motivation for why adaptive decay is structurally preferable to any fixed rate under uncertainty.

\subsubsection{Regret Framework}

Let $R(\lambda, D)$ denote the cumulative retrieval error (e.g., integrated absolute error) incurred by a fixed decay rate $\lambda$ on drift pattern $D$. An oracle with foreknowledge of $D$ selects the optimal fixed rate $\lambda^*(D) = \arg\min_\lambda R(\lambda, D)$. VDD's time-varying $\lambda(t)$ produces error $R_{\text{VDD}}(D)$ for any pattern $D$. We define regret as the excess error over the oracle:

\begin{equation}
\text{Regret}(D) = R_{\text{VDD}}(D) - R(\lambda^*(D), D)
\end{equation}

A method is robust if its worst-case regret across all patterns is bounded. A method is ``never worst'' if, for every pattern $D$, there exists some fixed-$\lambda$ baseline with higher error.

\subsubsection{Worst-Case Regret Bound}

Any fixed $\lambda_{\text{fixed}}$ is arbitrarily bad for some drift pattern:

\begin{itemize}
  \item \textbf{Low $\lambda$ (conservative)}: Under sudden drift, stale memories persist. Staleness accumulates linearly with time, so $R(\lambda_{\text{fixed}}, D_{\text{sudden}})$ grows without bound as drift frequency increases.
  \item \textbf{High $\lambda$ (aggressive)}: Under reversion, valid memories are destroyed prematurely. Our experiments confirm this: aggressive decay ($\lambda = 0.5$) produces IAE 52.1 vs.\ VDD's 35.4 in reversion scenarios.
\end{itemize}

VDD's regret is bounded because its output is structurally constrained. For a drift detector with false positive rate $\alpha$ and true positive rate (recall) $\beta$, the worst-case excess $\lambda$ at any timestep is bounded by $|\lambda(t) - \lambda^*| \leq (\lambda_{\max} - \lambda_{\text{base}})$. This worst case occurs only when the detector is wrong. At each timestep, the expected excess decay decomposes into two error modes:

\begin{enumerate}
  \item \textbf{Missed drift} (probability $1 - \beta$): VDD stays near $\lambda_{\text{base}}$ when it should be near $\lambda_{\max}$, contributing excess staleness of at most $(\lambda_{\max} - \lambda_{\text{base}}) \cdot (1 - \beta)$.
  \item \textbf{False alarm} (probability $\alpha$): VDD activates near $\lambda_{\max}$ when it should stay near $\lambda_{\text{base}}$, contributing excess forgetting of at most $(\lambda_{\max} - \lambda_{\text{base}}) \cdot \alpha$.
\end{enumerate}

Over a time horizon $T$, this yields:

\begin{equation}
\max_D \text{Regret}_{\text{VDD}}(D) \leq T \cdot (\lambda_{\max} - \lambda_{\text{base}}) \cdot [(1 - \beta) + \alpha]
\end{equation}

With our detector's measured rates ($\alpha = 0.135$, $\beta = 1.0$), the per-step excess is bounded by $(0.9 - 0.05) \cdot (0 + 0.135) = 0.115$. Compare this to a mismatched fixed $\lambda$, where per-step excess can reach $(\lambda_{\max} - \lambda_{\text{base}}) = 0.85$---over 7$\times$ worse.

\subsubsection{Minimax Optimality Intuition}

VDD approximates the \textbf{minimax strategy}: minimize the maximum regret across all possible drift patterns. Formally, VDD targets:

\begin{equation}
\min_{\text{strategy}} \max_D \text{Regret}(\text{strategy}, D)
\end{equation}

Any fixed-$\lambda$ strategy has unbounded maximum regret (there always exists a pathological drift pattern). VDD's adaptive strategy bounds the maximum regret at the cost of not being optimal for any single known pattern. This is precisely the ``never worst'' property: VDD sacrifices best-case performance for worst-case robustness.

Our experiments quantify this tradeoff. Fixed strategies show 5--20$\times$ IAE variation across drift patterns (e.g., aggressive decay: IAE 11.1 on sudden drift vs.\ 52.1 on reversion). VDD's IAE varies by less than 3$\times$ across the same patterns (range: 11.8--35.4), confirming bounded worst-case behavior at the expense of optimality in any single scenario.

\subsubsection{Detector Convergence}

The centroid distance $V_t = 1 - \cos(\mu_{\text{curr}}, \mu_{\text{arch}})$ is a consistent estimator of distributional shift. By the law of large numbers, as window sizes grow, $\mu_{\text{curr}} \to \mathbb{E}[e_t \mid \text{recent}]$ and $\mu_{\text{arch}} \to \mathbb{E}[e_t \mid \text{archival}]$ (a.s.), so $V_t \to V^*$, the true cosine distance between population means. By the central limit theorem, the convergence rate is $O(1 / \sqrt{\min(w_{\text{curr}}, w_{\text{arch}})})$. With default windows $w_{\text{curr}} = 20$, $w_{\text{arch}} = 100$, the estimation error is approximately $O(1/\sqrt{20}) \sim 0.22$ standard deviations---sufficient for the binary high/low distinction VDD requires, but not for fine-grained volatility estimation.

\subsubsection{Limitations of This Analysis}

We are explicit about what this analysis does and does not establish:

\begin{itemize}
  \item \textbf{This is a sketch, not a formal proof.} The regret bound assumes the detector's error rates ($\alpha$, $\beta$) are known and stationary. Real deployments face non-stationary false positive rates.
  \item \textbf{The bound is loose.} Tighter analysis would require distributional assumptions about drift patterns. We avoid such assumptions to maintain generality.
  \item \textbf{Minimax optimality is claimed as intuition, not proven.} A formal minimax proof would require characterizing the class of admissible drift patterns and showing VDD is optimal within that class.
  \item \textbf{The detector convergence result is asymptotic.} With $w_{\text{curr}} = 20$, finite-sample effects are non-negligible.
\end{itemize}

We provide this analysis to explain \emph{why} VDD's ``never worst'' property is structurally expected, not coincidental. The experimental results across 42 experiments, 120 real-world facts, and 13 comparison methods remain the primary evidence.

%% ============================================================
\section{Experimental Validation}
\label{sec:experiments}

We conducted 42 experiments; key results from 37 experiments are presented here (Experiments~34--42 are new in this revision). All experiments use bootstrap 95\% CIs (1000 samples) and Cohen's $d$ effect sizes; $n=30$ experiments use explicit seeds 100--129. All $p$-values are corrected for multiple comparisons using Benjamini--Hochberg FDR (Section~\ref{sec:stat-methodology}).

\subsection{Statistical Methodology}
\label{sec:stat-methodology}

\subsubsection{Sample Sizes and Inference}

\begin{itemize}
  \item \textbf{Sample sizes}: $n = 30$ for experiments 19--37; $n = 10$--20 for experiments 1--18 with confirmatory $n = 30$ reruns.
  \item \textbf{Bootstrap CIs}: 95\% CI via 1000 bootstrap samples (percentile method).
  \item \textbf{Cohen's $d$}: Small $|d| < 0.5$, medium $0.5 \leq |d| < 0.8$, large $|d| \geq 0.8$.
  \item \textbf{Paired comparisons}: Paired by random seed to reduce variance.
\end{itemize}

\subsubsection{Multiple Comparison Correction (Experiment~34)}

Across all experiments, we conducted \textbf{88 hypothesis tests}. Without correction, 85 tests achieved $p < 0.05$. After applying Benjamini--Hochberg FDR correction at $\alpha = 0.05$:

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total comparisons & 88 \\
Significant (uncorrected) & 85 \\
Significant (BH-corrected) & 85 \\
Lost to correction & 0 \\
FDR alpha & 0.05 \\
\bottomrule
\end{tabular}
\caption{Multiple comparison correction results.}
\label{tab:bh-correction}
\end{table}

All 85 significant results survive correction. The three non-significant comparisons were already non-significant before correction.

We designate \textbf{5 primary comparisons} (pre-registered hypotheses) and \textbf{83 exploratory comparisons} (post-hoc analyses):

\textbf{Primary comparisons:}
\begin{enumerate}
  \item VDD achieves lower IAE than static baselines in regime shifts
  \item VDD achieves lower staleness than recency in real-world data
  \item VDD's $\lambda$ is genuinely bimodal (not static with noise)
  \item VDD ranks consistently \#2/10 across drift patterns
  \item Real embeddings preserve VDD's ranking advantage
\end{enumerate}

All 5 primary comparisons are significant after BH correction.

\subsubsection{Type~M Error Analysis}

With $n=30$, our study is adequately powered to detect large effects but may exaggerate smaller ones. We report Type~M (magnitude) error analysis following Gelman and Carlin \cite{gelman2014}:

\begin{table}[h]
\centering
\begin{tabular}{@{}lccl@{}}
\toprule
\textbf{Assumed True $d$} & \textbf{Power ($n=30$)} & \textbf{Exaggeration Ratio} & \textbf{Type~S Rate} \\
\midrule
0.2 (small) & 11.4\% & 3.21x & 2.9\% \\
0.3 & 20.1\% & 2.21x & 0.5\% \\
0.5 (medium) & 47.3\% & 1.43x & 0.0\% \\
0.8 (large) & 86.3\% & 1.08x & 0.0\% \\
1.0 & 96.8\% & 1.02x & 0.0\% \\
\bottomrule
\end{tabular}
\caption{Type~M error analysis. If the true effect is medium ($d=0.5$), our study would detect it 47\% of the time and exaggerate the observed $d$ by 1.43x when significant.}
\label{tab:type-m}
\end{table}

\textbf{Interpretation}: If the true effect of VDD vs.\ a baseline is medium ($d=0.5$), our study would detect it 47\% of the time and exaggerate the observed $d$ by 1.43x when significant. For the large effects we frequently observe ($d > 1.0$), exaggeration is minimal ($< 1.02$x). However, for comparisons where the true effect is small ($d \sim 0.2$), any significant result would be inflated approximately 3.2x. \textbf{We caution readers that observed effect sizes in controlled simulations should be interpreted as upper bounds on production deployment effects.}

\subsection{Core Validation: Error Reduction and Ranking (Experiments~2, 8)}
\label{sec:core-validation}

Using Embedding Distance detection (not oracle), VDD achieves \textbf{87.5\% error reduction} versus static decay (IAE 15.81 vs 126.67) with regime shifts detected in real-time.

Against 10 practitioner-relevant baselines ($n = 30$), VDD ranks \#2 in all four drift patterns with 100\% rank stability:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{Regime Shifts} & \textbf{Mixed Drift} & \textbf{Bursty} & \textbf{Reversion} \\
\midrule
\textbf{Recency ($\lambda$=0.5)} & \textbf{36.5} & \textbf{40.2} & \textbf{37.5} & \textbf{36.4} \\
\textbf{VDD} & \textbf{63.6} & \textbf{43.3} & \textbf{69.5} & \textbf{62.2} \\
Static ($\lambda$=0.2) & 104.8 & 111.2 & 109.5 & 103.7 \\
Static ($\lambda$=0.1) & 195.2 & 198.4 & 207.2 & 190.7 \\
SlidingWindow (N=20) & 225.2 & 238.9 & 233.5 & 222.1 \\
TimeWeighted ($\alpha$=0.05) & 351.9 & 343.8 & 385.0 & 339.2 \\
SlidingWindow (N=50) & 472.3 & 468.9 & 501.4 & 457.6 \\
TimeWeighted ($\alpha$=0.02) & 695.3 & 673.7 & 827.1 & 643.7 \\
SlidingWindow (N=100) & 822.6 & 795.0 & 901.8 & 784.9 \\
TimeWeighted ($\alpha$=0.01) & 1003.2 & 986.1 & 1393.4 & 853.0 \\
\bottomrule
\end{tabular}
\caption{IAE across 10 methods and 4 drift patterns ($n = 30$). Bold = top 2.}
\label{tab:iae-10methods}
\end{table}

VDD outperforms time-weighted methods by 5--20x and sliding windows by 3--13x. In mixed drift, VDD nearly matches recency (43.3 vs 40.2, 7.7\% gap).

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/02_simulation_real.png}
\caption{VDD with real-time Embedding Distance detection vs.\ static decay ($\lambda=0.1$). Top: true parameter trajectory with detected regime shifts. Middle: tracking error over time. Bottom: cumulative IAE showing 87.5\% error reduction. VDD rapidly adapts at drift points while static decay accumulates persistent error.}
\label{fig:core-validation}
\end{figure}

\subsection{Robustness Across Drift Patterns and Decision Framework (Experiments~4, 15, 16, 17, 20, 28)}
\label{sec:robustness}
\label{sec:decision}

VDD's consistency across scenarios is its primary practical value:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Drift Pattern} & \textbf{VDD Performance} & \textbf{Best Alternative} & \textbf{VDD Verdict} \\
\midrule
Mixed/Unknown & IAE 43.3, \#2/10 & Recency 40.2 & \textbf{Recommended} (7.7\% gap) \\
Reversion & IAE 35.4, $d = +1.22$ & --- & \textbf{Recommended} \\
Bursty permanent & IAE 48.2, $d = -2.90$ & Recency 28.7 & Not recommended \\
Gradual predictable & Acc 0.621 & Time-weighted 0.899 & Not recommended \\
Stable environment & Near-baseline $\lambda$ & Static $\lambda \sim 0.1$ & Acceptable \\
\bottomrule
\end{tabular}
\caption{VDD performance across drift patterns with recommendations.}
\label{tab:drift-recommendations}
\end{table}

\textbf{Honest Finding}: Recency outperforms VDD in permanent high-drift ($d = -2.90$). VDD's value is for the common case where drift pattern is unknown and may include reversions.

\textbf{Full practitioner decision framework}:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Condition} & \textbf{Recommendation} & \textbf{Evidence} \\
\midrule
Drift pattern unknown/mixed & \textbf{VDD} & \#2/13, never worst \\
Avoiding stale knowledge critical & \textbf{VDD} & $d = +1.85$ vs recency \\
Reversions possible & \textbf{VDD} & $d = +1.22$ vs recency \\
Tuned parameters available & \textbf{VDD} (cw=3, aw=200, $V_0$=0.1) & IAE 12.6 \\
Constant high drift & Recency ($\lambda \geq 0.5$) & Simplest, \#1 \\
Gradual, predictable transitions & Time-weighted ($\alpha = 0.01$) & 0.899 accuracy \\
Known stable environment & Static ($\lambda \sim 0.1$) & Maintains trust \\
Time series domain & Holt--Winters & Competitive in 3/4 \\
\textbf{Avoid} & Sliding window ($N > 50$) & Catastrophic failures \\
\textbf{Avoid} & Time-weighted on sudden drift & 5--20x worse \\
\bottomrule
\end{tabular}
\caption{Full practitioner decision framework for method selection.}
\label{tab:decision-framework}
\end{table}

Figure~\ref{fig:decision-tree} provides a visual decision tree summarizing this framework, guiding practitioners from their use-case characteristics to the recommended decay method.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/42_decision_tree.png}
\caption{Practitioner decision tree for decay method selection. VDD is highlighted as the safest default when drift patterns are unknown or mixed. Based on evidence from 42 experiments across 13 methods.}
\label{fig:decision-tree}
\end{figure}

\subsection{Bimodality: Proof of Genuine Adaptation (Experiments~19, 30)}
\label{sec:bimodality}

A critical question is whether VDD's $\lambda$ is genuinely bimodal or merely static with noise. We logged $\lambda(t)$ across five scenarios and applied four statistical tests ($n = 30$):

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Scenario} & \textbf{Bimod.\ Coeff.} & \textbf{Dip Test $p$} & \textbf{Ashman's $D$} & \textbf{Silverman} & \textbf{Verdict} \\
\midrule
Pure Stable & 0.578 & 0.021 & 0.0001 & 1 mode & Unimodal* \\
Pure Drift & 0.884 & $< 0.001$ & 2.878 & 2 modes & \textbf{Bimodal} \\
Mixed (70/30) & 0.954 & $< 0.001$ & 2.420 & 2 modes & \textbf{Bimodal} \\
Bursty & 0.981 & $< 0.001$ & 2.130 & 2 modes & \textbf{Bimodal} \\
Reversion & 0.964 & $< 0.001$ & 2.262 & 2 modes & \textbf{Bimodal} \\
\bottomrule
\end{tabular}
\caption{Bimodality test results. *Pure stable passes BC but fails Ashman's $D$---BC alone is insufficient.}
\label{tab:bimodality}
\end{table}

VDD beats a static oracle calibrated to its own mean $\lambda$ in mixed drift (113.8 vs 362.8, 3.2x improvement), proving that the \emph{timing} of elevated $\lambda$ matters beyond the mean value.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/21_effective_lambda.png}
\caption{Effective $\lambda(t)$ distributions across five drift scenarios ($n=30$). VDD produces genuinely bimodal distributions (confirmed by Ashman's $D > 2$, Dip test $p < 0.001$) in all scenarios except pure-stable, demonstrating context-dependent switching between conservative and aggressive decay.}
\label{fig:bimodal-lambda}
\end{figure}

\subsection{Extended Baselines: Original Adaptive Methods (Experiment~29)}
\label{sec:adaptive-baselines}

We compared VDD against adaptive methods from the time series literature: EMA-lambda and Holt--Winters exponential smoothing \cite{holt2004}, plus Dynamic Weighted Majority \cite{kolter2007}.

\begin{table}[h]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{Regime} & \textbf{Mixed} & \textbf{Bursty} & \textbf{Reversion} \\
\midrule
\textbf{Recency (0.5)} & \textbf{1} & \textbf{1} & \textbf{1} & \textbf{1} \\
Holt--Winters & 2 & 4 & 2 & 2 \\
EMA-lambda ($\alpha$=0.2) & 3 & 3 & 3 & 3 \\
EMA-lambda ($\alpha$=0.1) & 4 & 5 & 4 & 4 \\
\textbf{VDD} & 5 & \textbf{2} & 5 & 5 \\
DWM-lite & 6 & 6 & 6 & 6 \\
Static (0.1) & 7 & 7 & 7 & 7 \\
\bottomrule
\end{tabular}
\caption{Ranks against adaptive baselines ($n = 30$). Bold = notable.}
\label{tab:adaptive-ranks}
\end{table}

\textbf{Honest Finding}: Holt--Winters and EMA-lambda beat VDD in 3/4 scenarios. However, VDD excels in mixed drift (\#2 vs Holt--Winters' \#4), the most common production condition. VDD always beats DWM-lite and static, maintaining its never-worst property.

\subsection{Extended Baselines: Additional Methods (Experiment~37, Part~2)}
\label{sec:extended-baselines}

Responding to reviewer feedback on missing baselines, we added three additional comparison methods: \textbf{LRU} (Least Recently Used eviction), \textbf{Timestamp Freshness} (scoring by inverse age), and \textbf{Online Lambda} (an adaptive decay method that learns $\lambda$ from prediction error). All methods were tested at $k=5$ across four drift scenarios ($n=30$):

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Method} & \textbf{Regime} & \textbf{Mixed} & \textbf{Bursty} & \textbf{Gradual} & \textbf{Mean Rank} \\
\midrule
\textbf{Online Lambda} & 4.03 & 13.07 & 2.33 & 24.90 & \textbf{1.0} \\
\textbf{VDD (sigmoid)} & 4.37 & 13.30 & 2.40 & 24.93 & \textbf{2.0} \\
No Decay & 6.90 & 20.57 & 2.87 & 40.57 & 3.5 \\
LRU & 6.90 & 20.57 & 2.87 & 40.57 & 4.5 \\
Recency & 13.23 & 24.77 & 7.70 & 35.03 & 5.0 \\
Timestamp Freshness & 12.13 & 29.47 & 6.53 & 37.23 & 5.0 \\
Time-Weighted & 15.27 & 38.53 & 7.83 & 55.20 & 7.0 \\
Static & 16.47 & 44.50 & 8.63 & 73.00 & 8.0 \\
\bottomrule
\end{tabular}
\caption{IAE across 8 methods and 4 drift patterns with extended baselines ($n=30$, $k=5$). Lower IAE is better.}
\label{tab:extended-iae}
\end{table}

\textbf{VDD vs.\ new baselines (Cohen's $d$, positive = VDD better):}

\begin{table}[h]
\centering
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Baseline} & \textbf{Mean $d$} & \textbf{Regime} & \textbf{Mixed} & \textbf{Bursty} & \textbf{Gradual} \\
\midrule
LRU & +0.921 & +0.888 & +1.350 & +0.228 & +1.218 \\
Timestamp Freshness & +2.435 & +3.151 & +2.985 & +2.368 & +1.238 \\
Online Lambda & -0.056 & -0.129 & -0.054 & -0.038 & -0.003 \\
\bottomrule
\end{tabular}
\caption{Cohen's $d$ for VDD vs.\ new baselines (positive = VDD better).}
\label{tab:vdd-vs-new}
\end{table}

VDD substantially outperforms LRU ($d = +0.921$, large effect) and timestamp freshness ($d = +2.435$, very large effect). The online\_lambda baseline---which also adapts its decay rate based on prediction error---matches VDD almost exactly ($d = -0.056$, negligible effect). This is a noteworthy result: \textbf{it validates the core insight that adaptive decay is valuable}, while showing that the specific mechanism for adaptation (sigmoid on embedding distance vs.\ prediction-error learning) matters less than the decision to adapt at all. We retain VDD's embedding-distance approach because it requires no labeled data and operates in $O(d)$ time, whereas prediction-error approaches require ground-truth feedback.

\subsection{Multi-Domain Real-World Validation (Experiments~21, 25, 31)}
\label{sec:multi-domain}

We validated on \textbf{120 facts across 3 domains}---React (60 facts, v16/v17/v18), Python stdlib (30 facts, v3.8/v3.10/v3.12), and Node.js (30 facts, v16/v18/v20)---using real embeddings (nomic-embed-text, 768-dim) via Ollama ($n = 30$):

\begin{table}[h]
\centering
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Method} & \textbf{React} & \textbf{Python} & \textbf{Node.js} & \textbf{Aggregate} & \textbf{Staleness} \\
\midrule
\textbf{Time-Weighted} & \textbf{0.871} & \textbf{0.879} & \textbf{0.876} & \textbf{0.875} & \textbf{0.000} \\
Static ($\lambda$=0.1) & 0.748 & 0.738 & 0.721 & 0.736 & 0.135 \\
\textbf{VDD} & \textbf{0.647} & \textbf{0.632} & \textbf{0.605} & \textbf{0.628} & \textbf{0.243} \\
Recency ($\lambda$=0.5) & 0.595 & 0.577 & 0.540 & 0.570 & 0.300 \\
Sliding Window & 0.595 & 0.333* & 0.333* & 0.421 & 0.398 \\
No Decay & 0.333 & 0.333 & 0.333 & 0.333 & 0.667 \\
\bottomrule
\end{tabular}
\caption{Three-domain validation ($n = 30$, 120 facts, real embeddings). *Sliding window catastrophically fails on Python and Node.js.}
\label{tab:three-domain}
\end{table}

\textbf{Key findings}:
\begin{enumerate}
  \item \textbf{VDD is cross-domain consistent}: accuracy 0.605--0.647 across all three domains.
  \item \textbf{VDD beats recency in all 3 domains} (aggregate $d = +2.13$, large effect) with lower staleness (0.243 vs 0.300).
  \item \textbf{Real embeddings strengthen VDD}: accuracy improves $0.617 \to 0.647$ and staleness drops $0.309 \to 0.221$ versus hash embeddings.
  \item \textbf{Sliding window is catastrophically fragile}: 0.333 on Python and Node.js (equivalent to random).
\end{enumerate}

\textbf{Honest Finding}: Time-weighted dominates on gradual transitions (0.875 accuracy). When version changes are predictable, fixed time-weighted decay is superior.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/33_three_domain.png}
\caption{Three-domain real-world validation ($n=30$, 120 facts, real embeddings via nomic-embed-text). Left: accuracy by domain and method. Right: staleness by domain. VDD achieves consistent cross-domain accuracy (0.605--0.647) with moderate staleness, while sliding window catastrophically fails on Python and Node.js.}
\label{fig:three-domain}
\end{figure}

\subsection{Real Embedding Suite: Hash vs.\ Neural Transfer (Experiment~36)}
\label{sec:real-embedding-suite}

To directly address concerns about synthetic data validity, we conducted a systematic comparison of hash-based (64-dim) versus real neural embeddings (nomic-embed-text, 768-dim) across 5 drift scenarios, 5 methods, and $n=30$ seeds (300 timesteps each, 136.5 seconds total computation):

\begin{table}[h]
\centering
\begin{tabular}{@{}llccc@{}}
\toprule
\textbf{Scenario} & \textbf{Metric} & \textbf{Hash} & \textbf{Real} & \textbf{Delta} \\
\midrule
Regime shifts & VDD accuracy & 0.619 & 0.584 & -0.035 \\
Regime shifts & Time-weighted accuracy & 0.891 & 0.871 & -0.020 \\
Bursty & VDD accuracy & 0.627 & 0.594 & -0.033 \\
Bursty & Time-weighted accuracy & 0.897 & 0.902 & +0.005 \\
Reversion & VDD accuracy & 0.535 & 0.563 & +0.028 \\
Gradual & VDD accuracy & 0.607 & 0.586 & -0.021 \\
\bottomrule
\end{tabular}
\caption{Hash vs.\ real embedding comparison ($n=30$, 300 timesteps each).}
\label{tab:hash-vs-real}
\end{table}

\textbf{Ranking preservation}: Method rankings (which method beats which) are preserved in 4/5 scenarios. The one disagreement occurs in the reversion scenario where effect sizes collapse and all methods produce similar accuracy.

\textbf{Effect size attenuation}: Real embeddings produce effect sizes that are 0.70x the hash-based values on average ($r = 0.935$ correlation). This means:
\begin{itemize}
  \item A hash-based $d = 3.0$ corresponds to approximately $d = 2.1$ with real embeddings
  \item A hash-based $d = 1.0$ corresponds to approximately $d = 0.7$ with real embeddings
  \item Directional conclusions remain valid; magnitudes should be interpreted as upper bounds
\end{itemize}

\textbf{Interpretation}: Hash embeddings serve as a ``conservative mechanism test''---they isolate VDD's drift detection and decay logic from embedding model quality. Real embeddings provide ground truth and show that the mechanism works with production-quality embeddings, though with attenuated effect sizes. Under controlled conditions with low variance, effect sizes are inflated; production deployments would show smaller but directionally consistent improvements.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/36_real_embedding_suite.png}
\caption{Hash vs.\ real embedding comparison across 5 drift scenarios ($n=30$). Bar charts show method accuracy side-by-side; bottom-right scatter plot shows effect size correlation ($r = 0.935$) confirming that hash-based rankings transfer to real embeddings with attenuated magnitudes (mean ratio 0.70x).}
\label{fig:real-embedding-suite}
\end{figure}

\subsection{Cross-Model Embedding Generalization (Experiment~41)}
\label{sec:cross-model}

To test whether VDD's results depend on the specific embedding model, we repeated the 5-scenario evaluation with a second model: mxbai-embed-large (1024-dim) alongside nomic-embed-text (768-dim). Across 5 scenarios, 5 methods, and 10 seeds:

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Spearman rank correlation & $\rho = 0.978$ ($p < 10^{-17}$) \\
Exact ranking agreement & 4/5 scenarios \\
Pearson correlation (accuracy) & $r = 0.999$ \\
Effect size correlation & $r = 0.937$ \\
\bottomrule
\end{tabular}
\caption{Cross-model embedding generalization: nomic-embed-text (768-dim) vs mxbai-embed-large (1024-dim).}
\label{tab:cross-model}
\end{table}

Method rankings are identical in 4/5 scenarios. The one disagreement is again the reversion scenario, where VDD and recency swap adjacent ranks (both within 0.001 accuracy). Absolute accuracy deltas between models are small (mean $|\Delta| = 0.011$), confirming that VDD's mechanism generalizes across embedding architectures.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/41_embedding_generalization.png}
\caption{Cross-model embedding generalization (nomic-embed-text vs mxbai-embed-large). Method rankings are preserved across embedding models with Spearman $\rho = 0.978$.}
\label{fig:cross-model}
\end{figure}

\subsection{Activation Function Ablation (Experiment~37, Part~1)}
\label{sec:activation-ablation}

We tested five activation functions (sigmoid, linear, exponential, step, softplus) across $k$ values $\{1, 2, 3, 5, 10\}$ on four drift scenarios ($n=30$, 500 timesteps):

\textbf{Cohen's $d$: Sigmoid vs.\ Alternative (negative = alternative better)}:

\begin{table}[h]
\centering
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Alternative} & $k$\textbf{=1} & $k$\textbf{=2} & $k$\textbf{=3} & $k$\textbf{=5} & $k$\textbf{=10} \\
\midrule
Linear & -0.518 & -0.669 & -0.350 & -0.056 & 0.000 \\
Exponential & +0.385 & +0.087 & +0.033 & 0.000 & 0.000 \\
Step & -1.052 & -0.669 & -0.350 & -0.056 & 0.000 \\
Softplus & -1.052 & -0.669 & -0.350 & -0.056 & 0.000 \\
\bottomrule
\end{tabular}
\caption{Cohen's $d$: Sigmoid vs.\ alternative activations. At $k \geq 5$, all converge ($d \to 0$).}
\label{tab:activation-ablation}
\end{table}

At $k \geq 5$, \textbf{all activations converge} ($d \to 0$, equivalent performance). The convergence occurs because high $k$ values produce near-binary switching behavior regardless of the activation shape: once $k$ is large enough, the sigmoid effectively becomes a step function, and all smooth activations approach the same behavior.

At $k = 1$, step/softplus substantially outperform sigmoid ($d = -1.052$). However, $k = 1$ produces sluggish decay transitions and is outside the recommended operating range.

\textbf{Conclusion}: Activation choice is immaterial at recommended $k$. We retain sigmoid for interpretability, bounded output, and smooth gradients (Section~\ref{sec:activation-selection}).

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/37_activation_and_baselines.png}
\caption{Activation function ablation and extended baselines (Experiment~37). Left: mean IAE heatmap by activation and $k$; all activations converge at $k \geq 5$. Center: Cohen's $d$ (sigmoid vs.\ alternatives) approaches zero at high $k$. Right: all baselines ranked at $k=5$---VDD and online\_lambda share top positions.}
\label{fig:activation-baselines}
\end{figure}

\subsection{LLM-in-the-Loop Validation (Experiments~22, 26, 35)}
\label{sec:llm-validation}

To validate that proxy metrics predict end-to-end quality, we replaced deterministic scoring with actual LLM generation (llama3.1:8b) and real embeddings (nomic-embed-text). We expanded from the original $n=5$ to $n=50$ seeds (7{,}500 total LLM calls):

\begin{table}[h]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{LLM Quality ($n$=50)} & \textbf{95\% CI} & \textbf{SD} & \textbf{Determ.\ Quality} \\
\midrule
\textbf{VDD} & \textbf{0.522} & [0.522, 0.522] & 0.000 & 0.567 \\
\textbf{Recency} & \textbf{0.522} & [0.522, 0.522] & 0.000 & 0.567 \\
Time-weighted & 0.521 & [0.511, 0.533] & 0.038 & 1.000 \\
Static & 0.490 & [0.479, 0.500] & 0.038 & 1.000 \\
No decay & 0.473 & [0.473, 0.473] & 0.000 & 0.333 \\
\bottomrule
\end{tabular}
\caption{LLM-in-the-loop validation results ($n=50$ seeds, 7{,}500 LLM calls).}
\label{tab:llm-validation}
\end{table}

The aggregate LLM-deterministic correlation is $r = 0.271$ (per-seed mean $r = 0.202$, 95\% CI $[0.075, 0.328]$), validating the need for end-to-end evaluation. VDD and recency produce identical LLM quality scores (both 0.522), reflecting identical retrieval outputs in this regime-shift scenario. All decay-aware methods outperform no-decay (0.473), confirming the core hypothesis that forgetting improves retrieval quality.

\textbf{Notable finding}: VDD and recency's zero variance across 50 seeds indicates deterministic retrieval behavior---these methods consistently retrieve the same documents regardless of random seed, while static and time-weighted show stochastic variation. The low LLM-deterministic correlation ($r = 0.271$) highlights the gap between proxy metrics and end-to-end evaluation, a known challenge in RAG evaluation \cite{es2023}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/35_llm_validation_expanded.png}
\caption{LLM-in-the-loop validation ($n=50$ seeds, 7{,}500 LLM calls). Left: LLM answer quality by method. Center: LLM vs.\ deterministic scores showing the evaluation gap. Right: score distributions---VDD and recency produce deterministic outputs (zero variance) while static and time-weighted show stochastic variation.}
\label{fig:llm-validation}
\end{figure}

\subsection{Hyperparameter Sensitivity}
\label{sec:hyperparameter}

Window sizes and sigmoid parameters substantially affect VDD performance. Key findings:

\begin{itemize}
  \item \textbf{Window sizes}: \texttt{curr\_window=3}, \texttt{arch\_window=200} achieves IAE 12.6, \textbf{beating recency's 23.3} (325\% range across configurations).
  \item \textbf{Sigmoid $V_0$}: Default 0.5 ranks 25/36; $V_0 \in [0.1, 0.3]$ consistently best (254\% performance range).
  \item \textbf{Sigmoid $k$}: $k \in [5, 10]$ balances responsiveness and stability.
  \item \textbf{Recommended}: $\lambda_{\text{base}} \in [0.2, 0.3]$, $\lambda_{\text{max}} \in [0.9, 0.95]$, $V_0 \in [0.1, 0.3]$, $k \in [5, 10]$, cw $\in [3, 5]$, aw $\in [100, 200]$.
\end{itemize}

\textbf{Window size ablation ($n=30$)}:

\begin{table}[h]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{curr\_window} & \textbf{arch\_window} & \textbf{IAE (mean $\pm$ std)} & \textbf{vs Default} \\
\midrule
\textbf{3} & \textbf{200} & \textbf{12.6 $\pm$ 1.4} & \textbf{Best} \\
3 & 100 & 12.8 $\pm$ 1.5 & -61\% \\
5 & 200 & 16.2 $\pm$ 1.3 & -50\% \\
5 & 100 & 16.4 $\pm$ 1.4 & -50\% \\
10 & 200 & 21.4 $\pm$ 1.4 & -35\% \\
3 & 50 & 22.6 $\pm$ 2.0 & -31\% \\
20 & 200 & 25.1 $\pm$ 1.2 & -23\% \\
3 & 20 & 28.3 $\pm$ 2.3 & -13\% \\
50 & 200 & 29.7 $\pm$ 1.6 & -9\% \\
\textbf{5} & \textbf{50} & \textbf{32.7 $\pm$ 2.3} & \textbf{Default} \\
5 & 20 & 38.8 $\pm$ 2.5 & +19\% \\
10 & 50 & 46.7 $\pm$ 2.6 & +43\% \\
10 & 20 & 53.7 $\pm$ 2.7 & +64\% \\
\bottomrule
\end{tabular}
\caption{Window size ablation ($n=30$). Smaller \texttt{curr\_window} increases drift sensitivity; \texttt{arch\_window} $\geq 200$ saturates.}
\label{tab:window-ablation}
\end{table}

\textbf{Sigmoid parameter sensitivity ($n=30$)}:

\begin{table}[h]
\centering
\begin{tabular}{@{}llccc@{}}
\toprule
\textbf{Scenario} & \textbf{Best Config} & \textbf{IAE} & \textbf{Default IAE} & \textbf{Default Rank} \\
\midrule
Regime Shifts & $k$=10, $V_0$=0.1 & \textbf{26.2} & 63.6 & 25/36 \\
Mixed Drift & $k$=5, $V_0$=0.1 & \textbf{28.8} & 43.3 & 25/36 \\
Bursty & $k$=5, $V_0$=0.1 & \textbf{34.1} & 69.5 & 25/36 \\
Reversion & $k$=10, $V_0$=0.1 & \textbf{29.7} & 62.2 & 25/36 \\
\bottomrule
\end{tabular}
\caption{Sigmoid parameter sensitivity ($n=30$). $V_0=0.1$ is consistently best.}
\label{tab:sigmoid-sensitivity}
\end{table}

\textbf{Lambda ablation heatmap (IAE values, lower is better)}:

\begin{table}[h]
\centering
\begin{tabular}{@{}lccccc@{}}
\toprule
$\lambda_{\text{base}}$ \textbackslash\ $\lambda_{\text{max}}$ & 0.50 & 0.70 & 0.80 & 0.90 & 0.95 \\
\midrule
0.01 & 34.8 & 24.3 & 20.8 & 17.9 & 16.6 \\
0.05 & 32.4 & 22.4 & 19.1 & 16.3 & 15.1 \\
0.10 & 30.0 & 20.6 & 17.3 & 14.7 & 13.5 \\
0.20 & 26.2 & 17.5 & 14.6 & 12.1 & 11.0 \\
0.30 & 23.2 & 15.1 & 12.3 & 10.0 & \textbf{9.0} \\
\bottomrule
\end{tabular}
\caption{Lambda ablation heatmap (IAE). Higher values for both parameters consistently improve performance. Smooth optimization landscape with no local minima.}
\label{tab:lambda-ablation}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{figures/29_sigmoid_heatmap.png}
\caption{Sigmoid hyperparameter sensitivity ($k$ vs.\ $V_0$) across four drift scenarios ($n=30$). Green = low IAE (best). Blue dashed box = default configuration. $V_0 = 0.1$ consistently outperforms the original default $V_0 = 0.5$, with performance range of 231--270\% across scenarios.}
\label{fig:sigmoid-heatmap}
\end{figure}

%% ============================================================
\FloatBarrier
\section{Discussion}
\label{sec:discussion}

\subsection{VDD in the Modern RAG Landscape}
\label{sec:vdd-landscape}

VDD operates at the memory management layer---below retrieval-time decisions (Self-RAG, CRAG) and above embedding model choices. This positioning suggests five deployment scenarios:

\textbf{Coding assistant} (mixed drift: gradual deprecations + sudden breaking changes). Drift pattern unpredictable. $\to$ \textbf{VDD recommended.} The assistant faces months of gradual API deprecation followed by sudden major-version breaks. VDD's adaptive forgetting handles both without manual tuning.

\textbf{News aggregation} (constant high drift). All content becomes stale quickly. $\to$ \textbf{Recency recommended.} Simple aggressive forgetting suffices when virtually all stored knowledge becomes obsolete within hours.

\textbf{Medical knowledge base} (gradual evolution + emergency updates). Guidelines evolve slowly, but emergency safety alerts require immediate adoption. $\to$ \textbf{VDD recommended.} The mixed pattern---stable baseline with occasional urgent updates---is precisely where VDD's volatility detection provides value.

\textbf{Enterprise documentation} (mostly stable, occasional reorganizations). Long stable periods punctuated by version migrations. $\to$ \textbf{VDD recommended.} VDD's low resting decay preserves stable knowledge while accelerating forgetting during migrations.

\textbf{Social media trending} (rapid, unpredictable churn). $\to$ \textbf{Recency recommended.} Content validity is so short-lived that aggressive forgetting is always appropriate.

\subsection{Positioning VDD Among Contemporary Systems}
\label{sec:positioning-discussion}

VDD occupies a specific niche in the contemporary RAG memory landscape. Several key distinctions:

\textbf{VDD vs.\ ARM} \cite{bursa2026}: ARM adapts retrieval \emph{strategy} (which retrieval approach to use for a given query), while VDD adapts memory \emph{retention} (how aggressively to discount stored knowledge over time). ARM operates at query time; VDD operates continuously on the knowledge base. A system could use ARM to select the retrieval strategy and VDD to manage the temporal quality of the underlying corpus.

\textbf{VDD vs.\ T-GRAG/HippoRAG}: Graph-based approaches (T-GRAG) and biological memory models (HippoRAG) provide structural representations of temporal relationships. VDD provides a simpler, complementary mechanism: rather than restructuring the knowledge representation, VDD adds a time-varying weight to existing retrieval scores. This makes VDD easier to integrate into existing RAG pipelines (3 lines of scoring code) but less capable of complex temporal reasoning.

\textbf{VDD vs.\ Mem0/FadeMem}: Both Mem0 and FadeMem implement forms of memory decay, validating the core premise. VDD's contribution is making the decay rate \emph{itself} adaptive rather than fixed. Experiment~37 validates this insight directly: online\_lambda (another adaptive approach) matches VDD ($d = -0.056$), while static methods fall significantly behind.

\textbf{VDD vs.\ Knowledge Editing (WISE, ROME)}: Knowledge editing modifies model parameters to update specific facts. VDD modifies retrieval scores to deprioritize outdated knowledge. For high-priority individual fact updates (e.g., ``the CEO of X is now Y''), knowledge editing is more precise. For managing the temporal validity of a large, evolving knowledge corpus, VDD's automatic adaptation is more practical.

\subsection{Scalability Considerations}
\label{sec:scalability}

VDD's core computation is $O(d)$ per query (centroid update + cosine distance + sigmoid), where $d$ is the embedding dimension. For the memory scoring loop, complexity is $O(n \cdot d)$ where $n$ is the number of memories. Practical scaling guidance:

\textbf{$< 10{,}000$ memories}: VDD operates within standard RAG latency budgets. Our benchmark shows 2.75ms mean, 6.48ms P99 for 1,000 memories.

\textbf{10,000--100,000 memories}: The memory scoring loop becomes the bottleneck. Integration with approximate nearest neighbor indices (FAISS, ScaNN) is recommended: first retrieve top-$k$ candidates via ANN, then apply VDD's temporal weighting only to the candidate set. VDD's overhead reduces to $O(k \cdot d)$ where $k \ll n$.

\textbf{$> 100{,}000$ memories}: Batch processing strategies for volatility updates become necessary. The drift detector can operate on sampled embeddings (e.g., 1\% sample) without significant detection loss, since centroid estimation is robust to subsampling. Volatility can be computed asynchronously and cached between queries.

\textbf{Key point}: VDD's overhead is $O(d)$ per query for drift detection, independent of corpus size. The $O(n)$ component is the memory scoring, which is shared with any retrieval system and can be optimized via ANN indexing.

\subsection{StreamingQA Benchmark Evaluation (Experiment 38)}
\label{sec:streamingqa}

To address the gap in standard benchmark evaluation, we tested VDD on StreamingQA \cite{liska2022}, which contains 36,378 temporal QA pairs spanning 2007--2020. We sampled 1,500 questions stratified by year, embedded documents using nomic-embed-text, and evaluated five methods across four temporal checkpoints (2011, 2014, 2017, 2020) with 10 seeds.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Overall} & \textbf{2011} & \textbf{2014} & \textbf{2017} & \textbf{2020} \\
\midrule
no\_decay & \textbf{0.104} & 0.102 & 0.107 & 0.107 & 0.101 \\
time\_weighted & 0.029 & 0.042 & 0.024 & 0.026 & 0.025 \\
static & 0.009 & 0.010 & 0.006 & 0.008 & 0.012 \\
VDD & 0.008 & 0.008 & 0.006 & 0.009 & 0.009 \\
recency & 0.008 & 0.008 & 0.006 & 0.009 & 0.009 \\
\bottomrule
\end{tabular}
\caption{StreamingQA results. No\_decay wins decisively ($d = -7.68$ vs VDD) because StreamingQA tests knowledge accumulation, not replacement.}
\end{table}

\textbf{VDD loses decisively on this benchmark} ($d = -7.68$ vs no\_decay). This is the expected result, and understanding \emph{why} is instructive.

StreamingQA is a \textbf{knowledge accumulation} benchmark: each question has a unique answer grounded in a specific news article. New facts do not replace old facts---they add to the corpus. In this setting, decay of any kind is counterproductive because discounting old documents removes correct answers that remain valid indefinitely.

VDD is designed for \textbf{knowledge replacement} scenarios: when React 18 deprecates a pattern from React 16, the old answer should lose weight. StreamingQA has no such replacement structure---a 2008 news fact about an election result is still correct in 2020.

This result directly validates the decision guide in Section~\ref{sec:decision}: \textbf{do not use VDD (or any decay method) when knowledge accumulates rather than replaces}. The temporal distance analysis confirms this---no\_decay scores uniformly across age bands (0--1yr: 0.114, 2--5yr: 0.104, 6+yr: 0.095), while decay methods suppress older correct answers (VDD 6+yr: 0.004 vs 0--1yr: 0.028).

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{figures/38_streamingqa_benchmark.png}
\caption{StreamingQA benchmark results. Decay methods underperform on knowledge accumulation tasks where old facts remain valid.}
\label{fig:streamingqa}
\end{figure}

\subsection{FreshQA Benchmark Evaluation (Experiment~39)}
\label{sec:freshqa}

To complement StreamingQA (a knowledge-accumulation benchmark where VDD loses), we evaluated on FreshQA \cite{vu2024}, which contains 600 temporal questions categorized as fast-changing, slow-changing, and never-changing. After excluding 148 false-premise questions, we evaluated 452 questions using nomic-embed-text embeddings (10 seeds, top-$k$=3). For each question, we constructed a temporal corpus with multiple outdated answer versions plus the current answer, simulating the knowledge-replacement scenario VDD targets.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Method} & \textbf{Fast-Changing} & \textbf{Slow-Changing} & \textbf{Never-Changing} & \textbf{Overall} & $d$\textbf{(vs VDD)} \\
\midrule
no\_decay & 0.077 & 0.058 & 0.075 & \textbf{0.070} & -9.88 \\
time\_weighted & 0.053 & 0.009 & 0.004 & 0.022 & -0.61 \\
static & 0.044 & 0.021 & 0.013 & 0.026 & -0.76 \\
recency & 0.036 & 0.015 & 0.011 & 0.021 & -0.27 \\
VDD & 0.033 & 0.013 & 0.010 & 0.019 & --- \\
\bottomrule
\end{tabular}
\caption{FreshQA keyword-overlap scores by question category (10 seeds, 452 questions). No\_decay wins on raw scores because outdated answers share keywords with ground truth.}
\label{tab:freshqa-scores}
\end{table}

On raw keyword-overlap scores, no\_decay wins (0.070 vs VDD 0.019). However, examining \textbf{current-answer retrieval rate} reveals the mechanism:

\begin{table}[h]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{Fast} & \textbf{Slow} & \textbf{Never} \\
\midrule
VDD & \textbf{1.000} & \textbf{1.000} & \textbf{1.000} \\
recency & \textbf{1.000} & \textbf{1.000} & \textbf{1.000} \\
static & \textbf{1.000} & \textbf{1.000} & \textbf{1.000} \\
time\_weighted & \textbf{1.000} & \textbf{1.000} & \textbf{1.000} \\
no\_decay & 0.699 & 0.799 & 0.906 \\
\bottomrule
\end{tabular}
\caption{FreshQA current-answer retrieval rate. All decay methods retrieve the current answer 100\% of the time; no\_decay retrieves it only 70--90\%.}
\label{tab:freshqa-retrieval}
\end{table}

All decay methods retrieve the current answer 100\% of the time, while no\_decay retrieves it only 70--90\% of the time. The keyword-overlap metric fails to distinguish between ``retrieved the current answer plus some outdated ones'' (decay methods) and ``retrieved mostly outdated answers that happen to share more keywords'' (no\_decay). No\_decay's higher keyword overlap comes from retrieving \emph{all} versions---outdated ones that partially match the ground truth contribute to the F1 score even though they represent stale knowledge.

\textbf{Methodological insight}: This result highlights a known limitation of keyword-overlap evaluation for temporal QA. A metric that rewards retrieving only the current answer (e.g., exact match on the most recent version, or a human evaluation of temporal correctness) would better capture the decay methods' advantage. We report both metrics transparently.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/39_freshqa_benchmark.png}
\caption{FreshQA benchmark results. All decay methods achieve 100\% current-answer retrieval rate, while no\_decay retrieves the current answer only 70--90\% of the time. Keyword-overlap scoring fails to capture this advantage.}
\label{fig:freshqa}
\end{figure}

\subsection{VDD's Value Proposition}
\label{sec:value-proposition}

Our 42 experiments reveal that VDD is \textbf{not universally optimal} but provides four distinct values:

\textbf{Provably adaptive}: VDD's $\lambda$ is genuinely bimodal (Hartigan's Dip $p < 0.001$, Ashman's $D > 2$) in 4/5 drift-containing scenarios. VDD beats a static oracle calibrated to its own mean $\lambda$ in mixed drift (3.2x improvement), proving that \emph{timing} of elevated decay matters.

\textbf{Safest default under uncertainty}: Among 13 baselines (expanded from 10), VDD ranks \#2 across all four drift patterns. Time-weighted methods fail 5--20x worse on sudden shifts; sliding windows collapse catastrophically on certain domains; recency fails on reversions ($d = +1.22$ VDD advantage).

\textbf{Staleness reduction on real data}: In realistic scenarios with accumulated legacy trust, VDD achieves significantly lower staleness than recency ($d = +1.85$), strengthening with real embeddings (0.221 vs 0.273).

\textbf{Validated adaptive principle}: The near-equivalence of VDD and online\_lambda ($d = -0.056$) validates that \emph{adaptive decay itself} is the valuable insight, not the specific mechanism. Any system implementing volatility-aware forgetting---whether via embedding distance, prediction error, or other signals---will outperform static approaches.

%% ============================================================
\FloatBarrier
\section{Limitations}
\label{sec:limitations}

We enumerate fourteen limitations transparently:

\begin{enumerate}
  \item \textbf{Recency often wins on accuracy}: $\lambda \geq 0.5$ outperforms VDD in permanent high-drift ($d = -2.90$). VDD's value is in staleness reduction, robustness, and adaptivity.

  \item \textbf{Adaptive baselines are competitive}: Holt--Winters and EMA-lambda beat VDD in 3/4 synthetic scenarios, though VDD excels in mixed drift.

  \item \textbf{Detector precision requires context}: Stable-period false activation is 13.5\% (not $\sim$1\% as initially reported). Post-drift cleanup (99.9\% elevated) is correct behavior, not error.

  \item \textbf{Time-weighted dominates gradual transitions}: 0.899 vs 0.621 on real-world documentation that evolves gradually.

  \item \textbf{Sigmoid parameters require tuning}: Default $V_0=0.5$ ranks 25/36; practitioners must tune $V_0 \in [0.1, 0.3]$. Auto-calibration via 75th-percentile burn-in (Experiment~40) improves over hand-tuned defaults by 22.7\% and eliminates manual tuning, but the optimal fixed $V_0$ still outperforms auto-calibration.

  \item \textbf{Window size sensitivity}: 325\% performance range. Poor choices (cw=10, aw=20) substantially degrade VDD.

  \item \textbf{Effect size inflation}: Large Cohen's $d$ values in synthetic experiments partly reflect controlled conditions with low variance; real-embedding experiments confirm attenuated but directionally consistent effects (0.70x ratio). Type~M analysis shows at $d=0.5$ true effect, observed $d$ is inflated 1.43x. \textbf{Production deployments would likely show smaller effects than those reported here.}

  \item \textbf{Scaling}: $O(n)$ complexity; $n > 10{,}000$ memories requires approximate nearest neighbor search (FAISS, ScaNN). Drift detection remains $O(d)$.

  \item \textbf{LLM validation scale}: LLM-in-the-loop evaluation expanded from $n=5$ to $n=50$ seeds (7,500 LLM calls), confirming stable rankings. However, VDD and recency show zero variance across seeds, indicating deterministic retrieval behavior rather than robust statistical estimation. Larger-scale evaluation with RAGAS \cite{es2023} and diverse fact sets would strengthen findings.

  \item \textbf{Domain scope}: Validated on 3 technical documentation domains. Generalization to news, medical, and legal domains requires future work.

  \item \textbf{No deployment study}: VDD has not been evaluated in a production RAG system. The computational overhead (2.75ms) suggests feasibility, but integration complexity, parameter tuning burden, and real-world drift characteristics remain untested.

  \item \textbf{Benchmark results are nuanced}: On StreamingQA (accumulation), VDD scores 0.008 vs no\_decay's 0.104 ($d = -7.68$). On FreshQA (replacement), all decay methods achieve 100\% current-answer retrieval vs no\_decay's 70--80\%, but keyword-overlap scoring obscures this advantage. Neither benchmark provides a clean evaluation of VDD's target scenario---StreamingQA lacks knowledge replacement, while FreshQA's evaluation metric penalizes decay methods that correctly suppress outdated answers.

  \item \textbf{Embedding transfer limitations}: While rankings transfer between hash and real embeddings (4/5 scenarios), effect sizes attenuate by approximately 30\%. The one scenario where rankings differ (reversion) suggests that real embedding similarity structure may compress the performance differences that hash embeddings amplify.

  \item \textbf{Online lambda equivalence}: An adaptive baseline using prediction-error-based lambda learning matches VDD ($d = -0.056$). This validates the adaptive principle but weakens the claim that VDD's specific mechanism (sigmoid on embedding distance) is uniquely valuable. VDD's advantages are practical (no ground-truth labels needed, $O(d)$ computation) rather than performance-based.
\end{enumerate}

%% ============================================================
\section{Future Work}
\label{sec:future-work}

\begin{itemize}
  \item \textbf{Temporal evaluation metrics}: FreshQA evaluation (Section~\ref{sec:freshqa}) revealed that keyword-overlap scoring fails to capture decay methods' advantage in suppressing outdated answers. Developing metrics that reward temporal precision (retrieving \emph{only} the current answer) would better evaluate VDD's target scenario. RealTimeQA (weekly updates) offers another evaluation axis.
  \item \textbf{Deployment study}: Integration with a production RAG system (e.g., LangChain, LlamaIndex) measuring real-world impact on answer quality, staleness reduction, and user satisfaction.
  \item \textbf{Hybrid detection}: Combining semantic distance (high recall) with prediction error (high precision) to improve the 13.5\% false activation rate.
  \item \textbf{Multi-topic memory}: Separate decay rates per domain to prevent topic-switch false positives, inspired by T-GRAG's domain-aware temporal structure.
  \item \textbf{Modern RAG integration}: Combining VDD with Self-RAG or CRAG for joint temporal and quality management. Specifically, VDD could modulate Self-RAG's reflection token thresholds based on temporal validity.
  \item \textbf{Adaptive threshold learning}: Learning optimal drift thresholds ($V_0$) from deployment feedback, potentially converging with online\_lambda's prediction-error approach.
  \item \textbf{Knowledge editing synergy}: Exploring VDD as a complement to WISE/ROME for joint parametric and non-parametric memory management.
  \item \textbf{HippoRAG integration}: Adding VDD-style temporal weighting to HippoRAG's knowledge graph edges, enabling joint structural and temporal memory management.
  \item \textbf{Distributional drift detection}: Upgrading from mean-embedding distance to full MMD or LSDD for distributional sensitivity, at the cost of increased computational overhead.
\end{itemize}

%% ============================================================
\section{Conclusion}
\label{sec:conclusion}

We have validated \textbf{Volatility-Driven Decay} across 42 experiments with rigorous statistical methodology ($n = 30$, bootstrap CIs, Benjamini--Hochberg FDR correction, Type~M error analysis, explicit seeds). VDD achieves 87.5\% error reduction versus static baselines, ranks \#2 among 13 methods across four drift patterns, and demonstrates genuinely bimodal adaptation (Hartigan's Dip $p < 0.001$, Ashman's $D > 2$). On 120 real-world facts across three domains with real embeddings, VDD beats recency on staleness ($d = +2.13$) while maintaining cross-domain consistency (accuracy 0.605--0.647).

Rankings transfer between hash-based and real neural embeddings (4/5 scenario agreement, $r = 0.935$ effect size correlation), with real embeddings producing attenuated but directionally consistent effects (0.70x ratio). All five activation functions converge at recommended $k \geq 5$ ($d \to 0$), justifying sigmoid selection on interpretability grounds. An adaptive baseline (online\_lambda) matches VDD's performance ($d = -0.056$), validating the core principle that adaptive decay is valuable regardless of the specific adaptation mechanism.

VDD is not universally best. Recency wins in permanent high-drift. Time-weighted retrieval dominates gradual transitions. Holt--Winters beats VDD in 3/4 synthetic scenarios. The key insight is not that VDD is the best method---it is not, in most individual scenarios. The insight is that VDD is the \textbf{safest method when the scenario is unknown}, which in production, it usually is.

VDD complements rather than competes with modern RAG architectures. Self-RAG, CRAG, and Adaptive-RAG improve retrieval quality; ARM adapts retrieval strategy; T-GRAG and HippoRAG provide temporal structure; VDD manages temporal validity through adaptive forgetting. A production system benefits from combining these approaches. We release the complete benchmark, dataset, and evaluation framework to support further research at the intersection of temporal memory management and retrieval-augmented generation.

%% ============================================================
\begin{thebibliography}{99}

\bibitem{abdallah2026}
Abdallah, M., et al. (2026).
TEMPO: A Temporal Reasoning Benchmark for Language Models.
\emph{arXiv preprint}.

\bibitem{an2025}
An, J. (2025).
Cognitive Workspace: An Attention-Gated Working Memory Architecture for LLM Agents.
\emph{arXiv preprint}.

\bibitem{asai2024}
Asai, A., Wu, Z., Wang, Y., Sil, A., \& Hajishirzi, H. (2024).
Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.
\emph{ICLR 2024}.

\bibitem{bifet2007}
Bifet, A., \& Gavalda, R. (2007).
Learning from Time-Changing Data with Adaptive Windowing.
\emph{SIAM International Conference on Data Mining}, 443--448.

\bibitem{borgeaud2022}
Borgeaud, S., et al. (2022).
Improving Language Models by Retrieving from Trillions of Tokens.
\emph{ICML 2022}.

\bibitem{bursa2026}
Bursa, M. (2026).
ARM: Adaptive Retrieval Memory for RAG Systems.
\emph{arXiv preprint}.

\bibitem{chen2021}
Chen, W., et al. (2021).
A Dataset for Answering Time-Sensitive Questions.
\emph{NeurIPS 2021 Datasets and Benchmarks}.

\bibitem{chen2025}
Chen, Y., et al. (2025).
ChronoQA: A Benchmark for Temporal Knowledge Evolution in Question Answering.
\emph{arXiv preprint}.

\bibitem{chhikara2025}
Chhikara, P., et al. (2025).
Mem0: The Memory Layer for Personalized AI.
\emph{arXiv preprint}.

\bibitem{dredze2008}
Dredze, M., Crammer, K., \& Pereira, F. (2008).
Confidence-Weighted Linear Classification.
\emph{ICML 2008}, 264--271.

\bibitem{ebbinghaus1885}
Ebbinghaus, H. (1885).
\emph{{\"U}ber das Ged{\"a}chtnis: Untersuchungen zur experimentellen Psychologie}.
Duncker \& Humblot.

\bibitem{es2023}
Es, S., James, J., Espinosa-Anke, L., \& Schockaert, S. (2023).
RAGAS: Automated Evaluation of Retrieval Augmented Generation.
\emph{arXiv:2309.15217}.

\bibitem{feldhans2021}
Feldhans, R., Wilke, A., Heusinger, S., \& Hammer, B. (2021).
Drift Detection in Text Data with Document Embeddings.
\emph{International Conference on Intelligent Data Engineering and Automated Learning (IDEAL)}, 107--118.

\bibitem{gama2014}
Gama, J., Zliobaite, I., Bifet, A., Pechenizkiy, M., \& Bouchachia, A. (2014).
A Survey on Concept Drift Adaptation.
\emph{ACM Computing Surveys}, 46(4), 1--37.

\bibitem{gao2024}
Gao, Y., et al. (2024).
Retrieval-Augmented Generation for Large Language Models: A Survey.
\emph{arXiv:2312.10997}.

\bibitem{gelman2014}
Gelman, A., \& Carlin, J. (2014).
Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors.
\emph{Perspectives on Psychological Science}, 9(6), 641--651.

\bibitem{gutierrez2024}
Gutierrez, B. J., et al. (2024).
HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models.
\emph{NeurIPS 2024}.

\bibitem{guu2020}
Guu, K., Lee, K., Tung, Z., Pasupat, P., \& Chang, M.-W. (2020).
REALM: Retrieval-Augmented Language Model Pre-Training.
\emph{ICML 2020}.

\bibitem{hartigan1985}
Hartigan, J. A., \& Hartigan, P. M. (1985).
The Dip Test of Unimodality.
\emph{Annals of Statistics}, 13(1), 70--84.

\bibitem{haykin2002}
Haykin, S. (2002).
\emph{Adaptive Filter Theory} (4th ed.).
Prentice Hall.

\bibitem{holt2004}
Holt, C. C. (2004).
Forecasting Seasonals and Trends by Exponentially Weighted Moving Averages.
\emph{International Journal of Forecasting}, 20(1), 5--10.

\bibitem{hu2026}
Hu, Z., et al. (2026).
RAG without Forgetting: Experience Replay Memory for Domain-Adaptive RAG.
\emph{arXiv preprint}.

\bibitem{izacard2023}
Izacard, G., et al. (2023).
Atlas: Few-shot Learning with Retrieval Augmented Language Models.
\emph{JMLR}, 24(251), 1--43.

\bibitem{jeong2024}
Jeong, S., Baek, J., Cho, S., Hwang, S. J., \& Park, J. C. (2024).
Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity.
\emph{NAACL 2024}.

\bibitem{jiang2023}
Jiang, Z., Xu, F. F., Gao, L., et al. (2023).
Active Retrieval Augmented Generation.
\emph{EMNLP 2023}.

\bibitem{karpukhin2020}
Karpukhin, V., et al. (2020).
Dense Passage Retrieval for Open-Domain Question Answering.
\emph{EMNLP 2020}.

\bibitem{kasai2023}
Kasai, J., et al. (2023).
RealTime QA: What's the Answer Right Now?
\emph{NeurIPS 2023}.

\bibitem{kirkpatrick2017}
Kirkpatrick, J., et al. (2017).
Overcoming Catastrophic Forgetting in Neural Networks.
\emph{PNAS}, 114(13), 3521--3526.

\bibitem{kolter2007}
Kolter, J. Z., \& Maloof, M. A. (2007).
Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts.
\emph{JMLR}, 8, 2755--2790.

\bibitem{lewis2020}
Lewis, P., et al. (2020).
Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.
\emph{NeurIPS}, 33.

\bibitem{li2025}
Li, X., et al. (2025).
T-GRAG: Temporal Graph-Based Retrieval-Augmented Generation.
\emph{arXiv preprint}.

\bibitem{liska2022}
Liska, A., Kocisky, T., Gribovskaya, E., et al. (2022).
StreamingQA: A Benchmark for Adaptation to New Knowledge over Time.
\emph{ICML 2022}.

\bibitem{liu2024}
Liu, N. F., et al. (2024).
Lost in the Middle: How Language Models Use Long Contexts.
\emph{TACL}, 12, 157--173.

\bibitem{lu2019}
Lu, J., Liu, A., Dong, F., Gu, F., Gama, J., \& Zhang, G. (2019).
Learning under Concept Drift: A Review.
\emph{IEEE TKDE}, 31(12), 2346--2363.

\bibitem{mallya2018}
Mallya, A., \& Lazebnik, S. (2018).
PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning.
\emph{CVPR 2018}.

\bibitem{mem0ai2024}
Mem0 AI. (2024).
Mem0: The Memory Layer for AI Applications.
\url{https://mem0.ai/}

\bibitem{meng2022}
Meng, K., Bau, D., Andonian, A., \& Belinkov, Y. (2022).
Locating and Editing Factual Associations in GPT.
\emph{NeurIPS 2022}.

\bibitem{meng2023}
Meng, K., et al. (2023).
Mass-Editing Memory in a Transformer.
\emph{ICLR 2023}.

\bibitem{page1954}
Page, E. S. (1954).
Continuous Inspection Schemes.
\emph{Biometrika}, 41(1/2), 100--115.

\bibitem{packer2023}
Packer, C., et al. (2023).
MemGPT: Towards LLMs as Operating Systems.
\emph{arXiv:2310.08560}.

\bibitem{park2023}
Park, J. S., et al. (2023).
Generative Agents: Interactive Simulacra of Human Behavior.
\emph{arXiv:2304.03442}.

\bibitem{pfister2013}
Pfister, R., Schwarz, K. A., Janczyk, M., Dale, R., \& Freeman, J. B. (2013).
Good Things Peak in Pairs: A Note on the Bimodality Coefficient.
\emph{Frontiers in Psychology}, 4, 700.

\bibitem{qian2025}
Qian, H., et al. (2025).
MemoRAG: Moving towards Next-Gen RAG via Memory-Inspired Knowledge Discovery.
\emph{arXiv preprint}.

\bibitem{rusu2016}
Rusu, A. A., et al. (2016).
Progressive Neural Networks.
\emph{arXiv:1606.04671}.

\bibitem{saadfalcon2024}
Saad-Falcon, J., et al. (2024).
ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems.
\emph{NAACL 2024}.

\bibitem{sarthi2024}
Sarthi, P., Abdullah, S., Tuli, A., et al. (2024).
RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval.
\emph{ICLR 2024}.

\bibitem{silverman1981}
Silverman, B. W. (1981).
Using Kernel Density Estimates to Investigate Multimodality.
\emph{JRSS Series B}, 43(1), 97--99.

\bibitem{vu2024}
Vu, T., et al. (2024).
FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation.
\emph{ACL 2024}.

\bibitem{wang2024}
Wang, P., et al. (2024).
WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models.
\emph{NeurIPS 2024}.

\bibitem{wei2026}
Wei, L., \& Zhang, Y. (2026).
FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory.
\emph{arXiv:2601.18642}.

\bibitem{wixted2004}
Wixted, J. T. (2004).
The Psychology and Neuroscience of Forgetting.
\emph{Annual Review of Psychology}, 55(1), 235--269.

\bibitem{yan2024}
Yan, S.-Q., Gu, J.-C., Zhu, Y., \& Ling, Z.-H. (2024).
Corrective Retrieval Augmented Generation.
\emph{ICML 2024}.

\bibitem{yin2024}
Yin, Z., et al. (2024).
History Matters: Temporal Knowledge Editing in Large Language Models.
\emph{AAAI 2024}.

\bibitem{zhang2025}
Zhang, Y., et al. (2025).
A Survey on Memory in Large Language Model Agents.
\emph{arXiv preprint}.

\bibitem{zhong2024}
Zhong, W., Guo, L., Gao, Q., \& Wang, Y. (2024).
MemoryBank: Enhancing Large Language Models with Long-Term Memory.
\emph{AAAI 2024}.

\end{thebibliography}

%% ============================================================
\appendix

\section{Complete Experimental Results}
\label{app:experimental-results}

All 42 experiments are summarized below. Experiments highlighted in the main body (Section~\ref{sec:experiments}) include narrative discussion; remaining experiments are presented as table entries.

\subsection{Core Validation (Experiments~1--8)}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Exp} & \textbf{Description} & \textbf{Key Result} & $n$ \\
\midrule
1 & Drift detection comparison & Emb.\ Dist.: 100\% recall, 0.12ms; ADWIN/PH: 0\% & 1 \\
2 & VDD with real detection & 87.5\% error reduction vs static (IAE 15.81 vs 126.67) & 1 \\
3 & Vector memory bank & VDD: 72\% smaller memory, 0.656 acc vs 0.968 static & 1 \\
4 & Mixed-drift scenario & Recency 32.14 beats VDD 40.84 ($p < 0.001$) & 10 \\
5 & Lambda ablation & Best: $\lambda_b$=0.3, $\lambda_m$=0.95 (IAE 9.01, 92\% impr.) & 3 \\
6 & 10K-step stability & 0.03\% oscillation, mean error 0.096 & 1 \\
7 & Latency benchmark & 2.75ms mean, 6.48ms P99 (1000 memories) & 1000 \\
8 & 7-baseline comparison & VDD \#2, beats all static ($p < 0.001$); recency \#1 & 10 \\
\bottomrule
\end{tabular}
\caption{Core validation experiments (1--8).}
\label{tab:core-experiments}
\end{table}

\subsection{Extended Validation (Experiments~9--15)}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Exp} & \textbf{Description} & \textbf{Key Result} & $n$ \\
\midrule
9 & Gradual drift & Recency 38.7 beats VDD 45.2 ($p < 0.01$) & 10 \\
10 & Detection method comparison & Semantic dist.: 89\% recall; Pred.\ error: 67\% & 10 \\
11 & ADWIN comparison & VDD 20.94 vs ADWIN 112.36 ($d = -2.2$) & 10 \\
12 & Computational scaling & $O(n)$ linear: 0.28ms (100) $\to$ 26.8ms (10K) & 1 \\
13 & 20-fold statistical validation & VDD 28.4 [26.8, 30.1]; recency 23.1 [22.3, 24.0] & 20 \\
14 & Real RAG (React docs) & VDD staleness 0.266 $<$ recency 0.302 ($d = +1.85$) & 10 \\
15 & Bursty drift & Recency 28.7 beats VDD 48.2 ($d = -2.90$) & 10 \\
\bottomrule
\end{tabular}
\caption{Extended validation experiments (9--15).}
\label{tab:extended-experiments}
\end{table}

\subsection{Robustness and Real-World (Experiments~16--20)}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Exp} & \textbf{Description} & \textbf{Key Result} & $n$ \\
\midrule
16 & Reversion scenario & VDD 35.4 beats recency 52.1 ($d = +1.22$) & 10 \\
17 & Mixed uncertainty & VDD never worst; recency best acc (0.687) & 10 \\
18 & Staleness evaluation & VDD 18.3\% stale vs recency 24.7\% ($d = +0.89$) & 10 \\
19 & Effective lambda analysis & Bimodal in 5/5 (BC $> 0.555$); VDD 3.2x better & 30 \\
20 & Extended baselines (10 methods) & VDD \#2/10 in all 4 patterns & 30 \\
\bottomrule
\end{tabular}
\caption{Robustness and real-world experiments (16--20).}
\label{tab:robustness-experiments}
\end{table}

\subsection{Statistical Hardening (Experiments~21--26)}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Exp} & \textbf{Description} & \textbf{Key Result} & $n$ \\
\midrule
21 & Multi-domain (React + Python) & Time-weighted 0.899; VDD 0.621 consistent & 30 \\
22 & End-to-end quality & Time-weighted 0.877; VDD 0.622 beats no-decay 0.333 & 30 \\
23 & $n$=30 confirmatory & All ranks confirmed; optimized VDD (12.6) beats recency & 30 \\
24 & Precision analysis & 13.5\% stable false activation; 99.9\% post-drift correct & 30 \\
25 & Real embedding transfer & VDD improves 0.617 $\to$ 0.647; rankings preserved & 30 \\
26 & LLM-in-the-loop & VDD 0.522 matches recency; $r = 0.277$ vs deterministic & 5 \\
\bottomrule
\end{tabular}
\caption{Statistical hardening experiments (21--26).}
\label{tab:hardening-experiments}
\end{table}

\subsection{Advanced Analysis (Experiments~27--31)}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Exp} & \textbf{Description} & \textbf{Key Result} & $n$ \\
\midrule
27 & Sigmoid sensitivity & $V_0$=0.1 best; default ranks 25/36 (254\% range) & 30 \\
28 & $n$=30 rank stability & 100\% rank stability across all 10 methods & 30 \\
29 & Adaptive baselines & Holt--Winters beats VDD 3/4; VDD \#2 in mixed & 30 \\
30 & Robust bimodality & Bimodal in 4/5 (Dip $p < 0.001$, Ashman's $D > 2$) & 30 \\
31 & Three-domain (120 facts) & VDD beats recency all 3 domains ($d = +2.13$) & 30 \\
\bottomrule
\end{tabular}
\caption{Advanced analysis experiments (27--31).}
\label{tab:advanced-experiments}
\end{table}

\subsection{Revision Experiments (Experiments~34--42)}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Exp} & \textbf{Description} & \textbf{Key Result} & $n$ \\
\midrule
34 & Statistical corrections & 85/88 survive BH FDR; Type M: 1.43x at $d$=0.5 & 30 \\
35 & LLM validation (expanded) & $n$=50: VDD = Recency = 0.522; 7,500 LLM calls & 50 \\
36 & Real embedding suite & Rankings preserved 4/5; effect sizes 0.70x; $r = 0.935$ & 30 \\
37 & Activation ablation + baselines & All activations converge $k \geq 5$; online\_lambda $\approx$ VDD & 30 \\
38 & StreamingQA benchmark & VDD 0.008 vs no\_decay 0.104; decay counterproductive & 10 \\
39 & FreshQA benchmark & 100\% current-answer retrieval for decay vs 70--80\% no\_decay & 10 \\
40 & Auto-calibration of $V_0$ & 75th-pctile burn-in improves over hand-tuned by 22.7\% & 20 \\
41 & Cross-model embedding gen. & Rankings preserved: Spearman $\rho$=0.978 (nomic vs mxbai) & 10 \\
42 & Practitioner decision tree & Visual method selection guide based on drift pattern knowledge and task type & -- \\
\bottomrule
\end{tabular}
\caption{Revision experiments (34--42).}
\label{tab:revision-experiments}
\end{table}

%% ============================================================
\section{Statistical Details}
\label{app:statistical-details}

\subsection{Bootstrap Methodology}

All confidence intervals are computed via the percentile bootstrap method with $B = 1000$ resamples. For each comparison, we compute:

\begin{enumerate}
  \item Point estimate: sample mean of IAE, accuracy, or staleness.
  \item 95\% CI: 2.5th and 97.5th percentiles of bootstrap distribution.
  \item Cohen's $d$: $(M_1 - M_2) / s_{\text{pooled}}$.
\end{enumerate}

\subsection{Bimodality Testing Protocol}

We apply four complementary tests:

\begin{enumerate}
  \item \textbf{Bimodality Coefficient (BC)}: $\text{BC} = (m_3^2 + 1) / (m_4 + 3(n-1)^2 / ((n-2)(n-3)))$ where $m_3$ is skewness and $m_4$ is kurtosis. BC $> 0.555$ suggests bimodality \cite{pfister2013}.
  \item \textbf{Hartigan's Dip Test}: Tests unimodality against any alternative. $p < 0.05$ rejects unimodality \cite{hartigan1985}.
  \item \textbf{Ashman's $D$}: $D = \sqrt{2} \cdot |\mu_1 - \mu_2| / \sqrt{\sigma_1^2 + \sigma_2^2}$. $D > 2$ indicates clean separation.
  \item \textbf{Silverman's bandwidth test}: Estimates number of modes via kernel density estimation \cite{silverman1981}.
\end{enumerate}

Using multiple tests guards against individual test limitations (e.g., BC's sensitivity to outliers).

\subsection{Multiple Comparison Correction Details}

We applied Benjamini--Hochberg FDR correction to all 88 hypothesis tests conducted across the paper. The procedure:

\begin{enumerate}
  \item Rank all 88 $p$-values from smallest to largest.
  \item For rank $i$, compute the BH threshold: $(i/88) \times 0.05$.
  \item Find the largest $i$ where $p_i \leq$ threshold.
  \item All tests with rank $\leq i$ are declared significant.
\end{enumerate}

Result: All 85 tests that were significant uncorrected remain significant after BH correction. The three non-significant tests (VDD vs.\ recency in specific scenarios where the difference was small) remain non-significant.

\subsection{Effect Size Caveat}

The large Cohen's $d$ values reported in synthetic experiments ($d > 3$ in several comparisons) partly reflect controlled simulation conditions producing low variance. With real embeddings, effect sizes attenuate to approximately 0.70x their hash-based values (Experiment~36). Type~M analysis (Experiment~34) further quantifies this: at a realistic true $d = 0.5$, observed $d$ would be exaggerated 1.43x in our study design. In noisier real-world deployments, effect sizes would likely be smaller but remain directionally consistent. \textbf{We report $d$ for completeness but emphasize practical significance alongside statistical significance, and we urge readers to treat reported effect sizes as upper bounds.}

%% ============================================================
\section{Integration Guide}
\label{app:integration}

\subsection{Minimal Integration}

\begin{verbatim}
from vdd import VDDRetriever

retriever = VDDRetriever(
    lambda_base=0.2,
    lambda_max=0.9,
    curr_window=3,
    arch_window=200,
    drift_threshold=0.4,
    v0=0.1,
    k=10
)

docs = retriever.retrieve(query, k=5)

print(f"Current lambda: {retriever.current_lambda:.3f}")
print(f"Drift detected: {retriever.drift_detected}")
\end{verbatim}

\subsection{Framework Compatibility}

VDD operates at the retrieval scoring layer and is compatible with any RAG framework (LangChain, LlamaIndex, Haystack). Integration requires:

\begin{enumerate}
  \item \textbf{Embedding stream access}: VDD needs the raw embeddings for drift detection.
  \item \textbf{Timestamp tracking}: Each memory requires an insertion timestamp.
  \item \textbf{Score modification}: Replace static similarity with $S = \text{Sim} \cdot \exp(-\lambda(t) \cdot \Delta t)$.
\end{enumerate}

No external APIs or model calls are required for drift detection---it operates entirely on embedding distances computed locally.

\subsection{Framework-Specific Considerations}

\textbf{LangChain}: VDD can be integrated as a custom retriever by subclassing \texttt{BaseRetriever} and overriding the \texttt{\_get\_relevant\_documents} method to apply temporal weighting. The embedding stream is accessible via LangChain's embedding model interface.

\textbf{LlamaIndex}: Integration via a custom \texttt{NodePostprocessor} that applies VDD's temporal weighting after initial retrieval. The query engine's embedding history provides the drift detection signal.

\textbf{Haystack}: VDD fits as a custom \texttt{Ranker} component in Haystack pipelines, operating between the retriever and reader stages.

The full implementation is available at: \url{https://github.com/abe238/volatility-driven-decay}

%% ============================================================
\section{Hyperparameter Selection Guide}
\label{app:hyperparameters}

For practitioners deploying VDD, we recommend the following selection procedure:

\begin{table}[h]
\centering
\begin{tabular}{@{}llllc@{}}
\toprule
\textbf{Parameter} & \textbf{Default} & \textbf{Tuning Range} & \textbf{Impact} & \textbf{Priority} \\
\midrule
$V_0$ (threshold) & 0.1 & $[0.05, 0.3]$ & 254\% IAE range & \textbf{Critical} \\
\texttt{curr\_window} & 3 & $[3, 10]$ & 325\% IAE range & \textbf{Critical} \\
\texttt{arch\_window} & 200 & $[100, 500]$ & Moderate & High \\
$k$ (steepness) & 10 & $[5, 20]$ & Low at $k \geq 5$ & Low \\
$\lambda_{\text{base}}$ & 0.2 & $[0.1, 0.3]$ & Moderate & Medium \\
$\lambda_{\text{max}}$ & 0.9 & $[0.8, 0.95]$ & Moderate & Medium \\
\bottomrule
\end{tabular}
\caption{Hyperparameter selection guide for VDD deployment.}
\label{tab:hyperparameters}
\end{table}

\textbf{Start with}: $V_0=0.1$, $k=10$, cw=3, aw=200, $\lambda_{\text{base}}=0.2$, $\lambda_{\text{max}}=0.9$. These defaults were selected based on grid search across 36 parameter configurations and 4 drift scenarios.

\textbf{Key insight from activation ablation}: At $k \geq 5$, the choice of activation function (sigmoid, linear, exponential, step, softplus) is immaterial ($d \to 0$). Therefore, practitioners should focus tuning effort on $V_0$ and window sizes rather than the activation shape.

\subsection{Auto-Calibration of $V_0$ (Experiment~40)}
\label{app:auto-calibration}

A key practical limitation is $V_0$ tuning (254\% performance range). We propose a simple auto-calibration method: set $V_0$ to the 75th percentile of volatility values observed during a burn-in period (first 20\% of the data stream). This requires no labeled data or manual intervention.

\textbf{Method}: During burn-in, collect volatility values $\{V_1, \ldots, V_b\}$ where $b = 0.2 \cdot T$. Set $V_0 = \text{percentile}(75, \{V_1, \ldots, V_b\})$. The 75th percentile ensures that only genuinely elevated volatility triggers aggressive decay, while accommodating the baseline volatility distribution of the specific embedding space.

\textbf{Results ($n=20$ seeds, 4 drift patterns)}:

\begin{table}[h]
\centering
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Pattern} & \textbf{Auto $V_0$} & \textbf{Hand-Tuned $V_0$} & \textbf{Auto IAE} & \textbf{Hand IAE} & \textbf{Improvement} \\
\midrule
Sudden & 0.048 & 0.10 & 21.6 & 29.9 & $-27.8\%$ \\
Gradual & 0.079 & 0.10 & 13.5 & 15.8 & $-14.6\%$ \\
Reversion & 0.048 & 0.10 & 18.9 & 25.9 & $-27.2\%$ \\
Mixed & 0.055 & 0.10 & 18.4 & 23.3 & $-21.1\%$ \\
\bottomrule
\end{tabular}
\caption{Auto-calibration of $V_0$ via 75th-percentile burn-in ($n=20$). Auto-calibration improves over hand-tuned defaults by 22.7\% on average.}
\label{tab:auto-calibration}
\end{table}

Auto-calibration improves over the hand-tuned default ($V_0=0.1$) by \textbf{22.7\% on average} (all $p < 10^{-8}$). The auto-calibrated values are consistently lower than the hand-tuned default, indicating that the default $V_0=0.1$ is slightly too conservative. The best fixed value ($V_0=0.01$) still outperforms auto-calibration, suggesting room for improvement, but auto-calibration eliminates the need for manual tuning entirely while providing substantial improvement over the default.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/40_auto_calibration.png}
\caption{Auto-calibration of $V_0$ via 75th-percentile burn-in. Auto-calibrated $V_0$ values adapt to each drift pattern's volatility distribution, improving over hand-tuned defaults by 22.7\% on average.}
\label{fig:auto-calibration}
\end{figure}

\newpage

\vspace*{\fill}

\begin{center}
{\small\itshape Opinions are my own. This work does not relate to my position at Amazon.}
\end{center}

\vspace*{\fill}

\end{document}
